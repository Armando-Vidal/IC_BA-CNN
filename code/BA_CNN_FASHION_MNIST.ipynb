{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvPb0YJNU1RM"
   },
   "source": [
    "\n",
    "<H3 align='center'> An Attention-Based Architecture for\n",
    "Hierarchical Classification with CNNs </H3>\n",
    "\n",
    "<H5 align='center'> FASHION MNIST </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CFPEBlEDW1R"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb27tNtvUuKD"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Concatenate, Add, Softmax\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "from keras.initializers import he_normal\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, CSVLogger\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from scipy import stats\n",
    "\n",
    "# Computes hierarchical metrics proposed by Kiritchenko et al (2005)\n",
    "def hierarchical_metrics(true,pred):\n",
    "  true_labels = []\n",
    "  true_fine = true[2].argmax(axis=1)\n",
    "  true_c2 = true[1].argmax(axis=1)\n",
    "  true_c1 = true[0].argmax(axis=1)\n",
    "  for i in range(len(true_fine)):\n",
    "    true_labels.append([true_c1[i],true_c2[i],true_fine[i]])\n",
    "  pred_labels = []\n",
    "  pred_c1 = pred[0].argmax(axis = 1)\n",
    "  pred_c2 = pred[1].argmax(axis = 1)\n",
    "  pred_fine = pred[2].argmax(axis = 1) \n",
    "  for i in range(len(pred_c1)):\n",
    "    pred_labels.append([pred_c1[i],pred_c2[i],pred_fine[i]])\n",
    "  preci = precision(true_labels,pred_labels)\n",
    "  reca = recall(true_labels,pred_labels)\n",
    "  f_1 = f1(true_labels,pred_labels)\n",
    "  return preci,reca,f_1\n",
    "\n",
    "# Hierarchical metrics, proposed by Kiritchenko et al (2005)\n",
    "# Implementation \n",
    "# https://gitlab.com/dacs-hpi/hiclass/-/blob/main/hiclass/metrics.py\n",
    "\n",
    "\n",
    "def precision(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute precision score for hierarchical classification.\n",
    "\n",
    "    hP = sum(|S intersection T|) / sum(|S|),\n",
    "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
    "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        What proportion of positive identifications was actually correct?\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    sum_intersection = 0\n",
    "    sum_prediction_and_ancestors = 0\n",
    "    for ground_truth, prediction in zip(y_true, y_pred):\n",
    "        sum_intersection = sum_intersection + len(\n",
    "            set(ground_truth).intersection(set(prediction))\n",
    "        )\n",
    "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
    "            set(prediction)\n",
    "        )\n",
    "    precision = sum_intersection / sum_prediction_and_ancestors\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute recall score for hierarchical classification.\n",
    "\n",
    "    hR = sum(|S intersection T|) / sum(|T|),\n",
    "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
    "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        What proportion of actual positives was identified correctly?\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    sum_intersection = 0\n",
    "    sum_prediction_and_ancestors = 0\n",
    "    for ground_truth, prediction in zip(y_true, y_pred):\n",
    "        sum_intersection = sum_intersection + len(\n",
    "            set(ground_truth).intersection(set(prediction))\n",
    "        )\n",
    "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
    "            set(ground_truth)\n",
    "        )\n",
    "    recall = sum_intersection / sum_prediction_and_ancestors\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute f1 score for hierarchical classification.\n",
    "\n",
    "    hF = 2 * hP * hR / (hP + hR),\n",
    "    where hP is the hierarchical precision and hR is the hierarchical recall.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Weighted average of the precision and recall\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDoP9BAcw0mj"
   },
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4htza5rw1uK"
   },
   "outputs": [],
   "source": [
    "height, width = 28, 28\n",
    "channel = 1\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (channel, height, width)\n",
    "else:\n",
    "    input_shape = (height, width, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ry3IEV28w5GC"
   },
   "outputs": [],
   "source": [
    "train_size = 60000\n",
    "test_size = 10000\n",
    "\n",
    "coarse1_classes = 2\n",
    "\n",
    "coarse2_classes = 6\n",
    "\n",
    "num_classes  = 10\n",
    "\n",
    "batch_size   = 128\n",
    "epochs       = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-PzlHwIw-Je",
    "outputId": "37e4001a-3f1e-4598-d477-b83d5a2e73de"
   },
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "class_names_c1 = ['Clothes', 'Goods']\n",
    "class_names_c2 = ['Tops', 'Bottoms', 'Dresses', 'Outers', 'Accessories', 'Shoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyIRYqIzxAty",
    "outputId": "d79323b5-9a75-4305-c120-9314b207bf58"
   },
   "outputs": [],
   "source": [
    "c2_to_c1 = {0:0, 1:0, 2:0, 3:0, 4:1, 5:1}\n",
    "fine_to_c2 = {0:0, 1:1, 2:0, 3:2, 4:3, 5:5, 6:0, 7:5, 8:4, 9:5}\n",
    "\n",
    "def print_mappings(mapping, source, dest):\n",
    "    for k,v in mapping.items():\n",
    "        print(source[k], \"->\", dest[v])\n",
    "\n",
    "print_mappings(c2_to_c1, class_names_c2, class_names_c1)\n",
    "print(\"-\"*10)\n",
    "print_mappings(fine_to_c2, class_names, class_names_c2)\n",
    "\n",
    "train_labels_fine = to_categorical(train_labels)\n",
    "test_labels_fine = to_categorical(test_labels)\n",
    "train_labels_c2_index = [fine_to_c2[i] for i in train_labels]\n",
    "train_labels_c2 = to_categorical(train_labels_c2_index)\n",
    "test_labels_c2_index = [fine_to_c2[i] for i in test_labels]\n",
    "test_labels_c2 = to_categorical(test_labels_c2_index)\n",
    "train_labels_c1_index = [c2_to_c1[i] for i in train_labels_c2_index]\n",
    "train_labels_c1 = to_categorical(train_labels_c1_index)\n",
    "test_labels_c1_index = [c2_to_c1[i] for i in test_labels_c2_index]\n",
    "test_labels_c1 = to_categorical(test_labels_c1_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3FDHvKqxN4v",
    "outputId": "413cc312-42af-400c-b2b1-d0e0950e009b"
   },
   "outputs": [],
   "source": [
    "x_train = train_images[..., np.newaxis]\n",
    "x_test = test_images[..., np.newaxis]\n",
    "\n",
    "y_train = train_labels_fine\n",
    "y_test = test_labels_fine\n",
    "\n",
    "y_c1_train = train_labels_c1\n",
    "y_c1_test = test_labels_c1\n",
    "\n",
    "y_c2_train = train_labels_c2\n",
    "y_c2_test = test_labels_c2\n",
    "\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"y_c1_train shape: \", y_c1_train.shape)\n",
    "print(\"y_c1_test shape: \", y_c1_test.shape)\n",
    "print(\"y_c2_train shape: \", y_c2_train.shape)\n",
    "print(\"y_c2_test shape: \", y_c2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crjqA1ulxOYg"
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "  learning_rate_init = 0.001\n",
    "  if epoch > 42:\n",
    "    learning_rate_init = 0.0002\n",
    "  if epoch > 52:\n",
    "    learning_rate_init = 0.00005\n",
    "  return learning_rate_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdFhe_pHxRmO"
   },
   "outputs": [],
   "source": [
    "class LossWeightsModifier(keras.callbacks.Callback):\n",
    "  def __init__(self, alpha, beta, gamma):\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "    # customize your behavior\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if epoch == 15:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.8)\n",
    "      K.set_value(self.gamma, 0.1)\n",
    "    if epoch == 25:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.2)\n",
    "      K.set_value(self.gamma, 0.7)\n",
    "    if epoch == 35:\n",
    "      K.set_value(self.alpha, 0)\n",
    "      K.set_value(self.beta, 0)\n",
    "      K.set_value(self.gamma, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC0Ua54ZxtYk"
   },
   "source": [
    "# B-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14lietWa1b4t"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vefa3Ra-xy9l",
    "outputId": "bddfb3d2-64a0-4b5f-eef4-ab163261f56d"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#block 1 \n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#block 2 \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 1 \n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax')(c_1_bch)\n",
    "\n",
    "#block 3 \n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 2\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax')(c_2_bch)\n",
    "\n",
    "#block 4 \n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "#block 5\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#fine \n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='hcnn_seo')\n",
    "\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14TPcCNU1xM0",
    "outputId": "eda38907-1d48-4e85-ddf6-19bc186671a0"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_b_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_b_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_b_cnn_c,reca_b_cnn_c,f1_b_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- B-CNN Base C  ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_b_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_b_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_b_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_b_cnn_c)\n",
    "print(\"Recall:\",reca_b_cnn_c)\n",
    "print(\"f1:\",f1_b_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_b_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4nSSIIa2CHK"
   },
   "source": [
    "# BA-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itH0PRLR2NCX"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True\n",
    "\n",
    "# neurons of all dense layers on each branch \n",
    "branch_neurons = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dS1Tb6Ca2NrB",
    "outputId": "b240793a-daa1-4b2a-fdb4-1ec586f2bd70"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#block 1 \n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#block 2 \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 1 \n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "#c_1_pred = Dense(coarse1_classes, activation='softmax')(c_1_bch)\n",
    "\n",
    "#block 3 \n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 2\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "\n",
    "\n",
    "#block 4 \n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "#block 5\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#fine \n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(branch_neurons, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(branch_neurons, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "#-- Att for coarse 1---\n",
    "# Coarse 1\n",
    "sfcn_1_1 = Dense(64, name='fc1_1')(c_1_bch)\n",
    "sfcn_1_1 = Dense(1, name='fc1_2')(sfcn_1_1)\n",
    "# Coarse 2\n",
    "sfcn_1_2 = Dense(64, name='fc1_3')(c_2_bch)\n",
    "sfcn_1_2 = Dense(1, name='fc1_4')(sfcn_1_2)\n",
    "# Fine\n",
    "sfcn_1_3 = Dense(64, name='fc1_5')(x)\n",
    "sfcn_1_3 = Dense(1, name='fc1_6')(sfcn_1_3)\n",
    "\n",
    "score_vector_1 = Concatenate()([sfcn_1_1,sfcn_1_2,sfcn_1_3]) # Score vector 1\n",
    "att_weights_1 = Activation('softmax', name='attention_weights_1')(score_vector_1) # Attention weights 1\n",
    "weightned_sum_1 = Add()([c_1_bch*att_weights_1[0][0],c_2_bch*att_weights_1[0][1],x*att_weights_1[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_1_concat = Concatenate()([c_1_bch,weightned_sum_1])\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar100')(coarse_1_concat)\n",
    "\n",
    "\n",
    "#-- Att for coarse 2---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_2_1 = Dense(64, name='fc2_1')(c_1_bch)\n",
    "sfcn_2_1 = Dense(1, name='fc2_2')(sfcn_2_1)\n",
    "# Coarse 2\n",
    "sfcn_2_2 = Dense(64, name='fc2_3')(c_2_bch)\n",
    "sfcn_2_2 = Dense(1, name='fc2_4')(sfcn_2_2)\n",
    "# Fine\n",
    "sfcn_2_3 = Dense(64, name='fc2_5')(x)\n",
    "sfcn_2_3 = Dense(1, name='fc2_6')(sfcn_2_3)\n",
    "\n",
    "score_vector_2 = Concatenate()([sfcn_2_1,sfcn_2_2,sfcn_2_3]) # Score vector 1\n",
    "att_weights_2 = Activation('softmax', name='attention_weights_2')(score_vector_2) # Attention weights 1\n",
    "weightned_sum_2 = Add()([c_1_bch*att_weights_2[0][0],c_2_bch*att_weights_2[0][1],x*att_weights_2[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_2_concat = Concatenate()([c_2_bch,weightned_sum_2])\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(coarse_2_concat)\n",
    "\n",
    "\n",
    "#-- Att for fine---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_3_1 = Dense(64, name='fc3_1')(c_1_bch)\n",
    "sfcn_3_1 = Dense(1, name='fc3_2')(sfcn_3_1)\n",
    "# Coarse 2\n",
    "sfcn_3_2 = Dense(64, name='fc3_3')(c_2_bch)\n",
    "sfcn_3_2 = Dense(1, name='fc3_4')(sfcn_3_2)\n",
    "# Fine\n",
    "sfcn_3_3 = Dense(64, name='fc3_5')(x)\n",
    "sfcn_3_3 = Dense(1, name='fc3_6')(sfcn_3_3)\n",
    "\n",
    "score_vector_3 = Concatenate()([sfcn_3_1,sfcn_3_2,sfcn_3_3]) # Score vector 1\n",
    "att_weights_3 = Activation('softmax', name='attention_weights_3')(score_vector_3) # Attention weights 1\n",
    "weightned_sum_3 = Add()([c_1_bch*att_weights_3[0][0],c_2_bch*att_weights_3[0][1],x*att_weights_3[0][2]]) # Weightned sum 3\n",
    "\n",
    "# Concat and prediction\n",
    "fine_concat = Concatenate()([x,weightned_sum_3])\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(fine_concat)\n",
    "\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bacnn')\n",
    "\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IEdToWz2i8r",
    "outputId": "d3a20851-70ee-49dd-cacc-413b3c155a51"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_ba_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_ba_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_ba_cnn_c,reca_ba_cnn_c,f1_ba_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- BA-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_ba_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_ba_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_ba_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_ba_cnn_c)\n",
    "print(\"Recall:\",reca_ba_cnn_c)\n",
    "print(\"f1:\",f1_ba_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_ba_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YW_jVc82qY9"
   },
   "source": [
    "# H-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTF6UA4Y2s3S"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seeol_UW2wlD",
    "outputId": "862554e0-75bb-40c2-c03a-82dabeacd937"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#block 1 \n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#block 2 \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 1 \n",
    "c_1_bch_flatt = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch_flatt)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax')(c_1_bch)\n",
    "\n",
    "#block 3 \n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 2\n",
    "c_2_bch_flatt = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch_concat = Concatenate()([c_1_bch_flatt,c_2_bch_flatt]) # Conectivity Pattern\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch_concat)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax')(c_2_bch)\n",
    "\n",
    "#block 4 \n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "#block 5\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#fine \n",
    "x_flatt = Flatten(name='flatten')(x)\n",
    "x = Concatenate()([c_2_bch_concat,x_flatt]) # Conectivity Pattern\n",
    "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='hcnn_zhang')\n",
    "\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVjFACX12_Iz",
    "outputId": "9b4b5387-a4bb-49f5-e08e-e404a3e8b83b"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_h_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_h_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_h_cnn_c,reca_h_cnn_c,f1_h_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- H-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_h_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_h_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_h_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_h_cnn_c)\n",
    "print(\"Recall:\",reca_h_cnn_c)\n",
    "print(\"f1:\",f1_h_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_h_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-net Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#block 1 \n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#block 2 \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 1 \n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(256, activation='relu')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax')(c_1_bch)\n",
    "\n",
    "#block 3 \n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 2\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(256, activation='relu')(c_2_bch)\n",
    "c_2_bch_out = Add()([c_2_bch,c_1_bch_out])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax')(c_2_bch)\n",
    "\n",
    "#block 4 \n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "#block 5\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#fine \n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Add()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='addnet_fashion')\n",
    "\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_addnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_addnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_addnet_c,reca_addnet_c,f1_addnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Add-net Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_addnet_c[4])\n",
    "print(\"Accuracy level 2:\",score_addnet_c[5])\n",
    "print(\"Accuracy level 3:\",score_addnet_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_addnet_c)\n",
    "print(\"Recall:\",reca_addnet_c)\n",
    "print(\"f1:\",f1_addnet_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_addnet_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat-net Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#block 1 \n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#block 2 \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 1 \n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(512, activation='relu')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax')(c_1_bch)\n",
    "\n",
    "#block 3 \n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#coarse 2\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu')(c_2_bch)\n",
    "c_2_bch_out = Concatenate()([c_2_bch,c_1_bch_out])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax')(c_2_bch)\n",
    "\n",
    "#block 4 \n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "#block 5\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#fine \n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = Concatenate()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='concatnet_fashion')\n",
    "\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_concatnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_concatnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_concatnet_c,reca_concatnet_c,f1_concatnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Concat-net Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_concatnet_c[4])\n",
    "print(\"Accuracy level 2:\",score_concatnet_c[5])\n",
    "print(\"Accuracy level 3:\",score_concatnet_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_concatnet_c)\n",
    "print(\"Recall:\",reca_concatnet_c)\n",
    "print(\"f1:\",f1_concatnet_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_concatnet_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeSabpqV3DYK"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "nhqFYqEa3ExA",
    "outputId": "2b8fbe1b-2375-4505-f95b-b77dbfa06334"
   },
   "outputs": [],
   "source": [
    "summary = {'':['B-CNN Base C','BA-CNN Base C ','H-CNN Base C','Add-net Base C','Concat-net Base C'],'Coarse 1': [score_b_cnn_c[4],score_ba_cnn_c[4],score_h_cnn_c[4],score_addnet_c[4],score_concatnet_c[4]],'Coarse 2': [score_b_cnn_c[5],score_ba_cnn_c[5],score_h_cnn_c[5],score_addnet_c[5],score_concatnet_c[5]],'Fine': [score_b_cnn_c[6],score_ba_cnn_c[6],score_h_cnn_c[6],score_addnet_c[6],score_concatnet_c[6]],'Precision':[preci_b_cnn_c,preci_ba_cnn_c,preci_h_cnn_c,preci_addnet_c,preci_concatnet_c],'Recall':[reca_b_cnn_c,reca_ba_cnn_c,reca_h_cnn_c,reca_addnet_c,reca_concatnet_c],'f1':[f1_b_cnn_c,f1_ba_cnn_c,f1_h_cnn_c,f1_addnet_c,f1_concatnet_c],'Parameters': [parameters_b_cnn_c,parameters_ba_cnn_c,parameters_h_cnn_c,parameters_addnet_c,parameters_concatnet_c]}\n",
    "summary = pd.DataFrame(summary)\n",
    "summary['Parameters'] = (summary['Parameters'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
    "summary = summary.set_index('')\n",
    "summary.style.highlight_max()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BA_CNN_FASHION_MNIST.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
