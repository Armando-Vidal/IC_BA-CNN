{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvPb0YJNU1RM"
   },
   "source": [
    "\n",
    "<H3 align='center'> An Attention-Based Architecture for\n",
    "Hierarchical Classification with CNNs </H3>\n",
    "\n",
    "<H5 align='center'> CIFAR-10 </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CFPEBlEDW1R"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vb27tNtvUuKD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 18:19:26.898701: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Concatenate, Add, Softmax\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "from keras.initializers import he_normal\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, CSVLogger\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from scipy import stats\n",
    "\n",
    "# Computes hierarchical metrics proposed by Kiritchenko et al (2005)\n",
    "def hierarchical_metrics(true,pred):\n",
    "  true_labels = []\n",
    "  true_fine = true[2].argmax(axis=1)\n",
    "  true_c2 = true[1].argmax(axis=1)\n",
    "  true_c1 = true[0].argmax(axis=1)\n",
    "  for i in range(len(true_fine)):\n",
    "    true_labels.append([true_c1[i],true_c2[i],true_fine[i]])\n",
    "  pred_labels = []\n",
    "  pred_c1 = pred[0].argmax(axis = 1)\n",
    "  pred_c2 = pred[1].argmax(axis = 1)\n",
    "  pred_fine = pred[2].argmax(axis = 1) \n",
    "  for i in range(len(pred_c1)):\n",
    "    pred_labels.append([pred_c1[i],pred_c2[i],pred_fine[i]])\n",
    "  preci = precision(true_labels,pred_labels)\n",
    "  reca = recall(true_labels,pred_labels)\n",
    "  f_1 = f1(true_labels,pred_labels)\n",
    "  return preci,reca,f_1\n",
    "\n",
    "# Hierarchical metrics, proposed by Kiritchenko et al (2005)\n",
    "# Implementation \n",
    "# https://gitlab.com/dacs-hpi/hiclass/-/blob/main/hiclass/metrics.py\n",
    "\n",
    "\n",
    "def precision(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute precision score for hierarchical classification.\n",
    "\n",
    "    hP = sum(|S intersection T|) / sum(|S|),\n",
    "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
    "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        What proportion of positive identifications was actually correct?\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    sum_intersection = 0\n",
    "    sum_prediction_and_ancestors = 0\n",
    "    for ground_truth, prediction in zip(y_true, y_pred):\n",
    "        sum_intersection = sum_intersection + len(\n",
    "            set(ground_truth).intersection(set(prediction))\n",
    "        )\n",
    "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
    "            set(prediction)\n",
    "        )\n",
    "    precision = sum_intersection / sum_prediction_and_ancestors\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute recall score for hierarchical classification.\n",
    "\n",
    "    hR = sum(|S intersection T|) / sum(|T|),\n",
    "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
    "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        What proportion of actual positives was identified correctly?\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    sum_intersection = 0\n",
    "    sum_prediction_and_ancestors = 0\n",
    "    for ground_truth, prediction in zip(y_true, y_pred):\n",
    "        sum_intersection = sum_intersection + len(\n",
    "            set(ground_truth).intersection(set(prediction))\n",
    "        )\n",
    "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
    "            set(ground_truth)\n",
    "        )\n",
    "    recall = sum_intersection / sum_prediction_and_ancestors\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute f1 score for hierarchical classification.\n",
    "\n",
    "    hF = 2 * hP * hR / (hP + hR),\n",
    "    where hP is the hierarchical precision and hR is the hierarchical recall.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Weighted average of the precision and recall\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFr3aR_HvlFA"
   },
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V2EC6Gn0vnZs"
   },
   "outputs": [],
   "source": [
    "#-------- dimensions ---------\n",
    "img_rows, img_cols = 32, 32\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "#-----------------------------\n",
    "\n",
    "train_size = 50000\n",
    "\n",
    "#--- coarse 1 classes ---\n",
    "num_c_1 = 2\n",
    "#--- coarse 2 classes ---\n",
    "num_c_2 = 7\n",
    "#--- fine classes ---\n",
    "num_classes  = 10\n",
    "\n",
    "batch_size   = 128\n",
    "epochs       = 60  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mhgnNBz7P5h9"
   },
   "outputs": [],
   "source": [
    "#-------------------- data loading ----------------------\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_cm = y_test\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#---------------- data preprocessing -------------------\n",
    "x_train = (x_train-np.mean(x_train)) / np.std(x_train)\n",
    "x_test = (x_test-np.mean(x_test)) / np.std(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FvrF29rSwGMB"
   },
   "outputs": [],
   "source": [
    "#---------------------- make coarse 2 labels --------------------------\n",
    "parent_f = {\n",
    "  2:3, 3:5, 5:5,\n",
    "  1:2, 7:6, 4:6,\n",
    "  0:0, 6:4, 8:1, 9:2\n",
    "}\n",
    "\n",
    "y_c2_train = np.zeros((y_train.shape[0], num_c_2)).astype(\"float32\")\n",
    "y_c2_test = np.zeros((y_test.shape[0], num_c_2)).astype(\"float32\")\n",
    "for i in range(y_c2_train.shape[0]):\n",
    "  y_c2_train[i][parent_f[np.argmax(y_train[i])]] = 1.0\n",
    "for i in range(y_c2_test.shape[0]):\n",
    "  y_c2_test[i][parent_f[np.argmax(y_test[i])]] = 1.0\n",
    "\n",
    "#---------------------- make coarse 1 labels --------------------------\n",
    "parent_c2 = {\n",
    "  0:0, 1:0, 2:0,\n",
    "  3:1, 4:1, 5:1, 6:1\n",
    "}\n",
    "y_c1_train = np.zeros((y_c2_train.shape[0], num_c_1)).astype(\"float32\")\n",
    "y_c1_test = np.zeros((y_c2_test.shape[0], num_c_1)).astype(\"float32\")\n",
    "for i in range(y_c1_train.shape[0]):\n",
    "  y_c1_train[i][parent_c2[np.argmax(y_c2_train[i])]] = 1.0\n",
    "for i in range(y_c1_test.shape[0]):\n",
    "  y_c1_test[i][parent_c2[np.argmax(y_c2_test[i])]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UCJ4bhapPP-Y"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "def scheduler(epoch):\n",
    "  learning_rate_init = 0.003\n",
    "  if epoch > 42:\n",
    "    learning_rate_init = 0.0005\n",
    "  if epoch > 52:\n",
    "    learning_rate_init = 0.0001\n",
    "  return learning_rate_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bmCQrJtYwa87"
   },
   "outputs": [],
   "source": [
    "# Loss Weights modifier, when BT-strategy is used\n",
    "class LossWeightsModifier(keras.callbacks.Callback):\n",
    "  def __init__(self, alpha, beta, gamma):\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if epoch == 10:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.8)\n",
    "      K.set_value(self.gamma, 0.1)\n",
    "    if epoch == 20:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.2)\n",
    "      K.set_value(self.gamma, 0.7)\n",
    "    if epoch == 30:\n",
    "      K.set_value(self.alpha, 0)\n",
    "      K.set_value(self.beta, 0)\n",
    "      K.set_value(self.gamma, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_n23WmqWm0r"
   },
   "source": [
    "# Flat CNN Base B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BrSqH3ZSQT5L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 18:19:31.888611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:31.928027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:31.929965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:31.931972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-23 18:19:31.932446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:31.934175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:31.935848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:32.632113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:32.633752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:32.635114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-23 18:19:32.636440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13596 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, fine_pred, name='flat_cnn_base_b')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHyicCv-XEmK",
    "outputId": "9f6430bb-af95-4258-f59e-21171245c83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 18:19:36.366241: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "2022-08-23 18:19:37.107974: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-08-23 18:19:37.108490: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-08-23 18:19:37.108518: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-08-23 18:19:37.109064: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-08-23 18:19:37.109123: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 21s 44ms/step - loss: 1.9148 - accuracy: 0.3831 - val_loss: 1.6674 - val_accuracy: 0.3932 - lr: 0.0030\n",
      "Epoch 2/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.2117 - accuracy: 0.5704 - val_loss: 1.0248 - val_accuracy: 0.6334 - lr: 0.0030\n",
      "Epoch 3/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.9513 - accuracy: 0.6653 - val_loss: 0.8800 - val_accuracy: 0.6916 - lr: 0.0030\n",
      "Epoch 4/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.7813 - accuracy: 0.7243 - val_loss: 0.8282 - val_accuracy: 0.7072 - lr: 0.0030\n",
      "Epoch 5/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.6538 - accuracy: 0.7707 - val_loss: 0.7299 - val_accuracy: 0.7546 - lr: 0.0030\n",
      "Epoch 6/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5446 - accuracy: 0.8084 - val_loss: 0.6852 - val_accuracy: 0.7684 - lr: 0.0030\n",
      "Epoch 7/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4534 - accuracy: 0.8411 - val_loss: 0.6957 - val_accuracy: 0.7702 - lr: 0.0030\n",
      "Epoch 8/60\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3647 - accuracy: 0.8734 - val_loss: 0.7417 - val_accuracy: 0.7673 - lr: 0.0030\n",
      "Epoch 9/60\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.2897 - accuracy: 0.8982 - val_loss: 0.7336 - val_accuracy: 0.7815 - lr: 0.0030\n",
      "Epoch 10/60\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.2264 - accuracy: 0.9194 - val_loss: 0.8483 - val_accuracy: 0.7708 - lr: 0.0030\n",
      "Epoch 11/60\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1743 - accuracy: 0.9384 - val_loss: 0.8698 - val_accuracy: 0.7736 - lr: 0.0030\n",
      "Epoch 12/60\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1239 - accuracy: 0.9562 - val_loss: 0.8525 - val_accuracy: 0.7904 - lr: 0.0030\n",
      "Epoch 13/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1037 - accuracy: 0.9650 - val_loss: 0.9391 - val_accuracy: 0.7875 - lr: 0.0030\n",
      "Epoch 14/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0917 - accuracy: 0.9685 - val_loss: 0.9249 - val_accuracy: 0.7881 - lr: 0.0030\n",
      "Epoch 15/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0693 - accuracy: 0.9755 - val_loss: 0.9398 - val_accuracy: 0.7862 - lr: 0.0030\n",
      "Epoch 16/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0604 - accuracy: 0.9793 - val_loss: 0.9338 - val_accuracy: 0.8009 - lr: 0.0030\n",
      "Epoch 17/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0459 - accuracy: 0.9842 - val_loss: 0.9937 - val_accuracy: 0.7963 - lr: 0.0030\n",
      "Epoch 18/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0401 - accuracy: 0.9863 - val_loss: 0.9990 - val_accuracy: 0.7966 - lr: 0.0030\n",
      "Epoch 19/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0350 - accuracy: 0.9888 - val_loss: 1.0425 - val_accuracy: 0.7963 - lr: 0.0030\n",
      "Epoch 20/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0315 - accuracy: 0.9892 - val_loss: 1.1153 - val_accuracy: 0.7971 - lr: 0.0030\n",
      "Epoch 21/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0307 - accuracy: 0.9897 - val_loss: 1.0760 - val_accuracy: 0.8023 - lr: 0.0030\n",
      "Epoch 22/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0273 - accuracy: 0.9908 - val_loss: 1.0739 - val_accuracy: 0.7964 - lr: 0.0030\n",
      "Epoch 23/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 1.1149 - val_accuracy: 0.7921 - lr: 0.0030\n",
      "Epoch 24/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0191 - accuracy: 0.9935 - val_loss: 1.0614 - val_accuracy: 0.8003 - lr: 0.0030\n",
      "Epoch 25/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0204 - accuracy: 0.9936 - val_loss: 1.1055 - val_accuracy: 0.7991 - lr: 0.0030\n",
      "Epoch 26/60\n",
      "391/391 [==============================] - 15s 40ms/step - loss: 0.0231 - accuracy: 0.9923 - val_loss: 1.1163 - val_accuracy: 0.7993 - lr: 0.0030\n",
      "Epoch 27/60\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 1.1102 - val_accuracy: 0.8057 - lr: 0.0030\n",
      "Epoch 28/60\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 0.0135 - accuracy: 0.9956 - val_loss: 1.1393 - val_accuracy: 0.8046 - lr: 0.0030\n",
      "Epoch 29/60\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 0.0111 - accuracy: 0.9967 - val_loss: 1.1152 - val_accuracy: 0.8101 - lr: 0.0030\n",
      "Epoch 30/60\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 0.0103 - accuracy: 0.9966 - val_loss: 1.0998 - val_accuracy: 0.8071 - lr: 0.0030\n",
      "Epoch 31/60\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 0.0141 - accuracy: 0.9956 - val_loss: 1.1881 - val_accuracy: 0.8011 - lr: 0.0030\n",
      "Epoch 32/60\n",
      "391/391 [==============================] - 15s 40ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 1.1081 - val_accuracy: 0.8123 - lr: 0.0030\n",
      "Epoch 33/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 1.1596 - val_accuracy: 0.7997 - lr: 0.0030\n",
      "Epoch 34/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 1.1349 - val_accuracy: 0.8111 - lr: 0.0030\n",
      "Epoch 35/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 1.1513 - val_accuracy: 0.8079 - lr: 0.0030\n",
      "Epoch 36/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 1.1548 - val_accuracy: 0.8082 - lr: 0.0030\n",
      "Epoch 37/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 1.1601 - val_accuracy: 0.8112 - lr: 0.0030\n",
      "Epoch 38/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 1.1845 - val_accuracy: 0.8106 - lr: 0.0030\n",
      "Epoch 39/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 1.1650 - val_accuracy: 0.8126 - lr: 0.0030\n",
      "Epoch 40/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 1.1790 - val_accuracy: 0.8130 - lr: 0.0030\n",
      "Epoch 41/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 1.1577 - val_accuracy: 0.8146 - lr: 0.0030\n",
      "Epoch 42/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 1.1317 - val_accuracy: 0.8197 - lr: 0.0030\n",
      "Epoch 43/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 1.1621 - val_accuracy: 0.8138 - lr: 0.0030\n",
      "Epoch 44/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 1.1183 - val_accuracy: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 45/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.1104 - val_accuracy: 0.8207 - lr: 5.0000e-04\n",
      "Epoch 46/60\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 9.3698e-04 - accuracy: 0.9998 - val_loss: 1.1102 - val_accuracy: 0.8204 - lr: 5.0000e-04\n",
      "Epoch 47/60\n",
      "391/391 [==============================] - 15s 40ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 1.1060 - val_accuracy: 0.8199 - lr: 5.0000e-04\n",
      "Epoch 48/60\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 7.2030e-04 - accuracy: 0.9998 - val_loss: 1.1021 - val_accuracy: 0.8209 - lr: 5.0000e-04\n",
      "Epoch 49/60\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 6.2924e-04 - accuracy: 1.0000 - val_loss: 1.1030 - val_accuracy: 0.8218 - lr: 5.0000e-04\n",
      "Epoch 50/60\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 9.7080e-04 - accuracy: 0.9997 - val_loss: 1.1021 - val_accuracy: 0.8213 - lr: 5.0000e-04\n",
      "Epoch 51/60\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 8.2839e-04 - accuracy: 0.9998 - val_loss: 1.0996 - val_accuracy: 0.8213 - lr: 5.0000e-04\n",
      "Epoch 52/60\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 5.1823e-04 - accuracy: 1.0000 - val_loss: 1.0989 - val_accuracy: 0.8212 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/60\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 5.7415e-04 - accuracy: 0.9999 - val_loss: 1.0996 - val_accuracy: 0.8213 - lr: 5.0000e-04\n",
      "Epoch 54/60\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 5.8990e-04 - accuracy: 0.9999 - val_loss: 1.1000 - val_accuracy: 0.8213 - lr: 1.0000e-04\n",
      "Epoch 55/60\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 6.2279e-04 - accuracy: 0.9999 - val_loss: 1.0993 - val_accuracy: 0.8214 - lr: 1.0000e-04\n",
      "Epoch 56/60\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 4.9108e-04 - accuracy: 0.9999 - val_loss: 1.0990 - val_accuracy: 0.8221 - lr: 1.0000e-04\n",
      "Epoch 57/60\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 5.5957e-04 - accuracy: 0.9999 - val_loss: 1.0985 - val_accuracy: 0.8223 - lr: 1.0000e-04\n",
      "Epoch 58/60\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 4.9711e-04 - accuracy: 0.9999 - val_loss: 1.0967 - val_accuracy: 0.8219 - lr: 1.0000e-04\n",
      "Epoch 59/60\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 5.9978e-04 - accuracy: 0.9999 - val_loss: 1.0981 - val_accuracy: 0.8219 - lr: 1.0000e-04\n",
      "Epoch 60/60\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 4.9997e-04 - accuracy: 1.0000 - val_loss: 1.0963 - val_accuracy: 0.8222 - lr: 1.0000e-04\n",
      "--- Flat CNN Base B ---\n",
      "Accuracy: 0.8222000002861023\n",
      "Parameters: 7,851,338\n"
     ]
    }
   ],
   "source": [
    "change_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Training\n",
    "history_base_b = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=change_lr,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation on test set\n",
    "score_base_b = model.evaluate(x_test, y_test, verbose=0)\n",
    "parameters_base_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "# Results\n",
    "print(\"--- Flat CNN Base B ---\")\n",
    "print(\"Accuracy:\",score_base_b[1])\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_base_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4isnsDMcKKm"
   },
   "source": [
    "# B-CNN Base B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sMGtDO4tdxoN"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UnKGZz_zOss",
    "outputId": "a68495c5-b1df-41cf-e9c0-135e3194f22a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bcnn_base_b\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 32, 32, 64)   1792        ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 64)  256         ['block1_conv1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 32, 32, 64)   36928       ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 64)  256         ['block1_conv2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 16, 16, 64)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 16, 16, 128)  73856       ['block1_pool[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 128)  512        ['block2_conv1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 16, 16, 128)  147584      ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 128)  512        ['block2_conv2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 8, 8, 128)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 8, 8, 256)    295168      ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 256)   1024        ['block3_conv1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 8, 8, 256)    590080      ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 256)   1024        ['block3_conv2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 4, 4, 256)    0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 4, 4, 512)    1180160     ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 4, 4, 512)   2048        ['block4_conv1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 4, 4, 512)    2359808     ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 4, 4, 512)   2048        ['block4_conv2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 2, 2, 512)    0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " c1_flatten (Flatten)           (None, 8192)         0           ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " c2_flatten (Flatten)           (None, 4096)         0           ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['block4_pool[0][0]']            \n",
      "                                                                                                  \n",
      " c1_fc_cifar10_1 (Dense)        (None, 256)          2097408     ['c1_flatten[0][0]']             \n",
      "                                                                                                  \n",
      " c2_fc_cifar10_1 (Dense)        (None, 512)          2097664     ['c2_flatten[0][0]']             \n",
      "                                                                                                  \n",
      " fc_cifar10_1 (Dense)           (None, 1024)         2098176     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 256)         1024        ['c1_fc_cifar10_1[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 512)         2048        ['c2_fc_cifar10_1[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 1024)        4096        ['fc_cifar10_1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 512)          0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 1024)         0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " c1_fc2 (Dense)                 (None, 256)          65792       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " c2_fc2 (Dense)                 (None, 512)          262656      ['dropout_4[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 1024)         1049600     ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 256)         1024        ['c1_fc2[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 512)         2048        ['c2_fc2[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 1024)        4096        ['fc2[0][0]']                    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 512)          0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 1024)         0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " c1_predictions_cifar10 (Dense)  (None, 2)           514         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " c2_predictions_cifar10 (Dense)  (None, 7)           3591        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " predictions_cifar10 (Dense)    (None, 10)           10250       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,393,043\n",
      "Trainable params: 12,382,035\n",
      "Non-trainable params: 11,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bcnn_base_b')\n",
    "\n",
    "#----------------------- compile  ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5J93jPue6Kr",
    "outputId": "29e47a08-fb30-4d7a-f717-aec526defd8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "  6/391 [..............................] - ETA: 20s - loss: 1.0634 - c1_predictions_cifar10_loss: 1.0119 - c2_predictions_cifar10_loss: 3.3220 - predictions_cifar10_loss: 3.8488 - c1_predictions_cifar10_accuracy: 0.6003 - c2_predictions_cifar10_accuracy: 0.1549 - predictions_cifar10_accuracy: 0.0859WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0238s vs `on_train_batch_end` time: 0.0295s). Check your callbacks.\n",
      "391/391 [==============================] - 21s 48ms/step - loss: 0.3714 - c1_predictions_cifar10_loss: 0.3227 - c2_predictions_cifar10_loss: 2.5402 - predictions_cifar10_loss: 2.9750 - c1_predictions_cifar10_accuracy: 0.8763 - c2_predictions_cifar10_accuracy: 0.2669 - predictions_cifar10_accuracy: 0.2030 - val_loss: 0.3560 - val_c1_predictions_cifar10_loss: 0.3261 - val_c2_predictions_cifar10_loss: 1.6178 - val_predictions_cifar10_loss: 2.0199 - val_c1_predictions_cifar10_accuracy: 0.8615 - val_c2_predictions_cifar10_accuracy: 0.3728 - val_predictions_cifar10_accuracy: 0.2783 - lr: 0.0030\n",
      "Epoch 2/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.2450 - c1_predictions_cifar10_loss: 0.2049 - c2_predictions_cifar10_loss: 1.9863 - predictions_cifar10_loss: 2.4337 - c1_predictions_cifar10_accuracy: 0.9188 - c2_predictions_cifar10_accuracy: 0.3900 - predictions_cifar10_accuracy: 0.3014 - val_loss: 0.2131 - val_c1_predictions_cifar10_loss: 0.1877 - val_c2_predictions_cifar10_loss: 1.2730 - val_predictions_cifar10_loss: 1.6343 - val_c1_predictions_cifar10_accuracy: 0.9234 - val_c2_predictions_cifar10_accuracy: 0.5387 - val_predictions_cifar10_accuracy: 0.4312 - lr: 0.0030\n",
      "Epoch 3/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.2152 - c1_predictions_cifar10_loss: 0.1788 - c2_predictions_cifar10_loss: 1.7789 - predictions_cifar10_loss: 2.2243 - c1_predictions_cifar10_accuracy: 0.9298 - c2_predictions_cifar10_accuracy: 0.4475 - predictions_cifar10_accuracy: 0.3461 - val_loss: 0.1876 - val_c1_predictions_cifar10_loss: 0.1638 - val_c2_predictions_cifar10_loss: 1.1889 - val_predictions_cifar10_loss: 1.5241 - val_c1_predictions_cifar10_accuracy: 0.9333 - val_c2_predictions_cifar10_accuracy: 0.5669 - val_predictions_cifar10_accuracy: 0.4603 - lr: 0.0030\n",
      "Epoch 4/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1928 - c1_predictions_cifar10_loss: 0.1589 - c2_predictions_cifar10_loss: 1.6396 - predictions_cifar10_loss: 2.0644 - c1_predictions_cifar10_accuracy: 0.9376 - c2_predictions_cifar10_accuracy: 0.4804 - predictions_cifar10_accuracy: 0.3821 - val_loss: 0.1802 - val_c1_predictions_cifar10_loss: 0.1575 - val_c2_predictions_cifar10_loss: 1.1232 - val_predictions_cifar10_loss: 1.4631 - val_c1_predictions_cifar10_accuracy: 0.9374 - val_c2_predictions_cifar10_accuracy: 0.5880 - val_predictions_cifar10_accuracy: 0.4849 - lr: 0.0030\n",
      "Epoch 5/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1773 - c1_predictions_cifar10_loss: 0.1451 - c2_predictions_cifar10_loss: 1.5451 - predictions_cifar10_loss: 1.9620 - c1_predictions_cifar10_accuracy: 0.9430 - c2_predictions_cifar10_accuracy: 0.5059 - predictions_cifar10_accuracy: 0.4055 - val_loss: 0.1736 - val_c1_predictions_cifar10_loss: 0.1518 - val_c2_predictions_cifar10_loss: 1.0878 - val_predictions_cifar10_loss: 1.3987 - val_c1_predictions_cifar10_accuracy: 0.9387 - val_c2_predictions_cifar10_accuracy: 0.6116 - val_predictions_cifar10_accuracy: 0.5115 - lr: 0.0030\n",
      "Epoch 6/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1652 - c1_predictions_cifar10_loss: 0.1345 - c2_predictions_cifar10_loss: 1.4680 - predictions_cifar10_loss: 1.8659 - c1_predictions_cifar10_accuracy: 0.9480 - c2_predictions_cifar10_accuracy: 0.5287 - predictions_cifar10_accuracy: 0.4274 - val_loss: 0.1836 - val_c1_predictions_cifar10_loss: 0.1631 - val_c2_predictions_cifar10_loss: 1.0464 - val_predictions_cifar10_loss: 1.3326 - val_c1_predictions_cifar10_accuracy: 0.9355 - val_c2_predictions_cifar10_accuracy: 0.6205 - val_predictions_cifar10_accuracy: 0.5328 - lr: 0.0030\n",
      "Epoch 7/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1525 - c1_predictions_cifar10_loss: 0.1232 - c2_predictions_cifar10_loss: 1.4018 - predictions_cifar10_loss: 1.7705 - c1_predictions_cifar10_accuracy: 0.9533 - c2_predictions_cifar10_accuracy: 0.5429 - predictions_cifar10_accuracy: 0.4497 - val_loss: 0.1588 - val_c1_predictions_cifar10_loss: 0.1384 - val_c2_predictions_cifar10_loss: 1.0140 - val_predictions_cifar10_loss: 1.3012 - val_c1_predictions_cifar10_accuracy: 0.9454 - val_c2_predictions_cifar10_accuracy: 0.6387 - val_predictions_cifar10_accuracy: 0.5425 - lr: 0.0030\n",
      "Epoch 8/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1414 - c1_predictions_cifar10_loss: 0.1132 - c2_predictions_cifar10_loss: 1.3405 - predictions_cifar10_loss: 1.7030 - c1_predictions_cifar10_accuracy: 0.9574 - c2_predictions_cifar10_accuracy: 0.5565 - predictions_cifar10_accuracy: 0.4664 - val_loss: 0.1599 - val_c1_predictions_cifar10_loss: 0.1401 - val_c2_predictions_cifar10_loss: 0.9840 - val_predictions_cifar10_loss: 1.2765 - val_c1_predictions_cifar10_accuracy: 0.9462 - val_c2_predictions_cifar10_accuracy: 0.6516 - val_predictions_cifar10_accuracy: 0.5540 - lr: 0.0030\n",
      "Epoch 9/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1304 - c1_predictions_cifar10_loss: 0.1033 - c2_predictions_cifar10_loss: 1.2931 - predictions_cifar10_loss: 1.6304 - c1_predictions_cifar10_accuracy: 0.9609 - c2_predictions_cifar10_accuracy: 0.5736 - predictions_cifar10_accuracy: 0.4866 - val_loss: 0.1529 - val_c1_predictions_cifar10_loss: 0.1340 - val_c2_predictions_cifar10_loss: 0.9372 - val_predictions_cifar10_loss: 1.2196 - val_c1_predictions_cifar10_accuracy: 0.9472 - val_c2_predictions_cifar10_accuracy: 0.6570 - val_predictions_cifar10_accuracy: 0.5653 - lr: 0.0030\n",
      "Epoch 10/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1182 - c1_predictions_cifar10_loss: 0.0919 - c2_predictions_cifar10_loss: 1.2432 - predictions_cifar10_loss: 1.5669 - c1_predictions_cifar10_accuracy: 0.9653 - c2_predictions_cifar10_accuracy: 0.5879 - predictions_cifar10_accuracy: 0.4989 - val_loss: 0.1509 - val_c1_predictions_cifar10_loss: 0.1329 - val_c2_predictions_cifar10_loss: 0.8967 - val_predictions_cifar10_loss: 1.1721 - val_c1_predictions_cifar10_accuracy: 0.9505 - val_c2_predictions_cifar10_accuracy: 0.6755 - val_predictions_cifar10_accuracy: 0.5880 - lr: 0.0030\n",
      "Epoch 11/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1139 - c1_predictions_cifar10_loss: 0.0885 - c2_predictions_cifar10_loss: 1.2000 - predictions_cifar10_loss: 1.5168 - c1_predictions_cifar10_accuracy: 0.9668 - c2_predictions_cifar10_accuracy: 0.6004 - predictions_cifar10_accuracy: 0.5129 - val_loss: 0.1504 - val_c1_predictions_cifar10_loss: 0.1327 - val_c2_predictions_cifar10_loss: 0.8860 - val_predictions_cifar10_loss: 1.1518 - val_c1_predictions_cifar10_accuracy: 0.9501 - val_c2_predictions_cifar10_accuracy: 0.6828 - val_predictions_cifar10_accuracy: 0.5872 - lr: 0.0030\n",
      "Epoch 12/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.9317 - c1_predictions_cifar10_loss: 0.1227 - c2_predictions_cifar10_loss: 0.9645 - predictions_cifar10_loss: 1.4778 - c1_predictions_cifar10_accuracy: 0.9530 - c2_predictions_cifar10_accuracy: 0.6661 - predictions_cifar10_accuracy: 0.5269 - val_loss: 0.7189 - val_c1_predictions_cifar10_loss: 0.1293 - val_c2_predictions_cifar10_loss: 0.7478 - val_predictions_cifar10_loss: 1.0776 - val_c1_predictions_cifar10_accuracy: 0.9483 - val_c2_predictions_cifar10_accuracy: 0.7303 - val_predictions_cifar10_accuracy: 0.6304 - lr: 0.0030\n",
      "Epoch 13/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.6562 - c1_predictions_cifar10_loss: 0.1094 - c2_predictions_cifar10_loss: 0.6649 - predictions_cifar10_loss: 1.1336 - c1_predictions_cifar10_accuracy: 0.9572 - c2_predictions_cifar10_accuracy: 0.7643 - predictions_cifar10_accuracy: 0.6204 - val_loss: 0.5744 - val_c1_predictions_cifar10_loss: 0.1273 - val_c2_predictions_cifar10_loss: 0.5932 - val_predictions_cifar10_loss: 0.8707 - val_c1_predictions_cifar10_accuracy: 0.9492 - val_c2_predictions_cifar10_accuracy: 0.7837 - val_predictions_cifar10_accuracy: 0.6879 - lr: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.5472 - c1_predictions_cifar10_loss: 0.1010 - c2_predictions_cifar10_loss: 0.5525 - predictions_cifar10_loss: 0.9511 - c1_predictions_cifar10_accuracy: 0.9602 - c2_predictions_cifar10_accuracy: 0.8018 - predictions_cifar10_accuracy: 0.6746 - val_loss: 0.5166 - val_c1_predictions_cifar10_loss: 0.1224 - val_c2_predictions_cifar10_loss: 0.5318 - val_predictions_cifar10_loss: 0.7889 - val_c1_predictions_cifar10_accuracy: 0.9506 - val_c2_predictions_cifar10_accuracy: 0.8085 - val_predictions_cifar10_accuracy: 0.7222 - lr: 0.0030\n",
      "Epoch 15/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.4727 - c1_predictions_cifar10_loss: 0.0998 - c2_predictions_cifar10_loss: 0.4739 - predictions_cifar10_loss: 0.8359 - c1_predictions_cifar10_accuracy: 0.9617 - c2_predictions_cifar10_accuracy: 0.8309 - predictions_cifar10_accuracy: 0.7088 - val_loss: 0.4837 - val_c1_predictions_cifar10_loss: 0.1204 - val_c2_predictions_cifar10_loss: 0.4962 - val_predictions_cifar10_loss: 0.7467 - val_c1_predictions_cifar10_accuracy: 0.9525 - val_c2_predictions_cifar10_accuracy: 0.8266 - val_predictions_cifar10_accuracy: 0.7462 - lr: 0.0030\n",
      "Epoch 16/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.4137 - c1_predictions_cifar10_loss: 0.0929 - c2_predictions_cifar10_loss: 0.4136 - predictions_cifar10_loss: 0.7357 - c1_predictions_cifar10_accuracy: 0.9648 - c2_predictions_cifar10_accuracy: 0.8534 - predictions_cifar10_accuracy: 0.7393 - val_loss: 0.4785 - val_c1_predictions_cifar10_loss: 0.1240 - val_c2_predictions_cifar10_loss: 0.4928 - val_predictions_cifar10_loss: 0.7188 - val_c1_predictions_cifar10_accuracy: 0.9506 - val_c2_predictions_cifar10_accuracy: 0.8258 - val_predictions_cifar10_accuracy: 0.7504 - lr: 0.0030\n",
      "Epoch 17/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.3610 - c1_predictions_cifar10_loss: 0.0936 - c2_predictions_cifar10_loss: 0.3567 - predictions_cifar10_loss: 0.6632 - c1_predictions_cifar10_accuracy: 0.9642 - c2_predictions_cifar10_accuracy: 0.8743 - predictions_cifar10_accuracy: 0.7634 - val_loss: 0.4717 - val_c1_predictions_cifar10_loss: 0.1197 - val_c2_predictions_cifar10_loss: 0.4860 - val_predictions_cifar10_loss: 0.7094 - val_c1_predictions_cifar10_accuracy: 0.9538 - val_c2_predictions_cifar10_accuracy: 0.8335 - val_predictions_cifar10_accuracy: 0.7557 - lr: 0.0030\n",
      "Epoch 18/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.3169 - c1_predictions_cifar10_loss: 0.0893 - c2_predictions_cifar10_loss: 0.3098 - predictions_cifar10_loss: 0.6017 - c1_predictions_cifar10_accuracy: 0.9658 - c2_predictions_cifar10_accuracy: 0.8907 - predictions_cifar10_accuracy: 0.7844 - val_loss: 0.4583 - val_c1_predictions_cifar10_loss: 0.1119 - val_c2_predictions_cifar10_loss: 0.4726 - val_predictions_cifar10_loss: 0.6905 - val_c1_predictions_cifar10_accuracy: 0.9564 - val_c2_predictions_cifar10_accuracy: 0.8420 - val_predictions_cifar10_accuracy: 0.7682 - lr: 0.0030\n",
      "Epoch 19/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.2733 - c1_predictions_cifar10_loss: 0.0900 - c2_predictions_cifar10_loss: 0.2630 - predictions_cifar10_loss: 0.5396 - c1_predictions_cifar10_accuracy: 0.9659 - c2_predictions_cifar10_accuracy: 0.9059 - predictions_cifar10_accuracy: 0.8030 - val_loss: 0.4898 - val_c1_predictions_cifar10_loss: 0.1092 - val_c2_predictions_cifar10_loss: 0.5115 - val_predictions_cifar10_loss: 0.6964 - val_c1_predictions_cifar10_accuracy: 0.9581 - val_c2_predictions_cifar10_accuracy: 0.8396 - val_predictions_cifar10_accuracy: 0.7720 - lr: 0.0030\n",
      "Epoch 20/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.2286 - c1_predictions_cifar10_loss: 0.0847 - c2_predictions_cifar10_loss: 0.2145 - predictions_cifar10_loss: 0.4847 - c1_predictions_cifar10_accuracy: 0.9673 - c2_predictions_cifar10_accuracy: 0.9244 - predictions_cifar10_accuracy: 0.8235 - val_loss: 0.5369 - val_c1_predictions_cifar10_loss: 0.1143 - val_c2_predictions_cifar10_loss: 0.5679 - val_predictions_cifar10_loss: 0.7113 - val_c1_predictions_cifar10_accuracy: 0.9548 - val_c2_predictions_cifar10_accuracy: 0.8276 - val_predictions_cifar10_accuracy: 0.7650 - lr: 0.0030\n",
      "Epoch 21/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.2014 - c1_predictions_cifar10_loss: 0.0850 - c2_predictions_cifar10_loss: 0.1854 - predictions_cifar10_loss: 0.4456 - c1_predictions_cifar10_accuracy: 0.9678 - c2_predictions_cifar10_accuracy: 0.9345 - predictions_cifar10_accuracy: 0.8382 - val_loss: 0.4752 - val_c1_predictions_cifar10_loss: 0.1100 - val_c2_predictions_cifar10_loss: 0.4987 - val_predictions_cifar10_loss: 0.6517 - val_c1_predictions_cifar10_accuracy: 0.9579 - val_c2_predictions_cifar10_accuracy: 0.8578 - val_predictions_cifar10_accuracy: 0.7882 - lr: 0.0030\n",
      "Epoch 22/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.3774 - c1_predictions_cifar10_loss: 0.0873 - c2_predictions_cifar10_loss: 0.1841 - predictions_cifar10_loss: 0.4741 - c1_predictions_cifar10_accuracy: 0.9664 - c2_predictions_cifar10_accuracy: 0.9352 - predictions_cifar10_accuracy: 0.8294 - val_loss: 0.6660 - val_c1_predictions_cifar10_loss: 0.1124 - val_c2_predictions_cifar10_loss: 0.5246 - val_predictions_cifar10_loss: 0.7855 - val_c1_predictions_cifar10_accuracy: 0.9564 - val_c2_predictions_cifar10_accuracy: 0.8416 - val_predictions_cifar10_accuracy: 0.7633 - lr: 0.0030\n",
      "Epoch 23/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.2873 - c1_predictions_cifar10_loss: 0.0837 - c2_predictions_cifar10_loss: 0.1637 - predictions_cifar10_loss: 0.3517 - c1_predictions_cifar10_accuracy: 0.9679 - c2_predictions_cifar10_accuracy: 0.9425 - predictions_cifar10_accuracy: 0.8705 - val_loss: 0.6312 - val_c1_predictions_cifar10_loss: 0.1169 - val_c2_predictions_cifar10_loss: 0.5147 - val_predictions_cifar10_loss: 0.7380 - val_c1_predictions_cifar10_accuracy: 0.9541 - val_c2_predictions_cifar10_accuracy: 0.8451 - val_predictions_cifar10_accuracy: 0.7816 - lr: 0.0030\n",
      "Epoch 24/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.2116 - c1_predictions_cifar10_loss: 0.0823 - c2_predictions_cifar10_loss: 0.1341 - predictions_cifar10_loss: 0.2522 - c1_predictions_cifar10_accuracy: 0.9691 - c2_predictions_cifar10_accuracy: 0.9535 - predictions_cifar10_accuracy: 0.9087 - val_loss: 0.5948 - val_c1_predictions_cifar10_loss: 0.1049 - val_c2_predictions_cifar10_loss: 0.4772 - val_predictions_cifar10_loss: 0.6984 - val_c1_predictions_cifar10_accuracy: 0.9597 - val_c2_predictions_cifar10_accuracy: 0.8544 - val_predictions_cifar10_accuracy: 0.7955 - lr: 0.0030\n",
      "Epoch 25/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.1607 - c1_predictions_cifar10_loss: 0.0789 - c2_predictions_cifar10_loss: 0.1156 - predictions_cifar10_loss: 0.1852 - c1_predictions_cifar10_accuracy: 0.9692 - c2_predictions_cifar10_accuracy: 0.9601 - predictions_cifar10_accuracy: 0.9322 - val_loss: 0.5709 - val_c1_predictions_cifar10_loss: 0.1103 - val_c2_predictions_cifar10_loss: 0.4717 - val_predictions_cifar10_loss: 0.6651 - val_c1_predictions_cifar10_accuracy: 0.9574 - val_c2_predictions_cifar10_accuracy: 0.8623 - val_predictions_cifar10_accuracy: 0.8156 - lr: 0.0030\n",
      "Epoch 26/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.1224 - c1_predictions_cifar10_loss: 0.0771 - c2_predictions_cifar10_loss: 0.0991 - predictions_cifar10_loss: 0.1356 - c1_predictions_cifar10_accuracy: 0.9707 - c2_predictions_cifar10_accuracy: 0.9669 - predictions_cifar10_accuracy: 0.9506 - val_loss: 0.6044 - val_c1_predictions_cifar10_loss: 0.1094 - val_c2_predictions_cifar10_loss: 0.4779 - val_predictions_cifar10_loss: 0.7112 - val_c1_predictions_cifar10_accuracy: 0.9584 - val_c2_predictions_cifar10_accuracy: 0.8597 - val_predictions_cifar10_accuracy: 0.8107 - lr: 0.0030\n",
      "Epoch 27/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.0869 - c1_predictions_cifar10_loss: 0.0716 - c2_predictions_cifar10_loss: 0.0845 - predictions_cifar10_loss: 0.0898 - c1_predictions_cifar10_accuracy: 0.9730 - c2_predictions_cifar10_accuracy: 0.9720 - predictions_cifar10_accuracy: 0.9689 - val_loss: 0.7262 - val_c1_predictions_cifar10_loss: 0.1192 - val_c2_predictions_cifar10_loss: 0.5142 - val_predictions_cifar10_loss: 0.8735 - val_c1_predictions_cifar10_accuracy: 0.9539 - val_c2_predictions_cifar10_accuracy: 0.8547 - val_predictions_cifar10_accuracy: 0.7978 - lr: 0.0030\n",
      "Epoch 28/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.0735 - c1_predictions_cifar10_loss: 0.0731 - c2_predictions_cifar10_loss: 0.0781 - predictions_cifar10_loss: 0.0722 - c1_predictions_cifar10_accuracy: 0.9719 - c2_predictions_cifar10_accuracy: 0.9738 - predictions_cifar10_accuracy: 0.9740 - val_loss: 0.7036 - val_c1_predictions_cifar10_loss: 0.1094 - val_c2_predictions_cifar10_loss: 0.5151 - val_predictions_cifar10_loss: 0.8423 - val_c1_predictions_cifar10_accuracy: 0.9592 - val_c2_predictions_cifar10_accuracy: 0.8620 - val_predictions_cifar10_accuracy: 0.8111 - lr: 0.0030\n",
      "Epoch 29/60\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.0580 - c1_predictions_cifar10_loss: 0.0682 - c2_predictions_cifar10_loss: 0.0686 - predictions_cifar10_loss: 0.0535 - c1_predictions_cifar10_accuracy: 0.9745 - c2_predictions_cifar10_accuracy: 0.9770 - predictions_cifar10_accuracy: 0.9817 - val_loss: 0.7112 - val_c1_predictions_cifar10_loss: 0.1046 - val_c2_predictions_cifar10_loss: 0.4936 - val_predictions_cifar10_loss: 0.8600 - val_c1_predictions_cifar10_accuracy: 0.9595 - val_c2_predictions_cifar10_accuracy: 0.8663 - val_predictions_cifar10_accuracy: 0.8112 - lr: 0.0030\n",
      "Epoch 30/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0445 - c1_predictions_cifar10_loss: 0.0671 - c2_predictions_cifar10_loss: 0.0575 - predictions_cifar10_loss: 0.0375 - c1_predictions_cifar10_accuracy: 0.9746 - c2_predictions_cifar10_accuracy: 0.9817 - predictions_cifar10_accuracy: 0.9877 - val_loss: 0.7366 - val_c1_predictions_cifar10_loss: 0.1149 - val_c2_predictions_cifar10_loss: 0.5107 - val_predictions_cifar10_loss: 0.8900 - val_c1_predictions_cifar10_accuracy: 0.9577 - val_c2_predictions_cifar10_accuracy: 0.8620 - val_predictions_cifar10_accuracy: 0.8088 - lr: 0.0030\n",
      "Epoch 31/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0371 - c1_predictions_cifar10_loss: 0.0641 - c2_predictions_cifar10_loss: 0.0514 - predictions_cifar10_loss: 0.0291 - c1_predictions_cifar10_accuracy: 0.9763 - c2_predictions_cifar10_accuracy: 0.9838 - predictions_cifar10_accuracy: 0.9904 - val_loss: 0.7342 - val_c1_predictions_cifar10_loss: 0.1090 - val_c2_predictions_cifar10_loss: 0.5184 - val_predictions_cifar10_loss: 0.8852 - val_c1_predictions_cifar10_accuracy: 0.9603 - val_c2_predictions_cifar10_accuracy: 0.8647 - val_predictions_cifar10_accuracy: 0.8175 - lr: 0.0030\n",
      "Epoch 32/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0944 - c1_predictions_cifar10_loss: 0.0722 - c2_predictions_cifar10_loss: 0.0929 - predictions_cifar10_loss: 0.0944 - c1_predictions_cifar10_accuracy: 0.9720 - c2_predictions_cifar10_accuracy: 0.9671 - predictions_cifar10_accuracy: 0.9674 - val_loss: 1.0664 - val_c1_predictions_cifar10_loss: 0.1266 - val_c2_predictions_cifar10_loss: 0.6304 - val_predictions_cifar10_loss: 1.0664 - val_c1_predictions_cifar10_accuracy: 0.9521 - val_c2_predictions_cifar10_accuracy: 0.8367 - val_predictions_cifar10_accuracy: 0.7785 - lr: 0.0030\n",
      "Epoch 33/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.1503 - c1_predictions_cifar10_loss: 0.0822 - c2_predictions_cifar10_loss: 0.1364 - predictions_cifar10_loss: 0.1503 - c1_predictions_cifar10_accuracy: 0.9685 - c2_predictions_cifar10_accuracy: 0.9521 - predictions_cifar10_accuracy: 0.9476 - val_loss: 0.8289 - val_c1_predictions_cifar10_loss: 0.1217 - val_c2_predictions_cifar10_loss: 0.5575 - val_predictions_cifar10_loss: 0.8289 - val_c1_predictions_cifar10_accuracy: 0.9550 - val_c2_predictions_cifar10_accuracy: 0.8536 - val_predictions_cifar10_accuracy: 0.8086 - lr: 0.0030\n",
      "Epoch 34/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.1000 - c1_predictions_cifar10_loss: 0.0793 - c2_predictions_cifar10_loss: 0.1193 - predictions_cifar10_loss: 0.1000 - c1_predictions_cifar10_accuracy: 0.9693 - c2_predictions_cifar10_accuracy: 0.9580 - predictions_cifar10_accuracy: 0.9659 - val_loss: 0.8959 - val_c1_predictions_cifar10_loss: 0.1179 - val_c2_predictions_cifar10_loss: 0.5713 - val_predictions_cifar10_loss: 0.8959 - val_c1_predictions_cifar10_accuracy: 0.9578 - val_c2_predictions_cifar10_accuracy: 0.8534 - val_predictions_cifar10_accuracy: 0.8005 - lr: 0.0030\n",
      "Epoch 35/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0655 - c1_predictions_cifar10_loss: 0.0781 - c2_predictions_cifar10_loss: 0.1033 - predictions_cifar10_loss: 0.0655 - c1_predictions_cifar10_accuracy: 0.9704 - c2_predictions_cifar10_accuracy: 0.9634 - predictions_cifar10_accuracy: 0.9776 - val_loss: 0.8770 - val_c1_predictions_cifar10_loss: 0.1094 - val_c2_predictions_cifar10_loss: 0.5411 - val_predictions_cifar10_loss: 0.8770 - val_c1_predictions_cifar10_accuracy: 0.9596 - val_c2_predictions_cifar10_accuracy: 0.8549 - val_predictions_cifar10_accuracy: 0.8135 - lr: 0.0030\n",
      "Epoch 36/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0539 - c1_predictions_cifar10_loss: 0.0740 - c2_predictions_cifar10_loss: 0.0979 - predictions_cifar10_loss: 0.0539 - c1_predictions_cifar10_accuracy: 0.9718 - c2_predictions_cifar10_accuracy: 0.9660 - predictions_cifar10_accuracy: 0.9813 - val_loss: 0.8816 - val_c1_predictions_cifar10_loss: 0.1122 - val_c2_predictions_cifar10_loss: 0.5330 - val_predictions_cifar10_loss: 0.8816 - val_c1_predictions_cifar10_accuracy: 0.9576 - val_c2_predictions_cifar10_accuracy: 0.8546 - val_predictions_cifar10_accuracy: 0.8119 - lr: 0.0030\n",
      "Epoch 37/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0368 - c1_predictions_cifar10_loss: 0.0770 - c2_predictions_cifar10_loss: 0.0915 - predictions_cifar10_loss: 0.0368 - c1_predictions_cifar10_accuracy: 0.9715 - c2_predictions_cifar10_accuracy: 0.9673 - predictions_cifar10_accuracy: 0.9876 - val_loss: 0.8799 - val_c1_predictions_cifar10_loss: 0.1117 - val_c2_predictions_cifar10_loss: 0.5170 - val_predictions_cifar10_loss: 0.8799 - val_c1_predictions_cifar10_accuracy: 0.9593 - val_c2_predictions_cifar10_accuracy: 0.8635 - val_predictions_cifar10_accuracy: 0.8221 - lr: 0.0030\n",
      "Epoch 38/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0274 - c1_predictions_cifar10_loss: 0.0739 - c2_predictions_cifar10_loss: 0.0872 - predictions_cifar10_loss: 0.0274 - c1_predictions_cifar10_accuracy: 0.9712 - c2_predictions_cifar10_accuracy: 0.9704 - predictions_cifar10_accuracy: 0.9903 - val_loss: 0.9678 - val_c1_predictions_cifar10_loss: 0.1119 - val_c2_predictions_cifar10_loss: 0.5550 - val_predictions_cifar10_loss: 0.9678 - val_c1_predictions_cifar10_accuracy: 0.9594 - val_c2_predictions_cifar10_accuracy: 0.8510 - val_predictions_cifar10_accuracy: 0.8105 - lr: 0.0030\n",
      "Epoch 39/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0269 - c1_predictions_cifar10_loss: 0.0742 - c2_predictions_cifar10_loss: 0.0869 - predictions_cifar10_loss: 0.0269 - c1_predictions_cifar10_accuracy: 0.9716 - c2_predictions_cifar10_accuracy: 0.9694 - predictions_cifar10_accuracy: 0.9909 - val_loss: 0.9556 - val_c1_predictions_cifar10_loss: 0.1142 - val_c2_predictions_cifar10_loss: 0.5054 - val_predictions_cifar10_loss: 0.9556 - val_c1_predictions_cifar10_accuracy: 0.9582 - val_c2_predictions_cifar10_accuracy: 0.8659 - val_predictions_cifar10_accuracy: 0.8210 - lr: 0.0030\n",
      "Epoch 40/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0243 - c1_predictions_cifar10_loss: 0.0769 - c2_predictions_cifar10_loss: 0.0834 - predictions_cifar10_loss: 0.0243 - c1_predictions_cifar10_accuracy: 0.9708 - c2_predictions_cifar10_accuracy: 0.9718 - predictions_cifar10_accuracy: 0.9920 - val_loss: 0.9609 - val_c1_predictions_cifar10_loss: 0.1127 - val_c2_predictions_cifar10_loss: 0.5267 - val_predictions_cifar10_loss: 0.9609 - val_c1_predictions_cifar10_accuracy: 0.9577 - val_c2_predictions_cifar10_accuracy: 0.8624 - val_predictions_cifar10_accuracy: 0.8205 - lr: 0.0030\n",
      "Epoch 41/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0202 - c1_predictions_cifar10_loss: 0.0748 - c2_predictions_cifar10_loss: 0.0796 - predictions_cifar10_loss: 0.0202 - c1_predictions_cifar10_accuracy: 0.9715 - c2_predictions_cifar10_accuracy: 0.9727 - predictions_cifar10_accuracy: 0.9928 - val_loss: 0.9655 - val_c1_predictions_cifar10_loss: 0.1130 - val_c2_predictions_cifar10_loss: 0.5001 - val_predictions_cifar10_loss: 0.9655 - val_c1_predictions_cifar10_accuracy: 0.9571 - val_c2_predictions_cifar10_accuracy: 0.8652 - val_predictions_cifar10_accuracy: 0.8224 - lr: 0.0030\n",
      "Epoch 42/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0205 - c1_predictions_cifar10_loss: 0.0762 - c2_predictions_cifar10_loss: 0.0867 - predictions_cifar10_loss: 0.0205 - c1_predictions_cifar10_accuracy: 0.9708 - c2_predictions_cifar10_accuracy: 0.9702 - predictions_cifar10_accuracy: 0.9932 - val_loss: 1.0089 - val_c1_predictions_cifar10_loss: 0.1101 - val_c2_predictions_cifar10_loss: 0.5576 - val_predictions_cifar10_loss: 1.0089 - val_c1_predictions_cifar10_accuracy: 0.9594 - val_c2_predictions_cifar10_accuracy: 0.8509 - val_predictions_cifar10_accuracy: 0.8179 - lr: 0.0030\n",
      "Epoch 43/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0212 - c1_predictions_cifar10_loss: 0.0766 - c2_predictions_cifar10_loss: 0.0877 - predictions_cifar10_loss: 0.0212 - c1_predictions_cifar10_accuracy: 0.9705 - c2_predictions_cifar10_accuracy: 0.9698 - predictions_cifar10_accuracy: 0.9932 - val_loss: 1.0393 - val_c1_predictions_cifar10_loss: 0.1107 - val_c2_predictions_cifar10_loss: 0.5201 - val_predictions_cifar10_loss: 1.0393 - val_c1_predictions_cifar10_accuracy: 0.9593 - val_c2_predictions_cifar10_accuracy: 0.8609 - val_predictions_cifar10_accuracy: 0.8205 - lr: 0.0030\n",
      "Epoch 44/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0104 - c1_predictions_cifar10_loss: 0.0750 - c2_predictions_cifar10_loss: 0.0751 - predictions_cifar10_loss: 0.0104 - c1_predictions_cifar10_accuracy: 0.9716 - c2_predictions_cifar10_accuracy: 0.9741 - predictions_cifar10_accuracy: 0.9966 - val_loss: 0.8787 - val_c1_predictions_cifar10_loss: 0.1081 - val_c2_predictions_cifar10_loss: 0.4800 - val_predictions_cifar10_loss: 0.8787 - val_c1_predictions_cifar10_accuracy: 0.9593 - val_c2_predictions_cifar10_accuracy: 0.8712 - val_predictions_cifar10_accuracy: 0.8340 - lr: 5.0000e-04\n",
      "Epoch 45/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0040 - c1_predictions_cifar10_loss: 0.0726 - c2_predictions_cifar10_loss: 0.0650 - predictions_cifar10_loss: 0.0040 - c1_predictions_cifar10_accuracy: 0.9719 - c2_predictions_cifar10_accuracy: 0.9779 - predictions_cifar10_accuracy: 0.9991 - val_loss: 0.8689 - val_c1_predictions_cifar10_loss: 0.1084 - val_c2_predictions_cifar10_loss: 0.4753 - val_predictions_cifar10_loss: 0.8689 - val_c1_predictions_cifar10_accuracy: 0.9589 - val_c2_predictions_cifar10_accuracy: 0.8720 - val_predictions_cifar10_accuracy: 0.8379 - lr: 5.0000e-04\n",
      "Epoch 46/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0030 - c1_predictions_cifar10_loss: 0.0714 - c2_predictions_cifar10_loss: 0.0616 - predictions_cifar10_loss: 0.0030 - c1_predictions_cifar10_accuracy: 0.9733 - c2_predictions_cifar10_accuracy: 0.9795 - predictions_cifar10_accuracy: 0.9993 - val_loss: 0.8661 - val_c1_predictions_cifar10_loss: 0.1078 - val_c2_predictions_cifar10_loss: 0.4715 - val_predictions_cifar10_loss: 0.8661 - val_c1_predictions_cifar10_accuracy: 0.9594 - val_c2_predictions_cifar10_accuracy: 0.8737 - val_predictions_cifar10_accuracy: 0.8381 - lr: 5.0000e-04\n",
      "Epoch 47/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0027 - c1_predictions_cifar10_loss: 0.0710 - c2_predictions_cifar10_loss: 0.0614 - predictions_cifar10_loss: 0.0027 - c1_predictions_cifar10_accuracy: 0.9736 - c2_predictions_cifar10_accuracy: 0.9800 - predictions_cifar10_accuracy: 0.9995 - val_loss: 0.8655 - val_c1_predictions_cifar10_loss: 0.1084 - val_c2_predictions_cifar10_loss: 0.4703 - val_predictions_cifar10_loss: 0.8655 - val_c1_predictions_cifar10_accuracy: 0.9591 - val_c2_predictions_cifar10_accuracy: 0.8735 - val_predictions_cifar10_accuracy: 0.8398 - lr: 5.0000e-04\n",
      "Epoch 48/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0021 - c1_predictions_cifar10_loss: 0.0715 - c2_predictions_cifar10_loss: 0.0584 - predictions_cifar10_loss: 0.0021 - c1_predictions_cifar10_accuracy: 0.9729 - c2_predictions_cifar10_accuracy: 0.9820 - predictions_cifar10_accuracy: 0.9996 - val_loss: 0.8657 - val_c1_predictions_cifar10_loss: 0.1067 - val_c2_predictions_cifar10_loss: 0.4689 - val_predictions_cifar10_loss: 0.8657 - val_c1_predictions_cifar10_accuracy: 0.9594 - val_c2_predictions_cifar10_accuracy: 0.8750 - val_predictions_cifar10_accuracy: 0.8386 - lr: 5.0000e-04\n",
      "Epoch 49/60\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.0018 - c1_predictions_cifar10_loss: 0.0718 - c2_predictions_cifar10_loss: 0.0577 - predictions_cifar10_loss: 0.0018 - c1_predictions_cifar10_accuracy: 0.9718 - c2_predictions_cifar10_accuracy: 0.9814 - predictions_cifar10_accuracy: 0.9997 - val_loss: 0.8619 - val_c1_predictions_cifar10_loss: 0.1068 - val_c2_predictions_cifar10_loss: 0.4665 - val_predictions_cifar10_loss: 0.8619 - val_c1_predictions_cifar10_accuracy: 0.9594 - val_c2_predictions_cifar10_accuracy: 0.8749 - val_predictions_cifar10_accuracy: 0.8394 - lr: 5.0000e-04\n",
      "Epoch 50/60\n",
      "295/391 [=====================>........] - ETA: 4s - loss: 0.0015 - c1_predictions_cifar10_loss: 0.0720 - c2_predictions_cifar10_loss: 0.0577 - predictions_cifar10_loss: 0.0015 - c1_predictions_cifar10_accuracy: 0.9722 - c2_predictions_cifar10_accuracy: 0.9818 - predictions_cifar10_accuracy: 0.9998"
     ]
    }
   ],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_b_cnn_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_b_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_b_cnn_b,reca_b_cnn_b,f1_b_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- B-CNN Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_b_cnn_b[4])\n",
    "print(\"Accuracy level 2:\",score_b_cnn_b[5])\n",
    "print(\"Accuracy level 3:\",score_b_cnn_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_b_cnn_b)\n",
    "print(\"Recall:\",reca_b_cnn_b)\n",
    "print(\"f1:\",f1_b_cnn_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_b_cnn_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD1DmaHZf2JK"
   },
   "source": [
    "# BA-CNN Base B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHyRTBmtf8A7"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True\n",
    "\n",
    "# neurons of all dense layers on each branch \n",
    "branch_neurons = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEBBfHP3gJkX",
    "outputId": "c40c6e70-9af6-409e-9f5e-1376ec4e6dfc"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch_out = Dropout(0.5)(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch_out = Dropout(0.5)(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x_out = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "#-- Att for coarse 1---\n",
    "# Coarse 1\n",
    "sfcn_1_1 = Dense(64, name='fc1_1')(c_1_bch_out)\n",
    "sfcn_1_1 = Dense(1, name='fc1_2')(sfcn_1_1)\n",
    "# Coarse 2\n",
    "sfcn_1_2 = Dense(64, name='fc1_3')(c_2_bch_out)\n",
    "sfcn_1_2 = Dense(1, name='fc1_4')(sfcn_1_2)\n",
    "# Fine\n",
    "sfcn_1_3 = Dense(64, name='fc1_5')(x_out)\n",
    "sfcn_1_3 = Dense(1, name='fc1_6')(sfcn_1_3)\n",
    "\n",
    "score_vector_1 = Concatenate()([sfcn_1_1,sfcn_1_2,sfcn_1_3]) # Score vector 1\n",
    "att_weights_1 = Activation('softmax', name='attention_weights_1')(score_vector_1) # Attention weights 1\n",
    "weightned_sum_1 = Add()([c_1_bch_out*att_weights_1[0][0],c_2_bch_out*att_weights_1[0][1],x_out*att_weights_1[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_1_concat = Concatenate()([c_1_bch_out,weightned_sum_1])\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(coarse_1_concat)\n",
    "\n",
    "\n",
    "#-- Att for coarse 2---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_2_1 = Dense(64, name='fc2_1')(c_1_bch_out)\n",
    "sfcn_2_1 = Dense(1, name='fc2_2')(sfcn_2_1)\n",
    "# Coarse 2\n",
    "sfcn_2_2 = Dense(64, name='fc2_3')(c_2_bch_out)\n",
    "sfcn_2_2 = Dense(1, name='fc2_4')(sfcn_2_2)\n",
    "# Fine\n",
    "sfcn_2_3 = Dense(64, name='fc2_5')(x_out)\n",
    "sfcn_2_3 = Dense(1, name='fc2_6')(sfcn_2_3)\n",
    "\n",
    "score_vector_2 = Concatenate()([sfcn_2_1,sfcn_2_2,sfcn_2_3]) # Score vector 1\n",
    "att_weights_2 = Activation('softmax', name='attention_weights_2')(score_vector_2) # Attention weights 1\n",
    "weightned_sum_2 = Add()([c_1_bch_out*att_weights_2[0][0],c_2_bch_out*att_weights_2[0][1],x_out*att_weights_2[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_2_concat = Concatenate()([c_2_bch_out,weightned_sum_2])\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(coarse_2_concat)\n",
    "\n",
    "\n",
    "#-- Att for fine---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_3_1 = Dense(64, name='fc3_1')(c_1_bch_out)\n",
    "sfcn_3_1 = Dense(1, name='fc3_2')(sfcn_3_1)\n",
    "# Coarse 2\n",
    "sfcn_3_2 = Dense(64, name='fc3_3')(c_2_bch_out)\n",
    "sfcn_3_2 = Dense(1, name='fc3_4')(sfcn_3_2)\n",
    "# Fine\n",
    "sfcn_3_3 = Dense(64, name='fc3_5')(x_out)\n",
    "sfcn_3_3 = Dense(1, name='fc3_6')(sfcn_3_3)\n",
    "\n",
    "score_vector_3 = Concatenate()([sfcn_3_1,sfcn_3_2,sfcn_3_3]) # Score vector 1\n",
    "att_weights_3 = Activation('softmax', name='attention_weights_3')(score_vector_3) # Attention weights 1\n",
    "weightned_sum_3 = Add()([c_1_bch_out*att_weights_3[0][0],c_2_bch_out*att_weights_3[0][1],x_out*att_weights_3[0][2]]) # Weightned sum 3\n",
    "\n",
    "# Concat and prediction\n",
    "fine_concat = Concatenate()([x_out,weightned_sum_3])\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(fine_concat)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bacnn_base_b')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmBDSugfkAcn",
    "outputId": "f4b90537-8888-46f6-ce6a-2a208abf8979"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bacnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_ba_cnn_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_ba_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_ba_cnn_b,reca_ba_cnn_b,f1_ba_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- BA-CNN Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_ba_cnn_b[4])\n",
    "print(\"Accuracy level 2:\",score_ba_cnn_b[5])\n",
    "print(\"Accuracy level 3:\",score_ba_cnn_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_ba_cnn_b)\n",
    "print(\"Recall:\",reca_ba_cnn_b)\n",
    "print(\"f1:\",f1_ba_cnn_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_ba_cnn_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLRG5--_lmjc"
   },
   "source": [
    "# H-CNN Base B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8lNJ_DDnzWB"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYXL_rl1FMR2",
    "outputId": "76412fd5-6c3d-4ad0-b0d7-3f8920c3bc26"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch_flatt = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch_flatt)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch_flatt = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch_concat = Concatenate()([c_1_bch_flatt,c_2_bch_flatt]) # Conectivity Pattern\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar100_1')(c_2_bch_concat)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x_flatt = Flatten(name='flatten')(x)\n",
    "x = Concatenate()([c_2_bch_concat,x_flatt]) # Conectivity Pattern\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='hcnn_base_b')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0-YWS7moD46",
    "outputId": "abda058d-d8af-4deb-9e53-648837febb00"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_hcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_h_cnn_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_h_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_h_cnn_b,reca_h_cnn_b,f1_h_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- H-CNN Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_h_cnn_b[4])\n",
    "print(\"Accuracy level 2:\",score_h_cnn_b[5])\n",
    "print(\"Accuracy level 3:\",score_h_cnn_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_h_cnn_b)\n",
    "print(\"Recall:\",reca_h_cnn_b)\n",
    "print(\"f1:\",f1_h_cnn_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_h_cnn_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-net Base B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out = Add()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu', name='fc2')(x)\n",
    "x = Add()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='add_net_Base_B')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_addnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_addnet_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_addnet_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_addnet_b,reca_addnet_b,f1_addnet_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Add-net Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_addnet_b[4])\n",
    "print(\"Accuracy level 2:\",score_addnet_b[5])\n",
    "print(\"Accuracy level 3:\",score_addnet_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_addnet_b)\n",
    "print(\"Recall:\",reca_addnet_b)\n",
    "print(\"f1:\",f1_addnet_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_addnet_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat-net Base B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out = Concatenate()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = Concatenate()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='Concatnet_Base_B')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_concatnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_concatnet_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_concatnet_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_concatnet_b,reca_concatnet_b,f1_concatnet_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Concat-net Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_concatnet_b[4])\n",
    "print(\"Accuracy level 2:\",score_concatnet_b[5])\n",
    "print(\"Accuracy level 3:\",score_concatnet_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_concatnet_b)\n",
    "print(\"Recall:\",reca_concatnet_b)\n",
    "print(\"f1:\",f1_concatnet_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_concatnet_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcqC1GZ6kLlH"
   },
   "source": [
    "# Flat CNN Base C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cpunwm3o5UAx",
    "outputId": "399d69c8-fcda-466c-e851-1bd88f758028"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "#----------------------- model definition ---------------------------\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, fine_pred, name='flat_cnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KojpAgRHkgig"
   },
   "outputs": [],
   "source": [
    "change_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Training\n",
    "history_base_c = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=change_lr,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation on test set\n",
    "score_base_c = model.evaluate(x_test, y_test, verbose=0)\n",
    "parameters_base_c = np.sum([K.count_params(w) for w in model.trainable_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnW09JGQpo_J"
   },
   "source": [
    "# B-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_8ahOlc50aKP"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w9rtWZ07IK77",
    "outputId": "bdc5941b-d61a-42c9-d317-9790d378f9f0"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bcnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LzCzPlRV0uhW",
    "outputId": "2486e230-0298-4f48-b3e5-f14ddc2407d9"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_b_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_b_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_b_cnn_c,reca_b_cnn_c,f1_b_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- B-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_b_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_b_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_b_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_b_cnn_c)\n",
    "print(\"Recall:\",reca_b_cnn_c)\n",
    "print(\"f1:\",f1_b_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_b_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmTNGOqEpu4g"
   },
   "source": [
    "# BA-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7VZWx0yfWQZk"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True\n",
    "\n",
    "# neurons of all dense layers on each branch \n",
    "branch_neurons = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5FPODG2fcuuK",
    "outputId": "10bfad21-c3da-4f9c-ead2-0c2d7905853f"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch_out = Dropout(0.5)(c_1_bch)\n",
    "#c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch_out)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch_out = Dropout(0.5)(c_2_bch)\n",
    "#c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch_out)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x_out = Dropout(0.5)(x)\n",
    "\n",
    "#-- Att for coarse 1---\n",
    "# Coarse 1\n",
    "sfcn_1_1 = Dense(64, name='fc1_1')(c_1_bch_out)\n",
    "sfcn_1_1 = Dense(1, name='fc1_2')(sfcn_1_1)\n",
    "# Coarse 2\n",
    "sfcn_1_2 = Dense(64, name='fc1_3')(c_2_bch_out)\n",
    "sfcn_1_2 = Dense(1, name='fc1_4')(sfcn_1_2)\n",
    "# Fine\n",
    "sfcn_1_3 = Dense(64, name='fc1_5')(x_out)\n",
    "sfcn_1_3 = Dense(1, name='fc1_6')(sfcn_1_3)\n",
    "\n",
    "score_vector_1 = Concatenate()([sfcn_1_1,sfcn_1_2,sfcn_1_3]) # Score vector 1\n",
    "att_weights_1 = Activation('softmax', name='attention_weights_1')(score_vector_1) # Attention weights 1\n",
    "weightned_sum_1 = Add()([c_1_bch_out*att_weights_1[0][0],c_2_bch_out*att_weights_1[0][1],x_out*att_weights_1[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_1_concat = Concatenate()([c_1_bch_out,weightned_sum_1])\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(coarse_1_concat)\n",
    "\n",
    "\n",
    "#-- Att for coarse 2---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_2_1 = Dense(64, name='fc2_1')(c_1_bch_out)\n",
    "sfcn_2_1 = Dense(1, name='fc2_2')(sfcn_2_1)\n",
    "# Coarse 2\n",
    "sfcn_2_2 = Dense(64, name='fc2_3')(c_2_bch_out)\n",
    "sfcn_2_2 = Dense(1, name='fc2_4')(sfcn_2_2)\n",
    "# Fine\n",
    "sfcn_2_3 = Dense(64, name='fc2_5')(x_out)\n",
    "sfcn_2_3 = Dense(1, name='fc2_6')(sfcn_2_3)\n",
    "\n",
    "score_vector_2 = Concatenate()([sfcn_2_1,sfcn_2_2,sfcn_2_3]) # Score vector 1\n",
    "att_weights_2 = Activation('softmax', name='attention_weights_2')(score_vector_2) # Attention weights 1\n",
    "weightned_sum_2 = Add()([c_1_bch_out*att_weights_2[0][0],c_2_bch_out*att_weights_2[0][1],x_out*att_weights_2[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_2_concat = Concatenate()([c_2_bch_out,weightned_sum_2])\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(coarse_2_concat)\n",
    "\n",
    "\n",
    "#-- Att for fine---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_3_1 = Dense(64, name='fc3_1')(c_1_bch_out)\n",
    "sfcn_3_1 = Dense(1, name='fc3_2')(sfcn_3_1)\n",
    "# Coarse 2\n",
    "sfcn_3_2 = Dense(64, name='fc3_3')(c_2_bch_out)\n",
    "sfcn_3_2 = Dense(1, name='fc3_4')(sfcn_3_2)\n",
    "# Fine\n",
    "sfcn_3_3 = Dense(64, name='fc3_5')(x_out)\n",
    "sfcn_3_3 = Dense(1, name='fc3_6')(sfcn_3_3)\n",
    "\n",
    "score_vector_3 = Concatenate()([sfcn_3_1,sfcn_3_2,sfcn_3_3]) # Score vector 1\n",
    "att_weights_3 = Activation('softmax', name='attention_weights_3')(score_vector_3) # Attention weights 1\n",
    "weightned_sum_3 = Add()([c_1_bch_out*att_weights_3[0][0],c_2_bch_out*att_weights_3[0][1],x_out*att_weights_3[0][2]]) # Weightned sum 3\n",
    "\n",
    "# Concat and prediction\n",
    "fine_concat = Concatenate()([x_out,weightned_sum_3])\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(fine_concat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bacnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "g_l6k6PeWZOr",
    "outputId": "a27eca1b-8524-4c3f-effe-1aac92bac444"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_ba_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_ba_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_ba_cnn_c,reca_ba_cnn_c,f1_ba_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- BA-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_ba_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_ba_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_ba_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_ba_cnn_c)\n",
    "print(\"Recall:\",reca_ba_cnn_c)\n",
    "print(\"f1:\",f1_ba_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_ba_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7qKvpSMp2iE"
   },
   "source": [
    "# H-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9pshU_L-p0Sm"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wJ7BXWiAFQ_J",
    "outputId": "b3acf63f-5951-45df-f473-b838f36b27c0"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch_flatt = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch_flatt)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch_flatt = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch_concat = Concatenate()([c_1_bch_flatt,c_2_bch_flatt]) # Conectivity Pattern\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch_concat)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x_flatt = Flatten(name='flatten')(x)\n",
    "x = Concatenate()([c_2_bch_concat,x_flatt]) # Conectivity Pattern\n",
    "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='hcnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uTUo07wAW__7",
    "outputId": "ce827be6-1466-4c43-f605-d356ac0e40bd"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_h_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_h_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_h_cnn_c,reca_h_cnn_c,f1_h_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- H-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_h_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_h_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_h_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_h_cnn_c)\n",
    "print(\"Recall:\",reca_h_cnn_c)\n",
    "print(\"f1:\",f1_h_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_h_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-net Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out= Add()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu', name='fc2_cifar10_1')(x)\n",
    "x = Add()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='addnet_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_addnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_addnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_addnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_addnet_c,reca_addnet_c,f1_addnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Add-net Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_addnet_c[4])\n",
    "print(\"Accuracy level 2:\",score_addnet_c[5])\n",
    "print(\"Accuracy level 3:\",score_addnet_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_addnet_c)\n",
    "print(\"Recall:\",reca_addnet_c)\n",
    "print(\"f1:\",f1_addnet_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_addnet_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat-net Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out= Concatenate()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc2_cifar10_1')(x)\n",
    "x = Concatenate()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='concatnet_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_concatnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_concatnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_concatnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_concatnet_c,reca_concatnet_c,f1_concatnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Concat-net Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_concatnet_c[4])\n",
    "print(\"Accuracy level 2:\",score_concatnet_c[5])\n",
    "print(\"Accuracy level 3:\",score_concatnett_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_concatnet_c)\n",
    "print(\"Recall:\",reca_concatnet_c)\n",
    "print(\"f1:\",f1_concatnet_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_concatnet_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOp_WriEXRdI"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-g9xmRuBXTx0",
    "outputId": "6524904f-d12e-4856-df3d-75f56fd3f581"
   },
   "outputs": [],
   "source": [
    "summary = {'':['Flat CNN Base B','B-CNN Base B','BA-CNN Base B','H-CNN Base B','Add-net Base B','Concat-net Base B'],'Coarse 1': [0,score_b_cnn_b[4],score_ba_cnn_b[4],score_h_cnn_b[4],score_addnet_b[4],score_concatnet_b[4]],'Coarse 2': [0,score_b_cnn_b[5],score_ba_cnn_b[5],score_h_cnn_b[5],score_addnet_b[5],score_concatnet_b[5]],'Fine': [score_base_b[1],score_b_cnn_b[6],score_ba_cnn_b[6],score_h_cnn_b[6],score_addnet_b[6],score_concatnet_b[6]],'Precision':[0,preci_b_cnn_b,preci_ba_cnn_b,preci_h_cnn_b,preci_addnet_b,preci_concatnet_b],'Recall':[0,reca_b_cnn_b,reca_ba_cnn_b,reca_h_cnn_b,reca_addnet_b,reca_concatnet_b],'f1':[0,f1_b_cnn_b,f1_ba_cnn_b,f1_h_cnn_b,f1_addnet_b,f1_concatnet_b],'Parameters': [parameters_base_b ,parameters_b_cnn_b,parameters_ba_cnn_b,parameters_h_cnn_b,parameters_addnet_b,parameters_concatnet_b]}\n",
    "summary = pd.DataFrame(summary)\n",
    "summary['Parameters'] = (summary['Parameters'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
    "summary = summary.set_index('')\n",
    "summary.style.highlight_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "iKqpTST7YtjT",
    "outputId": "4c1708bf-3919-4144-b488-6a2f8f6635e3"
   },
   "outputs": [],
   "source": [
    "summary = {'':['Flat CNN Base C','B-CNN Base C','BA-CNN Base C','H-CNN Base C','Add-net Base B','Concat-net Base B'],'Coarse 1': [0,score_b_cnn_c[4],score_ba_cnn_c[4],score_h_cnn_c[4],score_addnet_c[4],score_concatnet_c[4]],'Coarse 2': [0,score_b_cnn_c[5],score_ba_cnn_c[5],score_h_cnn_c[5],score_addnet_c[5],score_concatnet_c[5]],'Fine': [score_base_c[1],score_b_cnn_c[6],score_ba_cnn_c[6],score_h_cnn_c[6],score_addnet_c[6],score_concatnet_c[6]],'Precision':[0,preci_b_cnn_c,preci_ba_cnn_c,preci_h_cnn_c,preci_addnet_c,preci_concatnet_c],'Recall':[0,reca_b_cnn_c,reca_ba_cnn_c,reca_h_cnn_c,reca_addnet_c,reca_concatnet_c],'f1':[0,f1_b_cnn_c,f1_ba_cnn_c,f1_h_cnn_c,f1_addnet_c,f1_concatnet_c],'Parameters': [parameters_base_c ,parameters_b_cnn_c,parameters_ba_cnn_c,parameters_h_cnn_c,parameters_addnet_c,parameters_concatnet_c]}\n",
    "summary = pd.DataFrame(summary)\n",
    "summary['Parameters'] = (summary['Parameters'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
    "summary = summary.set_index('')\n",
    "summary.style.highlight_max()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BA_CNN_CIFAR_10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
