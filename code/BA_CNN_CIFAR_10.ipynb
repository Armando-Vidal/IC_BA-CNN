{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvPb0YJNU1RM"
      },
      "source": [
        "\n",
        "<H3 align='center'> An Attention-Based Architecture for\n",
        "Hierarchical Classification with CNNs </H3>\n",
        "\n",
        "<H5 align='center'> CIFAR-10 </H3>\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CFPEBlEDW1R"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vb27tNtvUuKD"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#from keras.datasets import cifar10\n",
        "#from keras.models import Model\n",
        "#from keras.layers import Dense, Dropout, Activation, Flatten, Concatenate, Add, Softmax\n",
        "#from keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
        "#from keras.initializers import he_normal\n",
        "#from keras import optimizers\n",
        "#from keras.callbacks import LearningRateScheduler, TensorBoard, CSVLogger\n",
        "from tensorflow.keras.utils import get_file\n",
        "#from keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import pickle\n",
        "#from keras.models import load_model\n",
        "#from keras.callbacks import CSVLogger\n",
        "from tensorflow.keras.utils import set_random_seed\n",
        "from scipy import stats\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, Activation, Flatten, Concatenate, Add, Softmax,\n",
        "    Conv2D, MaxPooling2D, Input, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.initializers import he_normal\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import (\n",
        "    LearningRateScheduler, TensorBoard, CSVLogger\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Correct relations for CIFAR-10\n",
        "relations = [[0,2,9],[0,3,17],[0,4,10],[0,4,18],[1,5,11],[1,6,15],[1,7,12],[1,7,14],[1,8,13],[1,8,16]]\n",
        "\n",
        "# Computes hierarchical metrics\n",
        "def hierarchical_metrics(true,pred):\n",
        "  true_labels = []\n",
        "  true_fine = true[2].argmax(axis=1)+9\n",
        "  true_c2 = true[1].argmax(axis=1)+2\n",
        "  true_c1 = true[0].argmax(axis=1)\n",
        "  for i in range(len(true_fine)):\n",
        "    true_labels.append([true_c1[i],true_c2[i],true_fine[i]])\n",
        "  pred_labels = []\n",
        "  pred_c1 = pred[0].argmax(axis = 1)\n",
        "  pred_c2 = pred[1].argmax(axis = 1)+2\n",
        "  pred_fine = pred[2].argmax(axis = 1)+9\n",
        "  for i in range(len(pred_c1)):\n",
        "    pred_labels.append([pred_c1[i],pred_c2[i],pred_fine[i]])\n",
        "  preci = precision(true_labels,pred_labels)\n",
        "  reca = recall(true_labels,pred_labels)\n",
        "  f_1 = f1(true_labels,pred_labels)\n",
        "\n",
        "  consistent_examples = 1\n",
        "  correct_pred = 0\n",
        "  test_set_size = len(true_labels)\n",
        "  for i in range(test_set_size):\n",
        "    if [pred_c1[i],pred_c2[i],pred_fine[i]] in relations:\n",
        "        consistent_examples = consistent_examples + 1\n",
        "    if [pred_c1[i],pred_c2[i],pred_fine[i]] == true_labels[i]:\n",
        "        correct_pred = correct_pred +1\n",
        "  h_accuracy = correct_pred/test_set_size\n",
        "  h_consistency = (consistent_examples-1)/test_set_size\n",
        "\n",
        "  return h_accuracy,h_consistency,f_1\n",
        "\n",
        "# Hierarchical metrics, proposed by Kiritchenko et al (2005)\n",
        "# Implementation\n",
        "# https://gitlab.com/dacs-hpi/hiclass/-/blob/main/hiclass/metrics.py\n",
        "\n",
        "\n",
        "def precision(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute precision score for hierarchical classification.\n",
        "\n",
        "    hP = sum(|S intersection T|) / sum(|S|),\n",
        "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
        "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : np.array of shape (n_samples, n_levels)\n",
        "        Ground truth (correct) labels.\n",
        "    y_pred : np.array of shape (n_samples, n_levels)\n",
        "        Predicted labels, as returned by a classifier.\n",
        "    Returns\n",
        "    -------\n",
        "    precision : float\n",
        "        What proportion of positive identifications was actually correct?\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_pred)\n",
        "    sum_intersection = 0\n",
        "    sum_prediction_and_ancestors = 0\n",
        "    for ground_truth, prediction in zip(y_true, y_pred):\n",
        "        sum_intersection = sum_intersection + len(\n",
        "            set(ground_truth).intersection(set(prediction))\n",
        "        )\n",
        "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
        "            set(prediction)\n",
        "        )\n",
        "    precision = sum_intersection / sum_prediction_and_ancestors\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute recall score for hierarchical classification.\n",
        "\n",
        "    hR = sum(|S intersection T|) / sum(|T|),\n",
        "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
        "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : np.array of shape (n_samples, n_levels)\n",
        "        Ground truth (correct) labels.\n",
        "    y_pred : np.array of shape (n_samples, n_levels)\n",
        "        Predicted labels, as returned by a classifier.\n",
        "    Returns\n",
        "    -------\n",
        "    recall : float\n",
        "        What proportion of actual positives was identified correctly?\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_pred)\n",
        "    sum_intersection = 0\n",
        "    sum_prediction_and_ancestors = 0\n",
        "    for ground_truth, prediction in zip(y_true, y_pred):\n",
        "        sum_intersection = sum_intersection + len(\n",
        "            set(ground_truth).intersection(set(prediction))\n",
        "        )\n",
        "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
        "            set(ground_truth)\n",
        "        )\n",
        "    recall = sum_intersection / sum_prediction_and_ancestors\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute f1 score for hierarchical classification.\n",
        "\n",
        "    hF = 2 * hP * hR / (hP + hR),\n",
        "    where hP is the hierarchical precision and hR is the hierarchical recall.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : np.array of shape (n_samples, n_levels)\n",
        "        Ground truth (correct) labels.\n",
        "    y_pred : np.array of shape (n_samples, n_levels)\n",
        "        Predicted labels, as returned by a classifier.\n",
        "    Returns\n",
        "    -------\n",
        "    f1 : float\n",
        "        Weighted average of the precision and recall\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_pred)\n",
        "    prec = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    f1 = 2 * prec * rec / (prec + rec)\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFr3aR_HvlFA"
      },
      "source": [
        "# General Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "V2EC6Gn0vnZs"
      },
      "outputs": [],
      "source": [
        "#-------- dimensions ---------\n",
        "img_rows, img_cols = 32, 32\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_rows, img_cols)\n",
        "else:\n",
        "    input_shape = (img_rows, img_cols, 3)\n",
        "#-----------------------------\n",
        "\n",
        "train_size = 50000\n",
        "\n",
        "#--- coarse 1 classes ---\n",
        "num_c_1 = 2\n",
        "#--- coarse 2 classes ---\n",
        "num_c_2 = 7\n",
        "#--- fine classes ---\n",
        "num_classes  = 10\n",
        "\n",
        "batch_size   = 128\n",
        "epochs       = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhgnNBz7P5h9",
        "outputId": "b62e6063-596e-4861-8416-9242b067e91a"
      },
      "outputs": [],
      "source": [
        "#-------------------- data loading ----------------------\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_cm = y_test\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#---------------- data preprocessing -------------------\n",
        "x_train = (x_train-np.mean(x_train)) / np.std(x_train)\n",
        "x_test = (x_test-np.mean(x_test)) / np.std(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "FvrF29rSwGMB"
      },
      "outputs": [],
      "source": [
        "#---------------------- make coarse 2 labels --------------------------\n",
        "parent_f = {\n",
        "  2:3, 3:5, 5:5,\n",
        "  1:2, 7:6, 4:6,\n",
        "  0:0, 6:4, 8:1, 9:2\n",
        "}\n",
        "\n",
        "y_c2_train = np.zeros((y_train.shape[0], num_c_2)).astype(\"float32\")\n",
        "y_c2_test = np.zeros((y_test.shape[0], num_c_2)).astype(\"float32\")\n",
        "for i in range(y_c2_train.shape[0]):\n",
        "  y_c2_train[i][parent_f[np.argmax(y_train[i])]] = 1.0\n",
        "for i in range(y_c2_test.shape[0]):\n",
        "  y_c2_test[i][parent_f[np.argmax(y_test[i])]] = 1.0\n",
        "\n",
        "#---------------------- make coarse 1 labels --------------------------\n",
        "parent_c2 = {\n",
        "  0:0, 1:0, 2:0,\n",
        "  3:1, 4:1, 5:1, 6:1\n",
        "}\n",
        "y_c1_train = np.zeros((y_c2_train.shape[0], num_c_1)).astype(\"float32\")\n",
        "y_c1_test = np.zeros((y_c2_test.shape[0], num_c_1)).astype(\"float32\")\n",
        "for i in range(y_c1_train.shape[0]):\n",
        "  y_c1_train[i][parent_c2[np.argmax(y_c2_train[i])]] = 1.0\n",
        "for i in range(y_c1_test.shape[0]):\n",
        "  y_c1_test[i][parent_c2[np.argmax(y_c2_test[i])]] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "UCJ4bhapPP-Y"
      },
      "outputs": [],
      "source": [
        "# Learning rate scheduler\n",
        "def scheduler(epoch):\n",
        "  learning_rate_init = 0.003\n",
        "  if epoch > 42:\n",
        "    learning_rate_init = 0.0005\n",
        "  if epoch > 52:\n",
        "    learning_rate_init = 0.0001\n",
        "  return learning_rate_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bmCQrJtYwa87"
      },
      "outputs": [],
      "source": [
        "# Loss Weights modifier, when BT-strategy is used\n",
        "class LossWeightsModifier(keras.callbacks.Callback):\n",
        "  def __init__(self, alpha, beta, gamma):\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.gamma = gamma\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if epoch == 10:\n",
        "      K.set_value(self.alpha, 0.1)\n",
        "      K.set_value(self.beta, 0.8)\n",
        "      K.set_value(self.gamma, 0.1)\n",
        "    if epoch == 20:\n",
        "      K.set_value(self.alpha, 0.1)\n",
        "      K.set_value(self.beta, 0.2)\n",
        "      K.set_value(self.gamma, 0.7)\n",
        "    if epoch == 30:\n",
        "      K.set_value(self.alpha, 0)\n",
        "      K.set_value(self.beta, 0)\n",
        "      K.set_value(self.gamma, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_n23WmqWm0r"
      },
      "source": [
        "# Flat CNN Base B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BrSqH3ZSQT5L"
      },
      "outputs": [],
      "source": [
        "#----------------------- model definition ---------------------------\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu', name='fc2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, fine_pred, name='flat_cnn_base_b')\n",
        "\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'],\n",
        "              )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHyicCv-XEmK",
        "outputId": "cefa45a5-f0d2-4251-fd53-4da0658a83b6"
      },
      "outputs": [],
      "source": [
        "change_lr = LearningRateScheduler(scheduler)# Training\n",
        "history_base_b = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          callbacks=change_lr,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluation on test set\n",
        "score_base_b = model.evaluate(x_test, y_test, verbose=0)\n",
        "parameters_base_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "# Results\n",
        "print(\"--- Flat CNN Base B ---\")\n",
        "print(\"Accuracy:\",score_base_b[1])\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_base_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save non-model variables\n",
        "with open(\"experiment_data.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"score_base_b\": score_base_b,\n",
        "        \"parameters_base_b\": parameters_base_b,\n",
        "        \"history_base_b\": history_base_b.history,  # Only the history dict\n",
        "    }, f)\n",
        "\n",
        "# Save model\n",
        "model.save(\"flat_cnn_base_b.keras\")  # or \"flat_cnn_base_b.h5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load non-model variables\n",
        "with open(\"./CIFAR10_models/saved_flat_cnn_b/flat_cnn_experiment_data.pkl\", \"rb\") as f:\n",
        "    flat_cnn_b_data = pickle.load(f)\n",
        "\n",
        "# Unpack the loaded data\n",
        "flat_cnn_score_base_b = flat_cnn_b_data[\"score_base_b\"]\n",
        "flat_cnn_parameters_base_b = flat_cnn_b_data[\"parameters_base_b\"]\n",
        "flat_cnn_history_base_b = flat_cnn_b_data[\"history_base_b\"]\n",
        "\n",
        "# Load the Keras model\n",
        "flat_cnn_b_model = keras.models.load_model(\"./CIFAR10_models/saved_flat_cnn_b/flat_cnn_base_b.keras\")\n",
        "\n",
        "# (Optional) If you want, you can reconstruct the history as a History-like object\n",
        "# But usually, people just use the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4isnsDMcKKm"
      },
      "source": [
        "# B-CNN Base B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sMGtDO4tdxoN"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0UnKGZz_zOss",
        "outputId": "8c8ed01d-ac5d-42c0-d528-a0ae168b33e0"
      },
      "outputs": [],
      "source": [
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu', name='fc2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bcnn_base_b')\n",
        "\n",
        "#----------------------- compile  ---------------------------\n",
        "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
        "#ADD - lista de loss=[...] e metrics[...]\n",
        "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy','accuracy','accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5J93jPue6Kr",
        "outputId": "59460f2e-8267-40c8-b86b-3a558c9c101d"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_b_cnn_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_b_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_b_cnn_b,cons_b_cnn_b,f1_b_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- B-CNN Base B ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_b_cnn_b[4])\n",
        "print(\"Accuracy level 2:\",score_b_cnn_b[5])\n",
        "print(\"Accuracy level 3:\",score_b_cnn_b[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_b_cnn_b)\n",
        "print(\"Consistency:\",cons_b_cnn_b)\n",
        "print(\"f1:\",f1_b_cnn_b)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_b_cnn_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Save the trained model (architecture + weights + optimizer state)\n",
        "model.save(\"bcnn_b_final_model.keras\")\n",
        "\n",
        "# 2. Save the training history\n",
        "with open(\"history_bcnn_b.pkl\", \"wb\") as f:\n",
        "    pickle.dump(history_bcnn_b.history, f)\n",
        "\n",
        "# 3. Save hyperparameters and other relevant variables\n",
        "meta = {\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'alpha_value': K.get_value(alpha),\n",
        "    'beta_value': K.get_value(beta),\n",
        "    'gamma_value': K.get_value(gamma),\n",
        "    'bt_strategy': bt_strategy,\n",
        "    'input_shape': input_shape,\n",
        "    'num_classes': num_classes,\n",
        "    'num_c_1': num_c_1,\n",
        "    'num_c_2': num_c_2,\n",
        "    'learning_rate': 0.003  # or however you're setting it\n",
        "}\n",
        "\n",
        "with open(\"bcnn_b_meta.pkl\", \"wb\") as f:\n",
        "    pickle.dump(meta, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Load the trained model (architecture + weights + optimizer state)\n",
        "b_cnn_b_model = keras.models.load_model(\"./CIFAR10_models/saved_bcnn_b/bcnn_b_final_model.keras\")\n",
        "\n",
        "# 2. Load the training history\n",
        "with open(\"./CIFAR10_models/saved_bcnn_b/history_bcnn_b.pkl\", \"rb\") as f:\n",
        "    history_bcnn_b = pickle.load(f)\n",
        "\n",
        "# 3. Load hyperparameters and other relevant variables\n",
        "with open(\"./CIFAR10_models/saved_bcnn_b/bcnn_b_meta.pkl\", \"rb\") as f:\n",
        "    b_cnn_b_meta = pickle.load(f)\n",
        "\n",
        "# Unpack meta if you want individual variables\n",
        "b_cnn_b_batch_size = b_cnn_b_meta['batch_size']\n",
        "b_cnn_b_epochs = b_cnn_b_meta['epochs']\n",
        "b_cnn_b_alpha_value = b_cnn_b_meta['alpha_value']\n",
        "b_cnn_b_beta_value = b_cnn_b_meta['beta_value']\n",
        "b_cnn_b_gamma_value = b_cnn_b_meta['gamma_value']\n",
        "b_cnn_b_bt_strategy = b_cnn_b_meta['bt_strategy']\n",
        "b_cnn_b_input_shape = b_cnn_b_meta['input_shape']\n",
        "b_cnn_b_num_classes = b_cnn_b_meta['num_classes']\n",
        "b_cnn_b_num_c_1 = b_cnn_b_meta['num_c_1']\n",
        "b_cnn_b_num_c_2 = b_cnn_b_meta['num_c_2']\n",
        "b_cnn_b_learning_rate = b_cnn_b_meta['learning_rate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- B-CNN Base B ---\n",
            "--- Accuracy per level ---\n",
            "Accuracy level 1: 0.9589999914169312\n",
            "Accuracy level 2: 0.8697999715805054\n",
            "Accuracy level 3: 0.8424999713897705\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 104ms/step\n",
            "--- Hierarchical Metrics ---\n",
            "Accuracy: 0.7889\n",
            "Consistency: 0.9028\n",
            "f1: 0.8904333333333333\n"
          ]
        }
      ],
      "source": [
        "# 4. Assuming you have your test data loaded, evaluate the model\n",
        "# Replace 'x_test', 'y_c1_test', 'y_c2_test', and 'y_test' with your actual test data variables\n",
        "score_b_cnn_b = b_cnn_b_model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "\n",
        "# 5. Print the accuracy at each level (assuming this information is in the score)\n",
        "print(\"--- B-CNN Base B ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\", score_b_cnn_b[4])  # Accuracy level 1\n",
        "print(\"Accuracy level 2:\", score_b_cnn_b[5])  # Accuracy level 2\n",
        "print(\"Accuracy level 3:\", score_b_cnn_b[6])  # Accuracy level 3\n",
        "\n",
        "# Calculate the number of trainable parameters\n",
        "parameters_b_cnn_b = np.sum([K.count_params(w) for w in b_cnn_b_model.trainable_weights])\n",
        "\n",
        "# You can also use hierarchical metrics if needed\n",
        "acc_b_cnn_b, cons_b_cnn_b, f1_b_cnn_b = hierarchical_metrics([y_c1_test, y_c2_test, y_test], b_cnn_b_model.predict(x_test))\n",
        "\n",
        "# Results\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\", acc_b_cnn_b)\n",
        "print(\"Consistency:\", cons_b_cnn_b)\n",
        "print(\"f1:\", f1_b_cnn_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD1DmaHZf2JK"
      },
      "source": [
        "# BA-CNN Base B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kHyRTBmtf8A7"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True\n",
        "\n",
        "# neurons of all dense layers on each branch\n",
        "branch_neurons = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EEBBfHP3gJkX",
        "outputId": "344e114e-7828-4049-ab62-651d3e3cfe08"
      },
      "outputs": [],
      "source": [
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch_out = Dropout(0.5)(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch_out = Dropout(0.5)(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(branch_neurons, activation='relu', name='fc2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x_out = Dropout(0.5)(x)\n",
        "\n",
        "\n",
        "#-- Att for coarse 1---\n",
        "# Coarse 1\n",
        "sfcn_1_1 = Dense(64, name='fc1_1')(c_1_bch_out)\n",
        "sfcn_1_1 = Dense(1, name='fc1_2')(sfcn_1_1)\n",
        "# Coarse 2\n",
        "sfcn_1_2 = Dense(64, name='fc1_3')(c_2_bch_out)\n",
        "sfcn_1_2 = Dense(1, name='fc1_4')(sfcn_1_2)\n",
        "# Fine\n",
        "sfcn_1_3 = Dense(64, name='fc1_5')(x_out)\n",
        "sfcn_1_3 = Dense(1, name='fc1_6')(sfcn_1_3)\n",
        "\n",
        "score_vector_1 = Concatenate()([sfcn_1_1,sfcn_1_2,sfcn_1_3]) # Score vector 1\n",
        "att_weights_1 = Activation('softmax', name='attention_weights_1')(score_vector_1) # Attention weights 1\n",
        "weightned_sum_1 = Add()([c_1_bch_out*att_weights_1[0][0],c_2_bch_out*att_weights_1[0][1],x_out*att_weights_1[0][2]]) # Weightned sum 1\n",
        "\n",
        "# Concat and prediction\n",
        "coarse_1_concat = Concatenate()([c_1_bch_out,weightned_sum_1])\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(coarse_1_concat)\n",
        "\n",
        "\n",
        "#-- Att for coarse 2---\n",
        "\n",
        "# Coarse 1\n",
        "sfcn_2_1 = Dense(64, name='fc2_1')(c_1_bch_out)\n",
        "sfcn_2_1 = Dense(1, name='fc2_2')(sfcn_2_1)\n",
        "# Coarse 2\n",
        "sfcn_2_2 = Dense(64, name='fc2_3')(c_2_bch_out)\n",
        "sfcn_2_2 = Dense(1, name='fc2_4')(sfcn_2_2)\n",
        "# Fine\n",
        "sfcn_2_3 = Dense(64, name='fc2_5')(x_out)\n",
        "sfcn_2_3 = Dense(1, name='fc2_6')(sfcn_2_3)\n",
        "\n",
        "score_vector_2 = Concatenate()([sfcn_2_1,sfcn_2_2,sfcn_2_3]) # Score vector 1\n",
        "att_weights_2 = Activation('softmax', name='attention_weights_2')(score_vector_2) # Attention weights 1\n",
        "weightned_sum_2 = Add()([c_1_bch_out*att_weights_2[0][0],c_2_bch_out*att_weights_2[0][1],x_out*att_weights_2[0][2]]) # Weightned sum 1\n",
        "\n",
        "# Concat and prediction\n",
        "coarse_2_concat = Concatenate()([c_2_bch_out,weightned_sum_2])\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(coarse_2_concat)\n",
        "\n",
        "\n",
        "#-- Att for fine---\n",
        "\n",
        "# Coarse 1\n",
        "sfcn_3_1 = Dense(64, name='fc3_1')(c_1_bch_out)\n",
        "sfcn_3_1 = Dense(1, name='fc3_2')(sfcn_3_1)\n",
        "# Coarse 2\n",
        "sfcn_3_2 = Dense(64, name='fc3_3')(c_2_bch_out)\n",
        "sfcn_3_2 = Dense(1, name='fc3_4')(sfcn_3_2)\n",
        "# Fine\n",
        "sfcn_3_3 = Dense(64, name='fc3_5')(x_out)\n",
        "sfcn_3_3 = Dense(1, name='fc3_6')(sfcn_3_3)\n",
        "\n",
        "score_vector_3 = Concatenate()([sfcn_3_1,sfcn_3_2,sfcn_3_3]) # Score vector 1\n",
        "att_weights_3 = Activation('softmax', name='attention_weights_3')(score_vector_3) # Attention weights 1\n",
        "weightned_sum_3 = Add()([c_1_bch_out*att_weights_3[0][0],c_2_bch_out*att_weights_3[0][1],x_out*att_weights_3[0][2]]) # Weightned sum 3\n",
        "\n",
        "# Concat and prediction\n",
        "fine_concat = Concatenate()([x_out,weightned_sum_3])\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(fine_concat)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bacnn_base_b')\n",
        "\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy','accuracy','accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmBDSugfkAcn",
        "outputId": "765870f9-b561-472b-c9f7-4313c24601f1"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_bacnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_ba_cnn_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_ba_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_ba_cnn_b,cons_ba_cnn_b,f1_ba_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- BA-CNN Base B ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_ba_cnn_b[4])\n",
        "print(\"Accuracy level 2:\",score_ba_cnn_b[5])\n",
        "print(\"Accuracy level 3:\",score_ba_cnn_b[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_ba_cnn_b)\n",
        "print(\"Consistency:\",cons_ba_cnn_b)\n",
        "print(\"f1:\",f1_ba_cnn_b)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_ba_cnn_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a save directory\n",
        "save_dir = 'saved_bacnn_b'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save Keras model\n",
        "model.save(os.path.join(save_dir, 'bacnn_b_model.keras'))\n",
        "\n",
        "# Save Keras history\n",
        "with open(os.path.join(save_dir, 'bacnn_b_history.pkl'), 'wb') as f:\n",
        "    pickle.dump(history_bacnn_b.history, f)\n",
        "\n",
        "# Save weights (optional redundancy)\n",
        "model.save_weights(os.path.join(save_dir, 'bacnn_b_weights.weights.h5'))\n",
        "\n",
        "# Save relevant variables using pickle\n",
        "important_vars = {\n",
        "    'alpha': K.get_value(alpha),\n",
        "    'beta': K.get_value(beta),\n",
        "    'gamma': K.get_value(gamma),\n",
        "    'bt_strategy': bt_strategy,\n",
        "    'branch_neurons': branch_neurons,\n",
        "    'score_ba_cnn_b': score_ba_cnn_b,\n",
        "    'parameters_ba_cnn_b': parameters_ba_cnn_b,\n",
        "    'acc_ba_cnn_b': acc_ba_cnn_b,\n",
        "    'cons_ba_cnn_b': cons_ba_cnn_b,\n",
        "    'f1_ba_cnn_b': f1_ba_cnn_b,\n",
        "}\n",
        "\n",
        "with open(os.path.join(save_dir, 'bacnn_b_variables.pkl'), 'wb') as f:\n",
        "    pickle.dump(important_vars, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the directory\n",
        "load_dir = '.\\CIFAR10_models\\saved_bacnn_b'\n",
        "\n",
        "# Load the Keras model\n",
        "ba_cnn_b_model = keras.models.load_model(os.path.join(load_dir, 'bacnn_b_model.keras'))\n",
        "\n",
        "# Load the training history\n",
        "with open(os.path.join(load_dir, 'bacnn_b_history.pkl'), 'rb') as f:\n",
        "    history_bacnn_b = pickle.load(f)\n",
        "\n",
        "# (Optional) Load the weights separately if you want to\n",
        "ba_cnn_b_model.load_weights(os.path.join(load_dir, 'bacnn_b_weights.weights.h5'))\n",
        "\n",
        "# Load important variables\n",
        "with open(os.path.join(load_dir, 'bacnn_b_variables.pkl'), 'rb') as f:\n",
        "    ba_cnn_b_important_vars = pickle.load(f)\n",
        "\n",
        "# Now unpack the important_vars dictionary\n",
        "ba_cnn_b_alpha_value = ba_cnn_b_important_vars['alpha']\n",
        "ba_cnn_b_beta_value = ba_cnn_b_important_vars['beta']\n",
        "ba_cnn_b_gamma_value = ba_cnn_b_important_vars['gamma']\n",
        "ba_cnn_b_bt_strategy = ba_cnn_b_important_vars['bt_strategy']\n",
        "ba_cnn_b_branch_neurons = ba_cnn_b_important_vars['branch_neurons']\n",
        "score_ba_cnn_b = ba_cnn_b_important_vars['score_ba_cnn_b']\n",
        "parameters_ba_cnn_b = ba_cnn_b_important_vars['parameters_ba_cnn_b']\n",
        "acc_ba_cnn_b = ba_cnn_b_important_vars['acc_ba_cnn_b']\n",
        "cons_ba_cnn_b = ba_cnn_b_important_vars['cons_ba_cnn_b']\n",
        "f1_ba_cnn_b = ba_cnn_b_important_vars['f1_ba_cnn_b']\n",
        "\n",
        "# (Optional) If you want to recreate the Keras variables for alpha, beta, gamma:\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "ba_cnn_b_alpha = K.variable(value=ba_cnn_b_alpha_value, dtype=tf.float32, name=\"alpha\")\n",
        "ba_cnn_b_beta = K.variable(value=ba_cnn_b_beta_value, dtype=tf.float32, name=\"beta\")\n",
        "ba_cnn_b_gamma = K.variable(value=ba_cnn_b_gamma_value, dtype=tf.float32, name=\"gamma\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLRG5--_lmjc"
      },
      "source": [
        "# H-CNN Base B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "A8lNJ_DDnzWB"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nYXL_rl1FMR2",
        "outputId": "7d3c08a2-3752-4956-8ca0-93888d867b29"
      },
      "outputs": [],
      "source": [
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch_flatt = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch_flatt)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch_flatt = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch_concat = Concatenate()([c_1_bch_flatt,c_2_bch_flatt]) # Conectivity Pattern\n",
        "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar100_1')(c_2_bch_concat)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x_flatt = Flatten(name='flatten')(x)\n",
        "x = Concatenate()([c_2_bch_concat,x_flatt]) # Conectivity Pattern\n",
        "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu', name='fc2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='hcnn_base_b')\n",
        "\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy','accuracy','accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0-YWS7moD46",
        "outputId": "86cc705f-8fdb-407f-b6a0-7b237452b78e"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_hcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_h_cnn+_b_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_h_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_h_cnn_b,cons_h_cnn_b,f1_h_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- H-CNN Base B ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_h_cnn_b[4])\n",
        "print(\"Accuracy level 2:\",score_h_cnn_b[5])\n",
        "print(\"Accuracy level 3:\",score_h_cnn_b[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_h_cnn_b)\n",
        "print(\"Consistency:\",cons_h_cnn_b)\n",
        "print(\"f1:\",f1_h_cnn_b)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_h_cnn_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Directory to save the results\n",
        "save_dir = 'results/hcnn_b'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model.save(os.path.join(save_dir, 'hcnn_b_model.keras'))\n",
        "\n",
        "# Save weights\n",
        "model.save_weights(os.path.join(save_dir, 'hcnn_b_weights.weights.h5'))\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(save_dir, 'hcnn_b_history.pkl'), 'wb') as f:\n",
        "    pickle.dump(history_hcnn_b.history, f)\n",
        "\n",
        "# Save predictions\n",
        "with open(os.path.join(save_dir, 'hcnn_b_predictions.pkl'), 'wb') as f:\n",
        "    pickle.dump(predictions, f)\n",
        "\n",
        "# Save evaluation scores\n",
        "with open(os.path.join(save_dir, 'hcnn_b_eval.pkl'), 'wb') as f:\n",
        "    pickle.dump(score_h_cnn_b, f)\n",
        "\n",
        "# Save number of parameters\n",
        "with open(os.path.join(save_dir, 'hcnn_b_params.pkl'), 'wb') as f:\n",
        "    pickle.dump(parameters_h_cnn_b, f)\n",
        "\n",
        "# Save hierarchical metrics\n",
        "hierarchical_results = {\n",
        "    'accuracy': acc_h_cnn_b,\n",
        "    'consistency': cons_h_cnn_b,\n",
        "    'f1': f1_h_cnn_b\n",
        "}\n",
        "with open(os.path.join(save_dir, 'hcnn_b_hierarchical_metrics.pkl'), 'wb') as f:\n",
        "    pickle.dump(hierarchical_results, f)\n",
        "\n",
        "# Save loss weights (alpha, beta, gamma)\n",
        "loss_weights_dict = {\n",
        "    'alpha': K.get_value(alpha),\n",
        "    'beta': K.get_value(beta),\n",
        "    'gamma': K.get_value(gamma)\n",
        "}\n",
        "with open(os.path.join(save_dir, 'hcnn_b_loss_weights.pkl'), 'wb') as f:\n",
        "    pickle.dump(loss_weights_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip of the results directory\n",
        "shutil.make_archive('hcnn_b_results', 'zip', 'results/hcnn_b')\n",
        "\n",
        "# Download the zip\n",
        "files.download('hcnn_b_results.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# Set the directory where everything is saved\n",
        "save_dir = '.\\CIFAR10_models\\saved_hcnn_b'\n",
        "\n",
        "# 1. Load the model (architecture + weights + optimizer state)\n",
        "h_cnn_b_model = keras.models.load_model(os.path.join(save_dir, 'hcnn_b_model.keras'))\n",
        "\n",
        "# 2. Load the model weights (optional — not needed if loading full model)\n",
        "# model.load_weights(os.path.join(save_dir, 'hcnn_b_weights.weights.h5'))\n",
        "\n",
        "# 3. Load the training history\n",
        "with open(os.path.join(save_dir, 'hcnn_b_history.pkl'), 'rb') as f:\n",
        "    history_hcnn_b = pickle.load(f)\n",
        "\n",
        "# 4. Load the predictions\n",
        "with open(os.path.join(save_dir, 'hcnn_b_predictions.pkl'), 'rb') as f:\n",
        "    h_cnn_b_predictions = pickle.load(f)\n",
        "\n",
        "# 5. Load evaluation scores\n",
        "with open(os.path.join(save_dir, 'hcnn_b_eval.pkl'), 'rb') as f:\n",
        "    score_h_cnn_b = pickle.load(f)\n",
        "\n",
        "# 6. Load number of parameters\n",
        "with open(os.path.join(save_dir, 'hcnn_b_params.pkl'), 'rb') as f:\n",
        "    parameters_h_cnn_b = pickle.load(f)\n",
        "\n",
        "# 7. Load hierarchical metrics\n",
        "with open(os.path.join(save_dir, 'hcnn_b_hierarchical_metrics.pkl'), 'rb') as f:\n",
        "    h_cnn_b_hierarchical_results = pickle.load(f)\n",
        "\n",
        "# Unpack hierarchical metrics if needed\n",
        "acc_h_cnn_b = h_cnn_b_hierarchical_results['accuracy']\n",
        "cons_h_cnn_b = h_cnn_b_hierarchical_results['consistency']\n",
        "f1_h_cnn_b = h_cnn_b_hierarchical_results['f1']\n",
        "\n",
        "# 8. Load loss weights (alpha, beta, gamma)\n",
        "with open(os.path.join(save_dir, 'hcnn_b_loss_weights.pkl'), 'rb') as f:\n",
        "    h_cnn_b_loss_weights_dict = pickle.load(f)\n",
        "\n",
        "# Unpack loss weights\n",
        "h_cnn_b_alpha_value = h_cnn_b_loss_weights_dict['alpha']\n",
        "h_cnn_b_beta_value = h_cnn_b_loss_weights_dict['beta']\n",
        "h_cnn_b_gamma_value = h_cnn_b_loss_weights_dict['gamma']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqFq_USMq-bi"
      },
      "source": [
        "# Add-net Base B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IlJVyh6sq-bi"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sd6oMpoHq-bi",
        "outputId": "5a3b9ea2-3659-47ed-d55c-384523d6cc97"
      },
      "outputs": [],
      "source": [
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(256, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch_out = Add()([c_1_bch_out,c_2_bch])\n",
        "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(256, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu', name='fc2')(x)\n",
        "x = Add()([x,c_2_bch_out])\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='add_net_Base_B')\n",
        "\n",
        "#----------------------- compile and fit ---------------------------\n",
        "#ADD - lr para learning_rate\n",
        "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
        "#ADD - Adiciona lista de loss e accuracy\n",
        "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy','accuracy','accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "-0hPWBieq-bi",
        "outputId": "d5aa7561-09a3-4a45-8041-5eac9acd72a7"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_addnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_addnet_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_addnet_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_addnet_b,cons_addnet_b,f1_addnet_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- Add-net Base B ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_addnet_b[4])\n",
        "print(\"Accuracy level 2:\",score_addnet_b[5])\n",
        "print(\"Accuracy level 3:\",score_addnet_b[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_addnet_b)\n",
        "print(\"Consistency:\",cons_addnet_b)\n",
        "print(\"f1:\",f1_addnet_b)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_addnet_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated directory path\n",
        "save_dir = 'results/add_net'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save model architecture and weights\n",
        "model.save(os.path.join(save_dir, 'addnet_b_model.keras'))  # Corrected extension\n",
        "model.save_weights(os.path.join(save_dir, 'addnet_b_weights.weights.h5'))  # Full .weights.h5\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(save_dir, 'addnet_b_history.pkl'), 'wb') as f:\n",
        "    pickle.dump(history_addnet_b.history, f)\n",
        "\n",
        "# Save loss weights variables (alpha, beta, gamma)\n",
        "with open(os.path.join(save_dir, 'addnet_b_loss_weights.pkl'), 'wb') as f:\n",
        "    pickle.dump({'alpha': K.get_value(alpha), 'beta': K.get_value(beta), 'gamma': K.get_value(gamma)}, f)\n",
        "\n",
        "# Save evaluation metrics\n",
        "with open(os.path.join(save_dir, 'addnet_b_evaluation.pkl'), 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'score': score_addnet_b,\n",
        "        'parameters': parameters_addnet_b,\n",
        "        'acc': acc_addnet_b,\n",
        "        'cons': cons_addnet_b,\n",
        "        'f1': f1_addnet_b\n",
        "    }, f)\n",
        "\n",
        "print(f\"Model and training artifacts saved to {save_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Define folder and zip paths\n",
        "folder_path = 'results/add_net'\n",
        "zip_path = 'add_net_results.zip'\n",
        "\n",
        "# Create a zip archive of the folder\n",
        "shutil.make_archive('add_net_results', 'zip', folder_path)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All model artifacts and important variables loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the directory where the files are saved\n",
        "save_dir = '.\\CIFAR10_models\\saved_add_net_b'\n",
        "\n",
        "# 1. Load the full model (architecture + weights + optimizer state)\n",
        "add_net_b_model = keras.models.load_model(os.path.join(save_dir, 'addnet_b_model.keras'))\n",
        "\n",
        "# 2. Load model weights (optional redundancy)\n",
        "# model.load_weights(os.path.join(save_dir, 'addnet_b_weights.weights.h5'))\n",
        "\n",
        "# 3. Load the training history\n",
        "with open(os.path.join(save_dir, 'addnet_b_history.pkl'), 'rb') as f:\n",
        "    history_addnet_b = pickle.load(f)\n",
        "\n",
        "# 4. Load the loss weights (alpha, beta, gamma)\n",
        "with open(os.path.join(save_dir, 'addnet_b_loss_weights.pkl'), 'rb') as f:\n",
        "    add_net_b_loss_weights = pickle.load(f)\n",
        "\n",
        "# Unpack alpha, beta, gamma values\n",
        "add_net_b_alpha_value = add_net_b_loss_weights['alpha']\n",
        "add_net_b_beta_value = add_net_b_loss_weights['beta']\n",
        "add_net_b_gamma_value = add_net_b_loss_weights['gamma']\n",
        "\n",
        "# 5. Load the evaluation metrics\n",
        "with open(os.path.join(save_dir, 'addnet_b_evaluation.pkl'), 'rb') as f:\n",
        "    add_net_b_evaluation_metrics = pickle.load(f)\n",
        "\n",
        "# Unpack evaluation metrics\n",
        "score_addnet_b = add_net_b_evaluation_metrics['score']\n",
        "parameters_addnet_b = add_net_b_evaluation_metrics['parameters']\n",
        "acc_addnet_b = add_net_b_evaluation_metrics['acc']\n",
        "cons_addnet_b = add_net_b_evaluation_metrics['cons']\n",
        "f1_addnet_b = add_net_b_evaluation_metrics['f1']\n",
        "\n",
        "print(\"✅ All model artifacts and important variables loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14rZlVpoq-bi"
      },
      "source": [
        "# Concat-net Base B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2pWsLxoq-bi"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Zv_5xXSq-bi"
      },
      "outputs": [],
      "source": [
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch_out = Concatenate()([c_1_bch_out,c_2_bch])\n",
        "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu', name='fc2')(x)\n",
        "x = Concatenate()([x,c_2_bch_out])\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='Concatnet_Base_B')\n",
        "\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLFDOlBxq-bj"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_concatnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_concatnet_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_concatnet_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_concatnet_b,cons_concatnet_b,f1_concatnet_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- Concat-net Base B ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_concatnet_b[4])\n",
        "print(\"Accuracy level 2:\",score_concatnet_b[5])\n",
        "print(\"Accuracy level 3:\",score_concatnet_b[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_concatnet_b)\n",
        "print(\"Consistency:\",cons_concatnet_b)\n",
        "print(\"f1:\",f1_concatnet_b)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_concatnet_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create folder if it doesn't exist\n",
        "save_dir = 'results/concat_net'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save model architecture + weights\n",
        "model.save(os.path.join(save_dir, 'concat_net_base_b.keras'))  # Full model\n",
        "model.save_weights(os.path.join(save_dir, 'concat_net_base_b.weights.h5'))  # Just weights\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(save_dir, 'history_concatnet_b.pkl'), 'wb') as f:\n",
        "    pickle.dump(history_concatnet_b.history, f)\n",
        "\n",
        "# Save results and metadata\n",
        "results_concatnet_b = {\n",
        "    \"bt_strategy\": bt_strategy,\n",
        "    \"alpha\": float(K.get_value(alpha)),\n",
        "    \"beta\": float(K.get_value(beta)),\n",
        "    \"gamma\": float(K.get_value(gamma)),\n",
        "    \"score\": score_concatnet_b,\n",
        "    \"parameters\": int(parameters_concatnet_b),\n",
        "    \"acc\": float(acc_concatnet_b),\n",
        "    \"cons\": float(cons_concatnet_b),\n",
        "    \"f1\": float(f1_concatnet_b),\n",
        "}\n",
        "\n",
        "with open(os.path.join(save_dir, 'results_concatnet_b.pkl'), 'wb') as f:\n",
        "    pickle.dump(results_concatnet_b, f)\n",
        "\n",
        "print(\"✅ All components saved to 'results/concat_net'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(\"concat_net\", 'zip', \"results/concat_net\")\n",
        "\n",
        "# Download the zip file\n",
        "files.download(\"concat_net.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model, history, and experiment metadata loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define the directory where the files are saved\n",
        "save_dir = '.\\CIFAR10_models\\saved_concat_net_b'\n",
        "\n",
        "# 1. Load the full model (architecture + weights + optimizer state)\n",
        "concat_net_b_model = keras.models.load_model(os.path.join(save_dir, 'concat_net_base_b.keras'))\n",
        "\n",
        "# 2. (Optional) Load model weights separately if needed (not necessary if you loaded the full model)\n",
        "concat_net_b_model.load_weights(os.path.join(save_dir, 'concat_net_base_b.weights.h5'))\n",
        "\n",
        "# 3. Load the training history\n",
        "with open(os.path.join(save_dir, 'history_concatnet_b.pkl'), 'rb') as f:\n",
        "    history_concatnet_b = pickle.load(f)\n",
        "\n",
        "# 4. Load the results and metadata\n",
        "with open(os.path.join(save_dir, 'results_concatnet_b.pkl'), 'rb') as f:\n",
        "    results_concatnet_b = pickle.load(f)\n",
        "\n",
        "# Unpack the important variables\n",
        "concat_net_b_bt_strategy = results_concatnet_b[\"bt_strategy\"]\n",
        "concat_net_b_alpha_value = results_concatnet_b[\"alpha\"]\n",
        "concat_net_b_beta_value = results_concatnet_b[\"beta\"]\n",
        "concat_net_b_gamma_value = results_concatnet_b[\"gamma\"]\n",
        "score_concatnet_b = results_concatnet_b[\"score\"]\n",
        "parameters_concatnet_b = results_concatnet_b[\"parameters\"]\n",
        "acc_concatnet_b = results_concatnet_b[\"acc\"]\n",
        "cons_concatnet_b = results_concatnet_b[\"cons\"]\n",
        "f1_concatnet_b = results_concatnet_b[\"f1\"]\n",
        "\n",
        "print(\"✅ Model, history, and experiment metadata loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcqC1GZ6kLlH"
      },
      "source": [
        "# Flat CNN Base C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cpunwm3o5UAx"
      },
      "outputs": [],
      "source": [
        "#----------get VGG16 pre-trained weights--------\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                         WEIGHTS_PATH,\n",
        "                         cache_subdir='models')\n",
        "#----------------------- model definition ---------------------------\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- block 5 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, fine_pred, name='flat_cnn_base_c')\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KojpAgRHkgig"
      },
      "outputs": [],
      "source": [
        "change_lr = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Training\n",
        "history_base_c = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=change_lr,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluation on test set\n",
        "score_base_c = model.evaluate(x_test, y_test, verbose=0)\n",
        "parameters_base_c = np.sum([K.count_params(w) for w in model.trainable_weights])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Ensure your trained model is named `model`\n",
        "# and your training results exist: history_base_c, score_base_c, etc.\n",
        "\n",
        "# --- Customize below if needed ---\n",
        "save_dir = \"results/flat_cnn_base_c\"\n",
        "model_name = \"flat_cnn_base_c\"\n",
        "model_path = os.path.join(save_dir, \"model.keras\")\n",
        "zip_path = f\"{model_name}.zip\"\n",
        "\n",
        "# Create directory\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "model.save(model_path)\n",
        "\n",
        "# Save additional data\n",
        "with open(os.path.join(save_dir, \"history.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(history_base_c, f)\n",
        "\n",
        "with open(os.path.join(save_dir, \"score.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(score_base_c, f)\n",
        "\n",
        "with open(os.path.join(save_dir, \"params.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(parameters_base_c, f)\n",
        "\n",
        "meta = {\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs,\n",
        "    \"input_shape\": input_shape,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"model_name\": model_name,\n",
        "    \"optimizer\": \"SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\",\n",
        "    \"loss\": \"categorical_crossentropy\",\n",
        "    \"metrics\": [\"accuracy\"]\n",
        "}\n",
        "with open(os.path.join(save_dir, \"meta.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(meta, f)\n",
        "\n",
        "# Zip everything\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(save_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, start=save_dir)\n",
        "            zipf.write(file_path, arcname=arcname)\n",
        "\n",
        "print(f\"Saved and zipped into: {zip_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Path to your zip file\n",
        "zip_filename = 'flat_cnn_base_c.zip'\n",
        "\n",
        "# Trigger the download\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model, history, evaluation, parameters, and metadata loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# --- Set directory ---\n",
        "save_dir = \".\\CIFAR10_models\\saved_flat_cnn_c\"\n",
        "\n",
        "# 1. Load the full model (architecture + weights + optimizer state)\n",
        "model = load_model(os.path.join(save_dir, \"model.keras\"))\n",
        "\n",
        "# 2. Load training history\n",
        "with open(os.path.join(save_dir, \"history.pkl\"), \"rb\") as f:\n",
        "    flat_cnn_history_base_c = pickle.load(f)\n",
        "\n",
        "# 3. Load evaluation score\n",
        "with open(os.path.join(save_dir, \"score.pkl\"), \"rb\") as f:\n",
        "    flat_cnn_score_base_c = pickle.load(f)\n",
        "\n",
        "# 4. Load number of parameters\n",
        "with open(os.path.join(save_dir, \"params.pkl\"), \"rb\") as f:\n",
        "    flat_cnn_parameters_base_c = pickle.load(f)\n",
        "\n",
        "# 5. Load meta information\n",
        "with open(os.path.join(save_dir, \"meta.pkl\"), \"rb\") as f:\n",
        "    flat_cnn_meta_base_c = pickle.load(f)\n",
        "\n",
        "# --- Optional: Unpack meta into variables ---\n",
        "flat_cnn_c_batch_size = flat_cnn_meta_base_c.get(\"batch_size\")\n",
        "flat_cnn_c_epochs = flat_cnn_meta_base_c.get(\"epochs\")\n",
        "flat_cnn_c_input_shape = flat_cnn_meta_base_c.get(\"input_shape\")\n",
        "flat_cnn_c_num_classes = flat_cnn_meta_base_c.get(\"num_classes\")\n",
        "flat_cnn_c_optimizer_used = flat_cnn_meta_base_c.get(\"optimizer\")\n",
        "flat_cnn_c_loss_used = flat_cnn_meta_base_c.get(\"loss\")\n",
        "flat_cnn_c_metrics_used = flat_cnn_meta_base_c.get(\"metrics\")\n",
        "flat_cnn_c_model_name = flat_cnn_meta_base_c.get(\"model_name\")\n",
        "\n",
        "print(\"✅ Model, history, evaluation, parameters, and metadata loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnW09JGQpo_J"
      },
      "source": [
        "# B-CNN Base C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_8ahOlc50aKP"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w9rtWZ07IK77"
      },
      "outputs": [],
      "source": [
        "#----------get VGG16 pre-trained weights--------\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                         WEIGHTS_PATH,\n",
        "                         cache_subdir='models')\n",
        "\n",
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- block 5 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bcnn_base_c')\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LzCzPlRV0uhW"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_b_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_b_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_b_cnn_c,cons_b_cnn_c,f1_b_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- B-CNN Base C ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_b_cnn_c[4])\n",
        "print(\"Accuracy level 2:\",score_b_cnn_c[5])\n",
        "print(\"Accuracy level 3:\",score_b_cnn_c[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_b_cnn_c)\n",
        "print(\"Consistency:\",cons_b_cnn_c)\n",
        "print(\"f1:\",f1_b_cnn_c)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_b_cnn_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "\n",
        "# Set up save directory\n",
        "save_dir = 'results/bcnn_base_c'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "model.save(os.path.join(save_dir, 'bcnn_base_c.keras'))\n",
        "\n",
        "# Save weights separately (optional redundancy)\n",
        "model.save_weights(os.path.join(save_dir, 'bcnn_base_c.weights.h5'))\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(save_dir, 'history_bcnn_base_c.pkl'), 'wb') as f:\n",
        "    pickle.dump(history_bcnn_b.history, f)\n",
        "\n",
        "# Save predictions\n",
        "with open(os.path.join(save_dir, 'predictions_bcnn_base_c.pkl'), 'wb') as f:\n",
        "    pickle.dump(predictions, f)\n",
        "\n",
        "# Save evaluation metrics\n",
        "with open(os.path.join(save_dir, 'evaluation_bcnn_base_c.pkl'), 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'score': score_b_cnn_c,\n",
        "        'parameters': int(parameters_b_cnn_c),\n",
        "        'accuracy': float(acc_b_cnn_c),\n",
        "        'consistency': float(cons_b_cnn_c),\n",
        "        'f1_score': float(f1_b_cnn_c)\n",
        "    }, f)\n",
        "\n",
        "# Save loss weights\n",
        "with open(os.path.join(save_dir, 'loss_weights_bcnn_base_c.pkl'), 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'alpha': float(K.get_value(alpha)),\n",
        "        'beta': float(K.get_value(beta)),\n",
        "        'gamma': float(K.get_value(gamma))\n",
        "    }, f)\n",
        "\n",
        "# Save meta info\n",
        "with open(os.path.join(save_dir, 'meta_bcnn_base_c.pkl'), 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'bt_strategy': bt_strategy,\n",
        "        'input_shape': input_shape,\n",
        "        'num_classes': num_classes,\n",
        "        'num_c_1': num_c_1,\n",
        "        'num_c_2': num_c_2,\n",
        "        'batch_size': batch_size,\n",
        "        'epochs': epochs,\n",
        "        'optimizer': 'SGD(learning_rate=0.003, momentum=0.9, nesterov=True)',\n",
        "        'loss': ['categorical_crossentropy'] * 3,\n",
        "        'metrics': ['accuracy'] * 3,\n",
        "        'pretrained_weights_source': WEIGHTS_PATH\n",
        "    }, f)\n",
        "\n",
        "# Zip the folder for download\n",
        "zip_path = 'bcnn_base_c.zip'\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(save_dir):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(full_path, save_dir)\n",
        "            zipf.write(full_path, arcname=arcname)\n",
        "\n",
        "print(\"✅ All components saved and zipped successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('bcnn_base_c.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model, history, predictions, evaluation, loss weights, and meta information loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Set directory where everything was saved\n",
        "save_dir = '.\\CIFAR10_models\\saved_bcnn_c'\n",
        "\n",
        "# --- 1. Load the full model (architecture + weights + optimizer state) ---\n",
        "b_cnn_c_model = load_model(os.path.join(save_dir, 'bcnn_base_c.keras'))\n",
        "\n",
        "# --- 2. Load the training history ---\n",
        "with open(os.path.join(save_dir, 'history_bcnn_base_c.pkl'), 'rb') as f:\n",
        "    history_bcnn_b = pickle.load(f)\n",
        "\n",
        "# --- 3. Load the predictions ---\n",
        "with open(os.path.join(save_dir, 'predictions_bcnn_base_c.pkl'), 'rb') as f:\n",
        "    b_cnn_c_predictions = pickle.load(f)\n",
        "\n",
        "# --- 4. Load the evaluation metrics ---\n",
        "with open(os.path.join(save_dir, 'evaluation_bcnn_base_c.pkl'), 'rb') as f:\n",
        "    evaluation_bcnn_base_c = pickle.load(f)\n",
        "    score_b_cnn_c = evaluation_bcnn_base_c['score']\n",
        "    parameters_b_cnn_c = evaluation_bcnn_base_c['parameters']\n",
        "    acc_b_cnn_c = evaluation_bcnn_base_c['accuracy']\n",
        "    cons_b_cnn_c = evaluation_bcnn_base_c['consistency']\n",
        "    f1_b_cnn_c = evaluation_bcnn_base_c['f1_score']\n",
        "\n",
        "# --- 5. Load the loss weights (alpha, beta, gamma values) ---\n",
        "with open(os.path.join(save_dir, 'loss_weights_bcnn_base_c.pkl'), 'rb') as f:\n",
        "    loss_weights_bcnn_base_c = pickle.load(f)\n",
        "    b_cnn_c_alpha_value = loss_weights_bcnn_base_c['alpha']\n",
        "    b_cnn_c_beta_value = loss_weights_bcnn_base_c['beta']\n",
        "    b_cnn_c_gamma_value = loss_weights_bcnn_base_c['gamma']\n",
        "\n",
        "# --- 6. Load the meta information (hyperparameters, dataset config, etc.) ---\n",
        "with open(os.path.join(save_dir, 'meta_bcnn_base_c.pkl'), 'rb') as f:\n",
        "    meta_bcnn_base_c = pickle.load(f)\n",
        "\n",
        "# --- 7. (Optional) Extract meta variables if you want them directly ---\n",
        "b_cnn_c_bt_strategy = meta_bcnn_base_c['bt_strategy']\n",
        "b_cnn_c_input_shape = meta_bcnn_base_c['input_shape']\n",
        "b_cnn_c_num_classes = meta_bcnn_base_c['num_classes']\n",
        "b_cnn_c_num_c_1 = meta_bcnn_base_c['num_c_1']\n",
        "b_cnn_c_num_c_2 = meta_bcnn_base_c['num_c_2']\n",
        "b_cnn_c_batch_size = meta_bcnn_base_c['batch_size']\n",
        "b_cnn_c_epochs = meta_bcnn_base_c['epochs']\n",
        "b_cnn_c_optimizer_used = meta_bcnn_base_c['optimizer']\n",
        "b_cnn_c_loss_used = meta_bcnn_base_c['loss']\n",
        "b_cnn_c_metrics_used = meta_bcnn_base_c['metrics']\n",
        "b_cnn_c_pretrained_weights_source = meta_bcnn_base_c['pretrained_weights_source']\n",
        "\n",
        "print(\"✅ Model, history, predictions, evaluation, loss weights, and meta information loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmTNGOqEpu4g"
      },
      "source": [
        "# BA-CNN Base C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7VZWx0yfWQZk"
      },
      "outputs": [],
      "source": [
        "# Best hyperparameters\n",
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True\n",
        "\n",
        "# neurons of all dense layers on each branch\n",
        "branch_neurons = 32 # Parsimonious version is 256\n",
        "\n",
        "# neurons of all attention mechanism\n",
        "att_neurons = 2048  # Parsimonious version is 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5FPODG2fcuuK"
      },
      "outputs": [],
      "source": [
        "#----------get VGG16 pre-trained weights--------\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                         WEIGHTS_PATH,\n",
        "                         cache_subdir='models')\n",
        "\n",
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch_out = Dropout(0.5)(c_1_bch)\n",
        "#c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch_out)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch_out = Dropout(0.5)(c_2_bch)\n",
        "#c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch_out)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- block 5 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x_out = Dropout(0.5)(x)\n",
        "\n",
        "#-- Att for coarse 1---\n",
        "# Coarse 1\n",
        "sfcn_1_1 = Dense(att_neurons, name='fc1_1')(c_1_bch_out)\n",
        "sfcn_1_1 = Dense(1, name='fc1_2')(sfcn_1_1)\n",
        "# Coarse 2\n",
        "sfcn_1_2 = Dense(att_neurons, name='fc1_3')(c_2_bch_out)\n",
        "sfcn_1_2 = Dense(1, name='fc1_4')(sfcn_1_2)\n",
        "# Fine\n",
        "sfcn_1_3 = Dense(att_neurons, name='fc1_5')(x_out)\n",
        "sfcn_1_3 = Dense(1, name='fc1_6')(sfcn_1_3)\n",
        "\n",
        "score_vector_1 = Concatenate()([sfcn_1_1,sfcn_1_2,sfcn_1_3]) # Score vector 1\n",
        "att_weights_1 = Activation('softmax', name='attention_weights_1')(score_vector_1) # Attention weights 1\n",
        "weightned_sum_1 = Add()([c_1_bch_out*att_weights_1[0][0],c_2_bch_out*att_weights_1[0][1],x_out*att_weights_1[0][2]]) # Weightned sum 1\n",
        "\n",
        "# Concat and prediction\n",
        "coarse_1_concat = Concatenate()([c_1_bch_out,weightned_sum_1])\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(coarse_1_concat)\n",
        "\n",
        "\n",
        "#-- Att for coarse 2---\n",
        "\n",
        "# Coarse 1\n",
        "sfcn_2_1 = Dense(att_neurons, name='fc2_1')(c_1_bch_out)\n",
        "sfcn_2_1 = Dense(1, name='fc2_2')(sfcn_2_1)\n",
        "# Coarse 2\n",
        "sfcn_2_2 = Dense(att_neurons, name='fc2_3')(c_2_bch_out)\n",
        "sfcn_2_2 = Dense(1, name='fc2_4')(sfcn_2_2)\n",
        "# Fine\n",
        "sfcn_2_3 = Dense(att_neurons, name='fc2_5')(x_out)\n",
        "sfcn_2_3 = Dense(1, name='fc2_6')(sfcn_2_3)\n",
        "\n",
        "score_vector_2 = Concatenate()([sfcn_2_1,sfcn_2_2,sfcn_2_3]) # Score vector 1\n",
        "att_weights_2 = Activation('softmax', name='attention_weights_2')(score_vector_2) # Attention weights 1\n",
        "weightned_sum_2 = Add()([c_1_bch_out*att_weights_2[0][0],c_2_bch_out*att_weights_2[0][1],x_out*att_weights_2[0][2]]) # Weightned sum 1\n",
        "\n",
        "# Concat and prediction\n",
        "coarse_2_concat = Concatenate()([c_2_bch_out,weightned_sum_2])\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(coarse_2_concat)\n",
        "\n",
        "\n",
        "#-- Att for fine---\n",
        "\n",
        "# Coarse 1\n",
        "sfcn_3_1 = Dense(att_neurons, name='fc3_1')(c_1_bch_out)\n",
        "sfcn_3_1 = Dense(1, name='fc3_2')(sfcn_3_1)\n",
        "# Coarse 2\n",
        "sfcn_3_2 = Dense(att_neurons, name='fc3_3')(c_2_bch_out)\n",
        "sfcn_3_2 = Dense(1, name='fc3_4')(sfcn_3_2)\n",
        "# Fine\n",
        "sfcn_3_3 = Dense(att_neurons, name='fc3_5')(x_out)\n",
        "sfcn_3_3 = Dense(1, name='fc3_6')(sfcn_3_3)\n",
        "\n",
        "score_vector_3 = Concatenate()([sfcn_3_1,sfcn_3_2,sfcn_3_3]) # Score vector 1\n",
        "att_weights_3 = Activation('softmax', name='attention_weights_3')(score_vector_3) # Attention weights 1\n",
        "weightned_sum_3 = Add()([c_1_bch_out*att_weights_3[0][0],c_2_bch_out*att_weights_3[0][1],x_out*att_weights_3[0][2]]) # Weightned sum 3\n",
        "\n",
        "# Concat and prediction\n",
        "fine_concat = Concatenate()([x_out,weightned_sum_3])\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(fine_concat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bacnn_base_c')\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g_l6k6PeWZOr"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_ba_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_ba_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_ba_cnn_c,cons_ba_cnn_c,f1_ba_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- BA-CNN Base C ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_ba_cnn_c[4])\n",
        "print(\"Accuracy level 2:\",score_ba_cnn_c[5])\n",
        "print(\"Accuracy level 3:\",score_ba_cnn_c[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_ba_cnn_c)\n",
        "print(\"Consistency:\",cons_ba_cnn_c)\n",
        "print(\"f1:\",f1_ba_cnn_c)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_ba_cnn_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "\n",
        "# Directory to save model artifacts\n",
        "save_dir = \"results/bacnn_base_c\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save model and weights\n",
        "model.save(os.path.join(save_dir, \"bacnn_base_c.keras\"))\n",
        "model.save_weights(os.path.join(save_dir, \"bacnn_base_c.weights.h5\"))\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(save_dir, \"history_bacnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(history_bcnn_b.history, f)\n",
        "\n",
        "# Save predictions\n",
        "with open(os.path.join(save_dir, \"predictions_bacnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(predictions, f)\n",
        "\n",
        "# Save evaluation\n",
        "with open(os.path.join(save_dir, \"evaluation_bacnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"score\": score_ba_cnn_c,\n",
        "        \"parameters\": int(parameters_ba_cnn_c),\n",
        "        \"accuracy\": float(acc_ba_cnn_c),\n",
        "        \"consistency\": float(cons_ba_cnn_c),\n",
        "        \"f1_score\": float(f1_ba_cnn_c)\n",
        "    }, f)\n",
        "\n",
        "# Save loss weights\n",
        "with open(os.path.join(save_dir, \"loss_weights_bacnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"alpha\": float(K.get_value(alpha)),\n",
        "        \"beta\": float(K.get_value(beta)),\n",
        "        \"gamma\": float(K.get_value(gamma))\n",
        "    }, f)\n",
        "\n",
        "# Save metadata\n",
        "with open(os.path.join(save_dir, \"meta_bacnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"model_name\": \"bacnn_base_c\",\n",
        "        \"bt_strategy\": bt_strategy,\n",
        "        \"branch_neurons\": branch_neurons,\n",
        "        \"att_neurons\": att_neurons,\n",
        "        \"input_shape\": input_shape,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"num_c_1\": num_c_1,\n",
        "        \"num_c_2\": num_c_2,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"optimizer\": \"SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\",\n",
        "        \"loss\": [\"categorical_crossentropy\"] * 3,\n",
        "        \"metrics\": [\"accuracy\"] * 3,\n",
        "        \"pretrained_weights_source\": WEIGHTS_PATH\n",
        "    }, f)\n",
        "\n",
        "# Zip everything\n",
        "zip_path = \"bacnn_base_c.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(save_dir):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(full_path, save_dir)\n",
        "            zipf.write(full_path, arcname=os.path.join(\"bacnn_base_c\", arcname))\n",
        "\n",
        "# Download the zip\n",
        "from google.colab import files as gfiles\n",
        "gfiles.download(\"bacnn_base_c.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model, history, predictions, evaluation, loss weights, and meta loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Set directory where everything was saved\n",
        "save_dir = \"./CIFAR10_models/saved_bacnn_c/bacnn_base_c\"\n",
        "\n",
        "# --- 1. Load the full model (architecture + weights + optimizer state) ---\n",
        "ba_cnn_c_model = load_model(os.path.join(save_dir, \"bacnn_base_c.keras\"))\n",
        "\n",
        "# --- 2. Load the training history ---\n",
        "with open(os.path.join(save_dir, \"history_bacnn_base_c.pkl\"), \"rb\") as f:\n",
        "    history_bacnn_b = pickle.load(f)\n",
        "\n",
        "# --- 3. Load the predictions ---\n",
        "with open(os.path.join(save_dir, \"predictions_bacnn_base_c.pkl\"), \"rb\") as f:\n",
        "    ba_cnn_c_predictions = pickle.load(f)\n",
        "\n",
        "# --- 4. Load the evaluation scores ---\n",
        "with open(os.path.join(save_dir, \"evaluation_bacnn_base_c.pkl\"), \"rb\") as f:\n",
        "    ba_cnn_c_evaluation_data = pickle.load(f)\n",
        "    score_ba_cnn_c = ba_cnn_c_evaluation_data[\"score\"]\n",
        "    parameters_ba_cnn_c = ba_cnn_c_evaluation_data[\"parameters\"]\n",
        "    acc_ba_cnn_c = ba_cnn_c_evaluation_data[\"accuracy\"]\n",
        "    cons_ba_cnn_c = ba_cnn_c_evaluation_data[\"consistency\"]\n",
        "    f1_ba_cnn_c = ba_cnn_c_evaluation_data[\"f1_score\"]\n",
        "\n",
        "# --- 5. Load the loss weights (alpha, beta, gamma values) ---\n",
        "with open(os.path.join(save_dir, \"loss_weights_bacnn_base_c.pkl\"), \"rb\") as f:\n",
        "    ba_cnn_c_loss_weights_data = pickle.load(f)\n",
        "    ba_cnn_c_alpha_value = ba_cnn_c_loss_weights_data[\"alpha\"]\n",
        "    ba_cnn_c_beta_value = ba_cnn_c_loss_weights_data[\"beta\"]\n",
        "    ba_cnn_c_gamma_value = ba_cnn_c_loss_weights_data[\"gamma\"]\n",
        "\n",
        "# --- 6. Load the metadata (model config and training hyperparameters) ---\n",
        "with open(os.path.join(save_dir, \"meta_bacnn_base_c.pkl\"), \"rb\") as f:\n",
        "    ba_cnn_c_meta_data = pickle.load(f)\n",
        "\n",
        "# --- 7. (Optional) Assign meta data variables separately if needed ---\n",
        "ba_cnn_c_model_name = ba_cnn_c_meta_data[\"model_name\"]\n",
        "ba_cnn_c_bt_strategy = ba_cnn_c_meta_data[\"bt_strategy\"]\n",
        "ba_cnn_c_branch_neurons = ba_cnn_c_meta_data[\"branch_neurons\"]\n",
        "ba_cnn_c_att_neurons = ba_cnn_c_meta_data[\"att_neurons\"]\n",
        "ba_cnn_c_input_shape = ba_cnn_c_meta_data[\"input_shape\"]\n",
        "ba_cnn_c_num_classes = ba_cnn_c_meta_data[\"num_classes\"]\n",
        "ba_cnn_c_num_c_1 = ba_cnn_c_meta_data[\"num_c_1\"]\n",
        "ba_cnn_c_num_c_2 = ba_cnn_c_meta_data[\"num_c_2\"]\n",
        "ba_cnn_c_batch_size = ba_cnn_c_meta_data[\"batch_size\"]\n",
        "ba_cnn_c_epochs = ba_cnn_c_meta_data[\"epochs\"]\n",
        "ba_cnn_c_optimizer_info = ba_cnn_c_meta_data[\"optimizer\"]\n",
        "ba_cnn_c_loss_info = ba_cnn_c_meta_data[\"loss\"]\n",
        "ba_cnn_c_metrics_info = ba_cnn_c_meta_data[\"metrics\"]\n",
        "ba_cnn_c_pretrained_weights_source = ba_cnn_c_meta_data[\"pretrained_weights_source\"]\n",
        "\n",
        "print(\"✅ Model, history, predictions, evaluation, loss weights, and meta loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7qKvpSMp2iE"
      },
      "source": [
        "# H-CNN Base C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9pshU_L-p0Sm"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wJ7BXWiAFQ_J"
      },
      "outputs": [],
      "source": [
        "#----------get VGG16 pre-trained weights--------\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                         WEIGHTS_PATH,\n",
        "                         cache_subdir='models')\n",
        "\n",
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch_flatt = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch_flatt)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch_flatt = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch_concat = Concatenate()([c_1_bch_flatt,c_2_bch_flatt]) # Conectivity Pattern\n",
        "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch_concat)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- block 5 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x_flatt = Flatten(name='flatten')(x)\n",
        "x = Concatenate()([c_2_bch_concat,x_flatt]) # Conectivity Pattern\n",
        "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='hcnn_base_c')\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uTUo07wAW__7"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_h_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_h_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_h_cnn_c,cons_h_cnn_c,f1_h_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- H-CNN Base C ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_h_cnn_c[4])\n",
        "print(\"Accuracy level 2:\",score_h_cnn_c[5])\n",
        "print(\"Accuracy level 3:\",score_h_cnn_c[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_h_cnn_c)\n",
        "print(\"Consistency:\",cons_h_cnn_c)\n",
        "print(\"f1:\",f1_h_cnn_c)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_h_cnn_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "\n",
        "# Save directory\n",
        "save_dir = \"results/hcnn_base_c\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save model and weights\n",
        "model.save(os.path.join(save_dir, \"hcnn_base_c.keras\"))\n",
        "model.save_weights(os.path.join(save_dir, \"hcnn_base_c.weights.h5\"))\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(save_dir, \"history_hcnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(history_bcnn_b.history, f)\n",
        "\n",
        "# Save predictions\n",
        "with open(os.path.join(save_dir, \"predictions_hcnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(predictions, f)\n",
        "\n",
        "# Save evaluation\n",
        "with open(os.path.join(save_dir, \"evaluation_hcnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"score\": score_h_cnn_c,\n",
        "        \"parameters\": int(parameters_h_cnn_c),\n",
        "        \"accuracy\": float(acc_h_cnn_c),\n",
        "        \"consistency\": float(cons_h_cnn_c),\n",
        "        \"f1_score\": float(f1_h_cnn_c)\n",
        "    }, f)\n",
        "\n",
        "# Save dynamic loss weights\n",
        "with open(os.path.join(save_dir, \"loss_weights_hcnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"alpha\": float(K.get_value(alpha)),\n",
        "        \"beta\": float(K.get_value(beta)),\n",
        "        \"gamma\": float(K.get_value(gamma))\n",
        "    }, f)\n",
        "\n",
        "# Save metadata\n",
        "with open(os.path.join(save_dir, \"meta_hcnn_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"model_name\": \"hcnn_base_c\",\n",
        "        \"bt_strategy\": bt_strategy,\n",
        "        \"input_shape\": input_shape,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"num_c_1\": num_c_1,\n",
        "        \"num_c_2\": num_c_2,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"optimizer\": \"SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\",\n",
        "        \"loss\": [\"categorical_crossentropy\"] * 3,\n",
        "        \"metrics\": [\"accuracy\"] * 3,\n",
        "        \"pretrained_weights_source\": WEIGHTS_PATH\n",
        "    }, f)\n",
        "\n",
        "# Zip and download\n",
        "zip_path = \"hcnn_base_c.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files in os.walk(save_dir):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(full_path, save_dir)\n",
        "            zipf.write(full_path, arcname=os.path.join(\"hcnn_base_c\", arcname))\n",
        "\n",
        "\n",
        "# Download the zip\n",
        "from google.colab import files as gfiles\n",
        "gfiles.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model, history, predictions, evaluation, loss weights, and metadata loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define the directory where everything was saved\n",
        "save_dir = \".\\CIFAR10_models\\saved_hcnn_c\\hcnn_base_c\"\n",
        "\n",
        "# --- 1. Load the full model (architecture + weights + optimizer) ---\n",
        "h_cnn_c_model = load_model(os.path.join(save_dir, \"hcnn_base_c.keras\"))\n",
        "\n",
        "# --- 2. Load the training history ---\n",
        "with open(os.path.join(save_dir, \"history_hcnn_base_c.pkl\"), \"rb\") as f:\n",
        "    history_hcnn_b = pickle.load(f)\n",
        "\n",
        "# --- 3. Load the predictions ---\n",
        "with open(os.path.join(save_dir, \"predictions_hcnn_base_c.pkl\"), \"rb\") as f:\n",
        "    h_cnn_c_predictions = pickle.load(f)\n",
        "\n",
        "# --- 4. Load evaluation results ---\n",
        "with open(os.path.join(save_dir, \"evaluation_hcnn_base_c.pkl\"), \"rb\") as f:\n",
        "    h_cnn_c_evaluation_data = pickle.load(f)\n",
        "    score_h_cnn_c = h_cnn_c_evaluation_data[\"score\"]\n",
        "    parameters_h_cnn_c = h_cnn_c_evaluation_data[\"parameters\"]\n",
        "    acc_h_cnn_c = h_cnn_c_evaluation_data[\"accuracy\"]\n",
        "    cons_h_cnn_c = h_cnn_c_evaluation_data[\"consistency\"]\n",
        "    f1_h_cnn_c = h_cnn_c_evaluation_data[\"f1_score\"]\n",
        "\n",
        "# --- 5. Load loss weights (alpha, beta, gamma values) ---\n",
        "with open(os.path.join(save_dir, \"loss_weights_hcnn_base_c.pkl\"), \"rb\") as f:\n",
        "    h_cnn_c_loss_weights_data = pickle.load(f)\n",
        "    h_cnn_c_alpha_value = h_cnn_c_loss_weights_data[\"alpha\"]\n",
        "    h_cnn_c_beta_value = h_cnn_c_loss_weights_data[\"beta\"]\n",
        "    h_cnn_c_gamma_value = h_cnn_c_loss_weights_data[\"gamma\"]\n",
        "\n",
        "# --- 6. Load the metadata ---\n",
        "with open(os.path.join(save_dir, \"meta_hcnn_base_c.pkl\"), \"rb\") as f:\n",
        "    h_cnn_c_meta_data = pickle.load(f)\n",
        "\n",
        "# --- 7. (Optional) Assign meta data to separate variables if needed ---\n",
        "h_cnn_c_model_name = h_cnn_c_meta_data[\"model_name\"]\n",
        "h_cnn_c_bt_strategy = h_cnn_c_meta_data[\"bt_strategy\"]\n",
        "h_cnn_c_input_shape = h_cnn_c_meta_data[\"input_shape\"]\n",
        "h_cnn_c_num_classes = h_cnn_c_meta_data[\"num_classes\"]\n",
        "h_cnn_c_num_c_1 = h_cnn_c_meta_data[\"num_c_1\"]\n",
        "h_cnn_c_num_c_2 = h_cnn_c_meta_data[\"num_c_2\"]\n",
        "h_cnn_c_batch_size = h_cnn_c_meta_data[\"batch_size\"]\n",
        "h_cnn_c_epochs = h_cnn_c_meta_data[\"epochs\"]\n",
        "h_cnn_c_optimizer_info = h_cnn_c_meta_data[\"optimizer\"]\n",
        "h_cnn_c_loss_info = h_cnn_c_meta_data[\"loss\"]\n",
        "h_cnn_c_metrics_info = h_cnn_c_meta_data[\"metrics\"]\n",
        "h_cnn_c_pretrained_weights_source = h_cnn_c_meta_data[\"pretrained_weights_source\"]\n",
        "\n",
        "print(\"✅ Model, history, predictions, evaluation, loss weights, and metadata loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZviMkG1Nq-bo"
      },
      "source": [
        "# Add-net Base C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MaI40JVq-bo"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKMDTxMmq-bp"
      },
      "outputs": [],
      "source": [
        "#----------get VGG16 pre-trained weights--------\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                         WEIGHTS_PATH,\n",
        "                         cache_subdir='models')\n",
        "\n",
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(256, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch_out= Add()([c_1_bch_out,c_2_bch])\n",
        "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- block 5 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(256, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu', name='fc2_cifar10_1')(x)\n",
        "x = Add()([x,c_2_bch_out])\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='addnet_base_c')\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFpSKSezq-bp"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_addnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_addnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_addnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_addnet_c,cons_addnet_c,f1_addnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- Add-net Base C ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_addnet_c[4])\n",
        "print(\"Accuracy level 2:\",score_addnet_c[5])\n",
        "print(\"Accuracy level 3:\",score_addnet_c[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_addnet_c)\n",
        "print(\"Consistency:\",cons_addnet_c)\n",
        "print(\"f1:\",f1_addnet_c)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_addnet_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Create a directory to store the model data\n",
        "save_dir = \"results/addnet_base_c\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save the model architecture and weights\n",
        "model.save(os.path.join(save_dir, \"addnet_base_c.keras\"))\n",
        "model.save_weights(os.path.join(save_dir, \"addnet_base_c.weights.h5\"))\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(save_dir, \"history_addnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(history_addnet_b.history, f)\n",
        "\n",
        "# Save predictions\n",
        "with open(os.path.join(save_dir, \"predictions_addnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(predictions, f)\n",
        "\n",
        "# Save evaluation metrics\n",
        "with open(os.path.join(save_dir, \"evaluation_addnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"score\": score_addnet_c,\n",
        "        \"parameters\": int(parameters_addnet_c),\n",
        "        \"accuracy\": float(acc_addnet_c),\n",
        "        \"consistency\": float(cons_addnet_c),\n",
        "        \"f1_score\": float(f1_addnet_c)\n",
        "    }, f)\n",
        "\n",
        "# Save loss weights\n",
        "with open(os.path.join(save_dir, \"loss_weights_addnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"alpha\": float(K.get_value(alpha)),\n",
        "        \"beta\": float(K.get_value(beta)),\n",
        "        \"gamma\": float(K.get_value(gamma))\n",
        "    }, f)\n",
        "\n",
        "# Save meta information\n",
        "with open(os.path.join(save_dir, \"meta_addnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"model_name\": \"addnet_base_c\",\n",
        "        \"bt_strategy\": bt_strategy,\n",
        "        \"input_shape\": input_shape,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"num_c_1\": num_c_1,\n",
        "        \"num_c_2\": num_c_2,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"optimizer\": \"SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\",\n",
        "        \"loss\": [\"categorical_crossentropy\"] * 3,\n",
        "        \"metrics\": [\"accuracy\"] * 3,\n",
        "        \"pretrained_weights_source\": WEIGHTS_PATH\n",
        "    }, f)\n",
        "\n",
        "# Zip everything into one file\n",
        "zip_path = \"addnet_base_c.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files_list in os.walk(save_dir):\n",
        "        for file in files_list:\n",
        "            full_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(full_path, os.path.dirname(save_dir))\n",
        "            zipf.write(full_path, arcname)\n",
        "\n",
        "# Download the zip file to your computer\n",
        "files.download(zip_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Define the directory where the zip file will be extracted\n",
        "extract_dir = \"./CIFAR10_models/saved_add_net_c/addnet_base_c\"\n",
        "\n",
        "# Load the model architecture and weights\n",
        "add_net_c_model = load_model(os.path.join(extract_dir, \"addnet_base_c.keras\"))\n",
        "add_net_c_model.load_weights(os.path.join(extract_dir, \"addnet_base_c.weights.h5\"))\n",
        "\n",
        "# Load training history\n",
        "with open(os.path.join(extract_dir, \"history_addnet_base_c.pkl\"), \"rb\") as f:\n",
        "    history_addnet_c = pickle.load(f)\n",
        "\n",
        "# Load predictions\n",
        "with open(os.path.join(extract_dir, \"predictions_addnet_base_c.pkl\"), \"rb\") as f:\n",
        "    add_net_c_predictions = pickle.load(f)\n",
        "\n",
        "# Load evaluation metrics\n",
        "with open(os.path.join(extract_dir, \"evaluation_addnet_base_c.pkl\"), \"rb\") as f:\n",
        "    evaluation_addnet_c = pickle.load(f)\n",
        "    score_addnet_c = evaluation_addnet_c[\"score\"]\n",
        "    parameters_addnet_c = evaluation_addnet_c[\"parameters\"]\n",
        "    acc_addnet_c = evaluation_addnet_c[\"accuracy\"]\n",
        "    consistency_addnet_c = evaluation_addnet_c[\"consistency\"]\n",
        "    f1_addnet_c = evaluation_addnet_c[\"f1_score\"]\n",
        "\n",
        "# Load loss weights\n",
        "with open(os.path.join(extract_dir, \"loss_weights_addnet_base_c.pkl\"), \"rb\") as f:\n",
        "    loss_weights_addnet_c = pickle.load(f)\n",
        "    add_net_c_alpha = K.variable(loss_weights_addnet_c[\"alpha\"])\n",
        "    add_net_c_beta = K.variable(loss_weights_addnet_c[\"beta\"])\n",
        "    add_net_c_gamma = K.variable(loss_weights_addnet_c[\"gamma\"])\n",
        "\n",
        "# Load meta information\n",
        "with open(os.path.join(extract_dir, \"meta_addnet_base_c.pkl\"), \"rb\") as f:\n",
        "    meta_addnet_c = pickle.load(f)\n",
        "    add_net_c_model_name = meta_addnet_c[\"model_name\"]\n",
        "    add_net_c_bt_strategy = meta_addnet_c[\"bt_strategy\"]\n",
        "    add_net_c_input_shape = meta_addnet_c[\"input_shape\"]\n",
        "    add_net_c_num_classes = meta_addnet_c[\"num_classes\"]\n",
        "    add_net_c_num_c_1 = meta_addnet_c[\"num_c_1\"]\n",
        "    add_net_c_num_c_2 = meta_addnet_c[\"num_c_2\"]\n",
        "    add_net_c_batch_size = meta_addnet_c[\"batch_size\"]\n",
        "    add_net_c_epochs = meta_addnet_c[\"epochs\"]\n",
        "    add_net_c_optimizer = meta_addnet_c[\"optimizer\"]\n",
        "    add_net_c_loss = meta_addnet_c[\"loss\"]\n",
        "    add_net_c_metrics = meta_addnet_c[\"metrics\"]\n",
        "    add_net_c_pretrained_weights_source = meta_addnet_c[\"pretrained_weights_source\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcxhdYepq-bq"
      },
      "source": [
        "# Concat-net Base C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvnS8xoDq-br"
      },
      "outputs": [],
      "source": [
        "# if True, the model uses BT-strategy for training\n",
        "bt_strategy = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlAy04sZq-br"
      },
      "outputs": [],
      "source": [
        "#----------get VGG16 pre-trained weights--------\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                         WEIGHTS_PATH,\n",
        "                         cache_subdir='models')\n",
        "\n",
        "#----------------------- model definition ---------------------------\n",
        "if bt_strategy == True:\n",
        "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "else:\n",
        "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
        "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
        "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
        "\n",
        "\n",
        "img_input = Input(shape=input_shape, name='input')\n",
        "\n",
        "#--- block 1 ---\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "#--- block 2 ---\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "#--- coarse 1 branch ---\n",
        "c_1_bch = Flatten(name='c1_flatten')(x)\n",
        "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_bch_out = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
        "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
        "c_1_bch = Dropout(0.5)(c_1_bch)\n",
        "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
        "\n",
        "#--- block 3 ---\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "#--- coarse 2 branch ---\n",
        "c_2_bch = Flatten(name='c2_flatten')(x)\n",
        "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
        "c_2_bch = BatchNormalization()(c_2_bch)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
        "c_2_bch_out= Concatenate()([c_1_bch_out,c_2_bch])\n",
        "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
        "c_2_bch = Dropout(0.5)(c_2_bch)\n",
        "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
        "\n",
        "#--- block 4 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "#--- block 5 ---\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "#--- fine block ---\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4096, activation='relu', name='fc2_cifar10_1')(x)\n",
        "x = Concatenate()([x,c_2_bch_out])\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
        "\n",
        "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='concatnet_base_c')\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "#----------------------- compile and fit ---------------------------\n",
        "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              loss_weights=[alpha, beta, gamma],\n",
        "              # optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZY0Ow9Mq-br"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
        "\n",
        "if bt_strategy == True:\n",
        "  cbks = [change_lr, change_lw]\n",
        "else:\n",
        "  cbks = [change_lr]\n",
        "\n",
        "history_concatnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=0,\n",
        "          callbacks=cbks,\n",
        "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
        "\n",
        "score_concatnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
        "parameters_concatnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "acc_concatnet_c,cons_concatnet_c,f1_concatnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
        "\n",
        "# Results\n",
        "print(\"--- Concat-net Base C ---\")\n",
        "print(\"--- Accuracy per level ---\")\n",
        "print(\"Accuracy level 1:\",score_concatnet_c[4])\n",
        "print(\"Accuracy level 2:\",score_concatnet_c[5])\n",
        "print(\"Accuracy level 3:\",score_concatnett_c[6])\n",
        "print(\"--- Hierarchical Metrics ---\")\n",
        "print(\"Accuracy:\",acc_concatnet_c)\n",
        "print(\"Consitency:\",cons_concatnet_c)\n",
        "print(\"f1:\",f1_concatnet_c)\n",
        "print(\"Parameters:\",\"{:,}\".format(parameters_concatnet_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Create a directory to store the model data\n",
        "save_dir = \"results/concatnet_base_c\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save the model architecture and weights\n",
        "model.save(os.path.join(save_dir, \"concatnet_base_c.keras\"))\n",
        "model.save_weights(os.path.join(save_dir, \"concatnet_base_c.weights.h5\"))\n",
        "\n",
        "# Save training history\n",
        "with open(os.path.join(save_dir, \"history_concatnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(history_concatnet_b.history, f)\n",
        "\n",
        "# Save predictions\n",
        "with open(os.path.join(save_dir, \"predictions_concatnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(predictions, f)\n",
        "\n",
        "# Save evaluation metrics\n",
        "with open(os.path.join(save_dir, \"evaluation_concatnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"score\": score_concatnet_c,\n",
        "        \"parameters\": int(parameters_concatnet_c),\n",
        "        \"accuracy\": float(acc_concatnet_c),\n",
        "        \"consistency\": float(cons_concatnet_c),\n",
        "        \"f1_score\": float(f1_concatnet_c)\n",
        "    }, f)\n",
        "\n",
        "# Save loss weights (alpha, beta, gamma)\n",
        "with open(os.path.join(save_dir, \"loss_weights_concatnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"alpha\": float(K.get_value(alpha)),\n",
        "        \"beta\": float(K.get_value(beta)),\n",
        "        \"gamma\": float(K.get_value(gamma))\n",
        "    }, f)\n",
        "\n",
        "# Save meta information\n",
        "with open(os.path.join(save_dir, \"meta_concatnet_base_c.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"model_name\": \"concatnet_base_c\",\n",
        "        \"bt_strategy\": bt_strategy,\n",
        "        \"input_shape\": input_shape,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"num_c_1\": num_c_1,\n",
        "        \"num_c_2\": num_c_2,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"optimizer\": \"SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\",\n",
        "        \"loss\": [\"categorical_crossentropy\"] * 3,\n",
        "        \"metrics\": [\"accuracy\"] * 3,\n",
        "        \"pretrained_weights_source\": WEIGHTS_PATH\n",
        "    }, f)\n",
        "\n",
        "# Zip everything into one file\n",
        "zip_path = \"concatnet_base_c.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, _, files_list in os.walk(save_dir):\n",
        "        for file in files_list:\n",
        "            full_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(full_path, os.path.dirname(save_dir))\n",
        "            zipf.write(full_path, arcname)\n",
        "\n",
        "# Download the zip file to your computer\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Define the directory where the zip file will be extracted\n",
        "extract_dir = \".\\CIFAR10_models\\saved_concatnet_c\\concatnet_base_c\"\n",
        "\n",
        "# Load the model architecture and weights\n",
        "concat_net_c_model = load_model(os.path.join(extract_dir, \"concatnet_base_c.keras\"))\n",
        "concat_net_c_model.load_weights(os.path.join(extract_dir, \"concatnet_base_c.weights.h5\"))\n",
        "\n",
        "# Load training history\n",
        "with open(os.path.join(extract_dir, \"history_concatnet_base_c.pkl\"), \"rb\") as f:\n",
        "    history_concatnet_c = pickle.load(f)\n",
        "\n",
        "# Load predictions\n",
        "with open(os.path.join(extract_dir, \"predictions_concatnet_base_c.pkl\"), \"rb\") as f:\n",
        "    concat_net_c_predictions = pickle.load(f)\n",
        "\n",
        "# Load evaluation metrics\n",
        "with open(os.path.join(extract_dir, \"evaluation_concatnet_base_c.pkl\"), \"rb\") as f:\n",
        "    evaluation_concatnet_c = pickle.load(f)\n",
        "    score_concatnet_c = evaluation_concatnet_c[\"score\"]\n",
        "    parameters_concatnet_c = evaluation_concatnet_c[\"parameters\"]\n",
        "    acc_concatnet_c = evaluation_concatnet_c[\"accuracy\"]\n",
        "    consistency_concatnet_c = evaluation_concatnet_c[\"consistency\"]\n",
        "    f1_concatnet_c = evaluation_concatnet_c[\"f1_score\"]\n",
        "\n",
        "# Load loss weights\n",
        "with open(os.path.join(extract_dir, \"loss_weights_concatnet_base_c.pkl\"), \"rb\") as f:\n",
        "    loss_weights_concatnet_c = pickle.load(f)\n",
        "    concat_net_c_alpha = K.variable(loss_weights_concatnet_c[\"alpha\"])\n",
        "    concat_net_c_beta = K.variable(loss_weights_concatnet_c[\"beta\"])\n",
        "    concat_net_c_gamma = K.variable(loss_weights_concatnet_c[\"gamma\"])\n",
        "\n",
        "# Load meta information\n",
        "with open(os.path.join(extract_dir, \"meta_concatnet_base_c.pkl\"), \"rb\") as f:\n",
        "    meta_concatnet_c = pickle.load(f)\n",
        "    concat_net_c_model_name = meta_concatnet_c[\"model_name\"]\n",
        "    concat_net_c_bt_strategy = meta_concatnet_c[\"bt_strategy\"]\n",
        "    concat_net_c_input_shape = meta_concatnet_c[\"input_shape\"]\n",
        "    concat_net_c_num_classes = meta_concatnet_c[\"num_classes\"]\n",
        "    concat_net_c_num_c_1 = meta_concatnet_c[\"num_c_1\"]\n",
        "    concat_net_c_num_c_2 = meta_concatnet_c[\"num_c_2\"]\n",
        "    concat_net_c_batch_size = meta_concatnet_c[\"batch_size\"]\n",
        "    concat_net_c_epochs = meta_concatnet_c[\"epochs\"]\n",
        "    concat_net_c_optimizer = meta_concatnet_c[\"optimizer\"]\n",
        "    concat_net_c_loss = meta_concatnet_c[\"loss\"]\n",
        "    concat_net_c_metrics = meta_concatnet_c[\"metrics\"]\n",
        "    concat_net_c_pretrained_weights_source = meta_concatnet_c[\"pretrained_weights_source\"]\n",
        "\n",
        "# Now all the saved variables and objects are loaded and available for use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOp_WriEXRdI"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-g9xmRuBXTx0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ef91b_row1_col2, #T_ef91b_row2_col0, #T_ef91b_row2_col1, #T_ef91b_row2_col3, #T_ef91b_row2_col4, #T_ef91b_row2_col5, #T_ef91b_row2_col6 {\n",
              "  background-color: yellow;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ef91b\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ef91b_level0_col0\" class=\"col_heading level0 col0\" >Coarse 1</th>\n",
              "      <th id=\"T_ef91b_level0_col1\" class=\"col_heading level0 col1\" >Coarse 2</th>\n",
              "      <th id=\"T_ef91b_level0_col2\" class=\"col_heading level0 col2\" >Fine</th>\n",
              "      <th id=\"T_ef91b_level0_col3\" class=\"col_heading level0 col3\" >h_Accuracy</th>\n",
              "      <th id=\"T_ef91b_level0_col4\" class=\"col_heading level0 col4\" >h_Consistency</th>\n",
              "      <th id=\"T_ef91b_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
              "      <th id=\"T_ef91b_level0_col6\" class=\"col_heading level0 col6\" >Parameters</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" ></th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ef91b_level0_row0\" class=\"row_heading level0 row0\" >Flat CNN Base B</th>\n",
              "      <td id=\"T_ef91b_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
              "      <td id=\"T_ef91b_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
              "      <td id=\"T_ef91b_row0_col2\" class=\"data row0 col2\" >0.831000</td>\n",
              "      <td id=\"T_ef91b_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
              "      <td id=\"T_ef91b_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
              "      <td id=\"T_ef91b_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
              "      <td id=\"T_ef91b_row0_col6\" class=\"data row0 col6\" >7.85MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ef91b_level0_row1\" class=\"row_heading level0 row1\" >B-CNN Base B</th>\n",
              "      <td id=\"T_ef91b_row1_col0\" class=\"data row1 col0\" >0.959000</td>\n",
              "      <td id=\"T_ef91b_row1_col1\" class=\"data row1 col1\" >0.869800</td>\n",
              "      <td id=\"T_ef91b_row1_col2\" class=\"data row1 col2\" >0.842500</td>\n",
              "      <td id=\"T_ef91b_row1_col3\" class=\"data row1 col3\" >0.788900</td>\n",
              "      <td id=\"T_ef91b_row1_col4\" class=\"data row1 col4\" >0.902800</td>\n",
              "      <td id=\"T_ef91b_row1_col5\" class=\"data row1 col5\" >0.890433</td>\n",
              "      <td id=\"T_ef91b_row1_col6\" class=\"data row1 col6\" >12.38MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ef91b_level0_row2\" class=\"row_heading level0 row2\" >BA-CNN Base B</th>\n",
              "      <td id=\"T_ef91b_row2_col0\" class=\"data row2 col0\" >0.967300</td>\n",
              "      <td id=\"T_ef91b_row2_col1\" class=\"data row2 col1\" >0.877300</td>\n",
              "      <td id=\"T_ef91b_row2_col2\" class=\"data row2 col2\" >0.834800</td>\n",
              "      <td id=\"T_ef91b_row2_col3\" class=\"data row2 col3\" >0.812100</td>\n",
              "      <td id=\"T_ef91b_row2_col4\" class=\"data row2 col4\" >0.954200</td>\n",
              "      <td id=\"T_ef91b_row2_col5\" class=\"data row2 col5\" >0.893133</td>\n",
              "      <td id=\"T_ef91b_row2_col6\" class=\"data row2 col6\" >8.72MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ef91b_level0_row3\" class=\"row_heading level0 row3\" >H-CNN Base B</th>\n",
              "      <td id=\"T_ef91b_row3_col0\" class=\"data row3 col0\" >0.959800</td>\n",
              "      <td id=\"T_ef91b_row3_col1\" class=\"data row3 col1\" >0.870800</td>\n",
              "      <td id=\"T_ef91b_row3_col2\" class=\"data row3 col2\" >0.842000</td>\n",
              "      <td id=\"T_ef91b_row3_col3\" class=\"data row3 col3\" >0.791700</td>\n",
              "      <td id=\"T_ef91b_row3_col4\" class=\"data row3 col4\" >0.908500</td>\n",
              "      <td id=\"T_ef91b_row3_col5\" class=\"data row3 col5\" >0.890867</td>\n",
              "      <td id=\"T_ef91b_row3_col6\" class=\"data row3 col6\" >29.16MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ef91b_level0_row4\" class=\"row_heading level0 row4\" >Add-net Base B</th>\n",
              "      <td id=\"T_ef91b_row4_col0\" class=\"data row4 col0\" >0.958100</td>\n",
              "      <td id=\"T_ef91b_row4_col1\" class=\"data row4 col1\" >0.864600</td>\n",
              "      <td id=\"T_ef91b_row4_col2\" class=\"data row4 col2\" >0.833200</td>\n",
              "      <td id=\"T_ef91b_row4_col3\" class=\"data row4 col3\" >0.794200</td>\n",
              "      <td id=\"T_ef91b_row4_col4\" class=\"data row4 col4\" >0.928700</td>\n",
              "      <td id=\"T_ef91b_row4_col5\" class=\"data row4 col5\" >0.885300</td>\n",
              "      <td id=\"T_ef91b_row4_col6\" class=\"data row4 col6\" >8.57MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ef91b_level0_row5\" class=\"row_heading level0 row5\" >Concat-net Base B</th>\n",
              "      <td id=\"T_ef91b_row5_col0\" class=\"data row5 col0\" >0.956600</td>\n",
              "      <td id=\"T_ef91b_row5_col1\" class=\"data row5 col1\" >0.867300</td>\n",
              "      <td id=\"T_ef91b_row5_col2\" class=\"data row5 col2\" >0.836400</td>\n",
              "      <td id=\"T_ef91b_row5_col3\" class=\"data row5 col3\" >0.792700</td>\n",
              "      <td id=\"T_ef91b_row5_col4\" class=\"data row5 col4\" >0.922700</td>\n",
              "      <td id=\"T_ef91b_row5_col5\" class=\"data row5 col5\" >0.886767</td>\n",
              "      <td id=\"T_ef91b_row5_col6\" class=\"data row5 col6\" >12.39MM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x205ec54a350>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary = {'':['Flat CNN Base B','B-CNN Base B','BA-CNN Base B','H-CNN Base B','Add-net Base B','Concat-net Base B'],'Coarse 1': [0,score_b_cnn_b[4],score_ba_cnn_b[4],score_h_cnn_b[4],score_addnet_b[4],score_concatnet_b[4]],'Coarse 2': [0,score_b_cnn_b[5],score_ba_cnn_b[5],score_h_cnn_b[5],score_addnet_b[5],score_concatnet_b[5]],'Fine': [score_base_b[1],score_b_cnn_b[6],score_ba_cnn_b[6],score_h_cnn_b[6],score_addnet_b[6],score_concatnet_b[6]],'h_Accuracy':[0,acc_b_cnn_b,acc_ba_cnn_b,acc_h_cnn_b,acc_addnet_b,acc_concatnet_b],'h_Consistency':[0,cons_b_cnn_b,cons_ba_cnn_b,cons_h_cnn_b,cons_addnet_b,cons_concatnet_b],'F1':[0,f1_b_cnn_b,f1_ba_cnn_b,f1_h_cnn_b,f1_addnet_b,f1_concatnet_b],'Parameters': [parameters_base_b ,parameters_b_cnn_b,parameters_ba_cnn_b,parameters_h_cnn_b,parameters_addnet_b,parameters_concatnet_b]}\n",
        "summary = pd.DataFrame(summary)\n",
        "summary['Parameters'] = (summary['Parameters'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
        "summary = summary.set_index('')\n",
        "summary.style.highlight_max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iKqpTST7YtjT"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_3407a_row1_col2, #T_3407a_row2_col0, #T_3407a_row2_col1, #T_3407a_row2_col3, #T_3407a_row2_col4, #T_3407a_row2_col5, #T_3407a_row5_col6 {\n",
              "  background-color: yellow;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_3407a\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_3407a_level0_col0\" class=\"col_heading level0 col0\" >Coarse 1</th>\n",
              "      <th id=\"T_3407a_level0_col1\" class=\"col_heading level0 col1\" >Coarse 2</th>\n",
              "      <th id=\"T_3407a_level0_col2\" class=\"col_heading level0 col2\" >Fine</th>\n",
              "      <th id=\"T_3407a_level0_col3\" class=\"col_heading level0 col3\" >h_Accuracy</th>\n",
              "      <th id=\"T_3407a_level0_col4\" class=\"col_heading level0 col4\" >Consistency</th>\n",
              "      <th id=\"T_3407a_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
              "      <th id=\"T_3407a_level0_col6\" class=\"col_heading level0 col6\" >Parameters</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" ></th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_3407a_level0_row0\" class=\"row_heading level0 row0\" >Flat CNN Base C</th>\n",
              "      <td id=\"T_3407a_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
              "      <td id=\"T_3407a_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
              "      <td id=\"T_3407a_row0_col2\" class=\"data row0 col2\" >0.883200</td>\n",
              "      <td id=\"T_3407a_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
              "      <td id=\"T_3407a_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
              "      <td id=\"T_3407a_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
              "      <td id=\"T_3407a_row0_col6\" class=\"data row0 col6\" >39.95MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3407a_level0_row1\" class=\"row_heading level0 row1\" >B-CNN Base C</th>\n",
              "      <td id=\"T_3407a_row1_col0\" class=\"data row1 col0\" >0.960700</td>\n",
              "      <td id=\"T_3407a_row1_col1\" class=\"data row1 col1\" >0.905000</td>\n",
              "      <td id=\"T_3407a_row1_col2\" class=\"data row1 col2\" >0.890900</td>\n",
              "      <td id=\"T_3407a_row1_col3\" class=\"data row1 col3\" >0.828100</td>\n",
              "      <td id=\"T_3407a_row1_col4\" class=\"data row1 col4\" >0.899600</td>\n",
              "      <td id=\"T_3407a_row1_col5\" class=\"data row1 col5\" >0.918867</td>\n",
              "      <td id=\"T_3407a_row1_col6\" class=\"data row1 col6\" >49.67MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3407a_level0_row2\" class=\"row_heading level0 row2\" >BA-CNN Base C</th>\n",
              "      <td id=\"T_3407a_row2_col0\" class=\"data row2 col0\" >0.983300</td>\n",
              "      <td id=\"T_3407a_row2_col1\" class=\"data row2 col1\" >0.923400</td>\n",
              "      <td id=\"T_3407a_row2_col2\" class=\"data row2 col2\" >0.888700</td>\n",
              "      <td id=\"T_3407a_row2_col3\" class=\"data row2 col3\" >0.881400</td>\n",
              "      <td id=\"T_3407a_row2_col4\" class=\"data row2 col4\" >0.984700</td>\n",
              "      <td id=\"T_3407a_row2_col5\" class=\"data row2 col5\" >0.931800</td>\n",
              "      <td id=\"T_3407a_row2_col6\" class=\"data row2 col6\" >15.81MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3407a_level0_row3\" class=\"row_heading level0 row3\" >H-CNN Base C</th>\n",
              "      <td id=\"T_3407a_row3_col0\" class=\"data row3 col0\" >0.958700</td>\n",
              "      <td id=\"T_3407a_row3_col1\" class=\"data row3 col1\" >0.904900</td>\n",
              "      <td id=\"T_3407a_row3_col2\" class=\"data row3 col2\" >0.883300</td>\n",
              "      <td id=\"T_3407a_row3_col3\" class=\"data row3 col3\" >0.826300</td>\n",
              "      <td id=\"T_3407a_row3_col4\" class=\"data row3 col4\" >0.905300</td>\n",
              "      <td id=\"T_3407a_row3_col5\" class=\"data row3 col5\" >0.915633</td>\n",
              "      <td id=\"T_3407a_row3_col6\" class=\"data row3 col6\" >108.39MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3407a_level0_row4\" class=\"row_heading level0 row4\" >Add-net Base B</th>\n",
              "      <td id=\"T_3407a_row4_col0\" class=\"data row4 col0\" >0.959600</td>\n",
              "      <td id=\"T_3407a_row4_col1\" class=\"data row4 col1\" >0.898700</td>\n",
              "      <td id=\"T_3407a_row4_col2\" class=\"data row4 col2\" >0.884200</td>\n",
              "      <td id=\"T_3407a_row4_col3\" class=\"data row4 col3\" >0.833400</td>\n",
              "      <td id=\"T_3407a_row4_col4\" class=\"data row4 col4\" >0.918400</td>\n",
              "      <td id=\"T_3407a_row4_col5\" class=\"data row4 col5\" >0.914167</td>\n",
              "      <td id=\"T_3407a_row4_col6\" class=\"data row4 col6\" >18.6MM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3407a_level0_row5\" class=\"row_heading level0 row5\" >Concat-net Base B</th>\n",
              "      <td id=\"T_3407a_row5_col0\" class=\"data row5 col0\" >0.957000</td>\n",
              "      <td id=\"T_3407a_row5_col1\" class=\"data row5 col1\" >0.897800</td>\n",
              "      <td id=\"T_3407a_row5_col2\" class=\"data row5 col2\" >0.881200</td>\n",
              "      <td id=\"T_3407a_row5_col3\" class=\"data row5 col3\" >0.830700</td>\n",
              "      <td id=\"T_3407a_row5_col4\" class=\"data row5 col4\" >0.922900</td>\n",
              "      <td id=\"T_3407a_row5_col5\" class=\"data row5 col5\" >0.912000</td>\n",
              "      <td id=\"T_3407a_row5_col6\" class=\"data row5 col6\" >49.69MM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x20588314910>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary = {'':['Flat CNN Base C','B-CNN Base C','BA-CNN Base C','H-CNN Base C','Add-net Base B','Concat-net Base B'],'Coarse 1': [0,score_b_cnn_c[4],score_ba_cnn_c[4],score_h_cnn_c[4],score_addnet_c[4],score_concatnet_c[4]],'Coarse 2': [0,score_b_cnn_c[5],score_ba_cnn_c[5],score_h_cnn_c[5],score_addnet_c[5],score_concatnet_c[5]],'Fine': [flat_cnn_score_base_c[1],score_b_cnn_c[6],score_ba_cnn_c[6],score_h_cnn_c[6],score_addnet_c[6],score_concatnet_c[6]],'h_Accuracy':[0,acc_b_cnn_c,acc_ba_cnn_c,acc_h_cnn_c,acc_addnet_c,acc_concatnet_c],'Consistency':[0,cons_b_cnn_c,cons_ba_cnn_c,cons_h_cnn_c,consistency_addnet_c,consistency_concatnet_c],'F1':[0,f1_b_cnn_c,f1_ba_cnn_c,f1_h_cnn_c,f1_addnet_c,f1_concatnet_c],'Parameters': [flat_cnn_parameters_base_c ,parameters_b_cnn_c,parameters_ba_cnn_c,parameters_h_cnn_c,parameters_addnet_c,parameters_concatnet_c]}\n",
        "summary = pd.DataFrame(summary)\n",
        "summary['Parameters'] = (summary['Parameters'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
        "summary = summary.set_index('')\n",
        "summary.style.highlight_max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "14rZlVpoq-bi",
        "FcqC1GZ6kLlH",
        "wnW09JGQpo_J",
        "QmTNGOqEpu4g",
        "Y7qKvpSMp2iE",
        "ZviMkG1Nq-bo",
        "hcxhdYepq-bq",
        "SOp_WriEXRdI"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (venv_BA_CNN_IC)",
      "language": "python",
      "name": "venv_ba_cnn_ic"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
