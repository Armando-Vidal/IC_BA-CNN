{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvPb0YJNU1RM"
   },
   "source": [
    "\n",
    "<H3 align='center'> An Attention-Based Architecture for\n",
    "Hierarchical Classification with CNNs </H3>\n",
    "\n",
    "<H5 align='center'> CIFAR-10 </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CFPEBlEDW1R"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vb27tNtvUuKD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 19:38:18.451384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 19:38:18.579918: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-23 19:38:18.616863: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-23 19:38:19.353654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-23 19:38:19.353739: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-23 19:38:19.353745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Concatenate, Add, Softmax\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "from keras.initializers import he_normal\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, CSVLogger\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from scipy import stats\n",
    "\n",
    "# Correct relations for CIFAR-10\n",
    "relations = [[0,2,9],[0,3,17],[0,4,10],[0,4,18],[1,5,11],[1,6,15],[1,7,12],[1,7,14],[1,8,13],[1,8,16]]\n",
    "\n",
    "# Computes hierarchical metrics\n",
    "def hierarchical_metrics(true,pred):\n",
    "  true_labels = []\n",
    "  true_fine = true[2].argmax(axis=1)+9\n",
    "  true_c2 = true[1].argmax(axis=1)+2\n",
    "  true_c1 = true[0].argmax(axis=1)\n",
    "  for i in range(len(true_fine)):\n",
    "    true_labels.append([true_c1[i],true_c2[i],true_fine[i]])\n",
    "  pred_labels = []\n",
    "  pred_c1 = pred[0].argmax(axis = 1)\n",
    "  pred_c2 = pred[1].argmax(axis = 1)+2\n",
    "  pred_fine = pred[2].argmax(axis = 1)+9 \n",
    "  for i in range(len(pred_c1)):\n",
    "    pred_labels.append([pred_c1[i],pred_c2[i],pred_fine[i]])\n",
    "  preci = precision(true_labels,pred_labels)\n",
    "  reca = recall(true_labels,pred_labels)\n",
    "  f_1 = f1(true_labels,pred_labels)\n",
    "    \n",
    "  consistent_examples = 1\n",
    "  correct_pred = 0\n",
    "  test_set_size = len(true_labels)\n",
    "  for i in range(test_set_size):\n",
    "    if [pred_c1[i],pred_c2[i],pred_fine[i]] in relations:\n",
    "        consistent_examples = consistent_examples + 1\n",
    "    if [pred_c1[i],pred_c2[i],pred_fine[i]] == true_labels[i]:\n",
    "        correct_pred = correct_pred +1\n",
    "  h_accuracy = correct_pred/test_set_size\n",
    "  h_consistency = (consistent_examples-1)/test_set_size\n",
    "\n",
    "  return h_accuracy,h_consistency,f_1\n",
    "\n",
    "# Hierarchical metrics, proposed by Kiritchenko et al (2005)\n",
    "# Implementation \n",
    "# https://gitlab.com/dacs-hpi/hiclass/-/blob/main/hiclass/metrics.py\n",
    "\n",
    "\n",
    "def precision(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute precision score for hierarchical classification.\n",
    "\n",
    "    hP = sum(|S intersection T|) / sum(|S|),\n",
    "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
    "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        What proportion of positive identifications was actually correct?\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    sum_intersection = 0\n",
    "    sum_prediction_and_ancestors = 0\n",
    "    for ground_truth, prediction in zip(y_true, y_pred):\n",
    "        sum_intersection = sum_intersection + len(\n",
    "            set(ground_truth).intersection(set(prediction))\n",
    "        )\n",
    "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
    "            set(prediction)\n",
    "        )\n",
    "    precision = sum_intersection / sum_prediction_and_ancestors\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute recall score for hierarchical classification.\n",
    "\n",
    "    hR = sum(|S intersection T|) / sum(|T|),\n",
    "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
    "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        What proportion of actual positives was identified correctly?\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    sum_intersection = 0\n",
    "    sum_prediction_and_ancestors = 0\n",
    "    for ground_truth, prediction in zip(y_true, y_pred):\n",
    "        sum_intersection = sum_intersection + len(\n",
    "            set(ground_truth).intersection(set(prediction))\n",
    "        )\n",
    "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
    "            set(ground_truth)\n",
    "        )\n",
    "    recall = sum_intersection / sum_prediction_and_ancestors\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute f1 score for hierarchical classification.\n",
    "\n",
    "    hF = 2 * hP * hR / (hP + hR),\n",
    "    where hP is the hierarchical precision and hR is the hierarchical recall.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Weighted average of the precision and recall\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFr3aR_HvlFA"
   },
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V2EC6Gn0vnZs"
   },
   "outputs": [],
   "source": [
    "#-------- dimensions ---------\n",
    "img_rows, img_cols = 32, 32\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "#-----------------------------\n",
    "\n",
    "train_size = 50000\n",
    "\n",
    "#--- coarse 1 classes ---\n",
    "num_c_1 = 2\n",
    "#--- coarse 2 classes ---\n",
    "num_c_2 = 7\n",
    "#--- fine classes ---\n",
    "num_classes  = 10\n",
    "\n",
    "batch_size   = 128\n",
    "epochs       = 60  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mhgnNBz7P5h9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#-------------------- data loading ----------------------\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_cm = y_test\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#---------------- data preprocessing -------------------\n",
    "x_train = (x_train-np.mean(x_train)) / np.std(x_train)\n",
    "x_test = (x_test-np.mean(x_test)) / np.std(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FvrF29rSwGMB"
   },
   "outputs": [],
   "source": [
    "#---------------------- make coarse 2 labels --------------------------\n",
    "parent_f = {\n",
    "  2:3, 3:5, 5:5,\n",
    "  1:2, 7:6, 4:6,\n",
    "  0:0, 6:4, 8:1, 9:2\n",
    "}\n",
    "\n",
    "y_c2_train = np.zeros((y_train.shape[0], num_c_2)).astype(\"float32\")\n",
    "y_c2_test = np.zeros((y_test.shape[0], num_c_2)).astype(\"float32\")\n",
    "for i in range(y_c2_train.shape[0]):\n",
    "  y_c2_train[i][parent_f[np.argmax(y_train[i])]] = 1.0\n",
    "for i in range(y_c2_test.shape[0]):\n",
    "  y_c2_test[i][parent_f[np.argmax(y_test[i])]] = 1.0\n",
    "\n",
    "#---------------------- make coarse 1 labels --------------------------\n",
    "parent_c2 = {\n",
    "  0:0, 1:0, 2:0,\n",
    "  3:1, 4:1, 5:1, 6:1\n",
    "}\n",
    "y_c1_train = np.zeros((y_c2_train.shape[0], num_c_1)).astype(\"float32\")\n",
    "y_c1_test = np.zeros((y_c2_test.shape[0], num_c_1)).astype(\"float32\")\n",
    "for i in range(y_c1_train.shape[0]):\n",
    "  y_c1_train[i][parent_c2[np.argmax(y_c2_train[i])]] = 1.0\n",
    "for i in range(y_c1_test.shape[0]):\n",
    "  y_c1_test[i][parent_c2[np.argmax(y_c2_test[i])]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UCJ4bhapPP-Y"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "def scheduler(epoch):\n",
    "  learning_rate_init = 0.003\n",
    "  if epoch > 42:\n",
    "    learning_rate_init = 0.0005\n",
    "  if epoch > 52:\n",
    "    learning_rate_init = 0.0001\n",
    "  return learning_rate_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bmCQrJtYwa87"
   },
   "outputs": [],
   "source": [
    "# Loss Weights modifier, when BT-strategy is used\n",
    "class LossWeightsModifier(keras.callbacks.Callback):\n",
    "  def __init__(self, alpha, beta, gamma):\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if epoch == 10:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.8)\n",
    "      K.set_value(self.gamma, 0.1)\n",
    "    if epoch == 20:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.2)\n",
    "      K.set_value(self.gamma, 0.7)\n",
    "    if epoch == 30:\n",
    "      K.set_value(self.alpha, 0)\n",
    "      K.set_value(self.beta, 0)\n",
    "      K.set_value(self.gamma, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_n23WmqWm0r"
   },
   "source": [
    "# Flat CNN Base B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BrSqH3ZSQT5L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 19:39:21.215563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:21.253729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:21.255315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:21.257332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 19:39:21.257807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:21.259329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:21.260824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:22.954320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:22.956165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:22.957383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:39:22.958563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13643 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, fine_pred, name='flat_cnn_base_b')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHyicCv-XEmK",
    "outputId": "9f6430bb-af95-4258-f59e-21171245c83a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 19:39:46.449323: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "2023-02-23 19:39:47.204755: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-23 19:39:47.205253: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-23 19:39:47.205281: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-02-23 19:39:47.205773: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-02-23 19:39:47.205827: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 41ms/step - loss: 1.9125 - accuracy: 0.3828 - val_loss: 1.6895 - val_accuracy: 0.3893 - lr: 0.0030\n",
      "--- Flat CNN Base B ---\n",
      "Accuracy: 0.38929998874664307\n",
      "Parameters: 7,851,338\n"
     ]
    }
   ],
   "source": [
    "change_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Training\n",
    "history_base_b = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=change_lr,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation on test set\n",
    "score_base_b = model.evaluate(x_test, y_test, verbose=0)\n",
    "parameters_base_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "# Results\n",
    "print(\"--- Flat CNN Base B ---\")\n",
    "print(\"Accuracy:\",score_base_b[1])\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_base_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4isnsDMcKKm"
   },
   "source": [
    "# B-CNN Base B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sMGtDO4tdxoN"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UnKGZz_zOss",
    "outputId": "a68495c5-b1df-41cf-e9c0-135e3194f22a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bcnn_base_b\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv2D)          (None, 32, 32, 64)   1792        ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 64)  256         ['block1_conv1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv2D)          (None, 32, 32, 64)   36928       ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 64)  256         ['block1_conv2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling2D)     (None, 16, 16, 64)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv2D)          (None, 16, 16, 128)  73856       ['block1_pool[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 128)  512        ['block2_conv1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv2D)          (None, 16, 16, 128)  147584      ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 128)  512        ['block2_conv2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling2D)     (None, 8, 8, 128)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv2D)          (None, 8, 8, 256)    295168      ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 256)   1024        ['block3_conv1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv2D)          (None, 8, 8, 256)    590080      ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 256)   1024        ['block3_conv2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling2D)     (None, 4, 4, 256)    0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv2D)          (None, 4, 4, 512)    1180160     ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 4, 4, 512)   2048        ['block4_conv1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv2D)          (None, 4, 4, 512)    2359808     ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 4, 4, 512)   2048        ['block4_conv2[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling2D)     (None, 2, 2, 512)    0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " c1_flatten (Flatten)           (None, 8192)         0           ['block2_pool[0][0]']            \n",
      "                                                                                                  \n",
      " c2_flatten (Flatten)           (None, 4096)         0           ['block3_pool[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['block4_pool[0][0]']            \n",
      "                                                                                                  \n",
      " c1_fc_cifar10_1 (Dense)        (None, 256)          2097408     ['c1_flatten[0][0]']             \n",
      "                                                                                                  \n",
      " c2_fc_cifar10_1 (Dense)        (None, 512)          2097664     ['c2_flatten[0][0]']             \n",
      "                                                                                                  \n",
      " fc_cifar10_1 (Dense)           (None, 1024)         2098176     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 256)         1024        ['c1_fc_cifar10_1[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 512)         2048        ['c2_fc_cifar10_1[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 1024)        4096        ['fc_cifar10_1[0][0]']           \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 512)          0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 1024)         0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " c1_fc2 (Dense)                 (None, 256)          65792       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " c2_fc2 (Dense)                 (None, 512)          262656      ['dropout_4[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 1024)         1049600     ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 256)         1024        ['c1_fc2[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 512)         2048        ['c2_fc2[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 1024)        4096        ['fc2[0][0]']                    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 512)          0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 1024)         0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " c1_predictions_cifar10 (Dense)  (None, 2)           514         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " c2_predictions_cifar10 (Dense)  (None, 7)           3591        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " predictions_cifar10 (Dense)    (None, 10)           10250       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,393,043\n",
      "Trainable params: 12,382,035\n",
      "Non-trainable params: 11,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bcnn_base_b')\n",
    "\n",
    "#----------------------- compile  ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5J93jPue6Kr",
    "outputId": "29e47a08-fb30-4d7a-f717-aec526defd8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/391 [..............................] - ETA: 15s - loss: 0.3364 - c1_predictions_cifar10_loss: 0.2863 - c2_predictions_cifar10_loss: 2.4660 - predictions_cifar10_loss: 3.1116 - c1_predictions_cifar10_accuracy: 0.8641 - c2_predictions_cifar10_accuracy: 0.2875 - predictions_cifar10_accuracy: 0.1906WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0182s vs `on_train_batch_end` time: 0.0227s). Check your callbacks.\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.2882 - c1_predictions_cifar10_loss: 0.2442 - c2_predictions_cifar10_loss: 2.1911 - predictions_cifar10_loss: 2.6958 - c1_predictions_cifar10_accuracy: 0.9013 - c2_predictions_cifar10_accuracy: 0.3396 - predictions_cifar10_accuracy: 0.2550 - val_loss: 0.2325 - val_c1_predictions_cifar10_loss: 0.2060 - val_c2_predictions_cifar10_loss: 1.3146 - val_predictions_cifar10_loss: 1.7416 - val_c1_predictions_cifar10_accuracy: 0.9151 - val_c2_predictions_cifar10_accuracy: 0.5045 - val_predictions_cifar10_accuracy: 0.3760 - lr: 0.0030\n",
      "313/313 [==============================] - 2s 4ms/step\n",
      "--- B-CNN Base B ---\n",
      "--- Accuracy per level ---\n",
      "Accuracy level 1: 0.9150999784469604\n",
      "Accuracy level 2: 0.5044999718666077\n",
      "Accuracy level 3: 0.37599998712539673\n",
      "--- Hierarchical Metrics ---\n",
      "Accuracy: 0.2557\n",
      "Consistency: 0.5194\n",
      "f1: 0.5985333333333334\n",
      "Parameters: 12,382,035\n"
     ]
    }
   ],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_b_cnn_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_b_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_b_cnn_b,cons_b_cnn_b,f1_b_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- B-CNN Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_b_cnn_b[4])\n",
    "print(\"Accuracy level 2:\",score_b_cnn_b[5])\n",
    "print(\"Accuracy level 3:\",score_b_cnn_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_b_cnn_b)\n",
    "print(\"Consistency:\",cons_b_cnn_b)\n",
    "print(\"f1:\",f1_b_cnn_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_b_cnn_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD1DmaHZf2JK"
   },
   "source": [
    "# BA-CNN Base B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHyRTBmtf8A7"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True\n",
    "\n",
    "# neurons of all dense layers on each branch \n",
    "branch_neurons = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEBBfHP3gJkX",
    "outputId": "c40c6e70-9af6-409e-9f5e-1376ec4e6dfc"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch_out = Dropout(0.5)(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch_out = Dropout(0.5)(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x_out = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "#-- Att for coarse 1---\n",
    "# Coarse 1\n",
    "sfcn_1_1 = Dense(64, name='fc1_1')(c_1_bch_out)\n",
    "sfcn_1_1 = Dense(1, name='fc1_2')(sfcn_1_1)\n",
    "# Coarse 2\n",
    "sfcn_1_2 = Dense(64, name='fc1_3')(c_2_bch_out)\n",
    "sfcn_1_2 = Dense(1, name='fc1_4')(sfcn_1_2)\n",
    "# Fine\n",
    "sfcn_1_3 = Dense(64, name='fc1_5')(x_out)\n",
    "sfcn_1_3 = Dense(1, name='fc1_6')(sfcn_1_3)\n",
    "\n",
    "score_vector_1 = Concatenate()([sfcn_1_1,sfcn_1_2,sfcn_1_3]) # Score vector 1\n",
    "att_weights_1 = Activation('softmax', name='attention_weights_1')(score_vector_1) # Attention weights 1\n",
    "weightned_sum_1 = Add()([c_1_bch_out*att_weights_1[0][0],c_2_bch_out*att_weights_1[0][1],x_out*att_weights_1[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_1_concat = Concatenate()([c_1_bch_out,weightned_sum_1])\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(coarse_1_concat)\n",
    "\n",
    "\n",
    "#-- Att for coarse 2---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_2_1 = Dense(64, name='fc2_1')(c_1_bch_out)\n",
    "sfcn_2_1 = Dense(1, name='fc2_2')(sfcn_2_1)\n",
    "# Coarse 2\n",
    "sfcn_2_2 = Dense(64, name='fc2_3')(c_2_bch_out)\n",
    "sfcn_2_2 = Dense(1, name='fc2_4')(sfcn_2_2)\n",
    "# Fine\n",
    "sfcn_2_3 = Dense(64, name='fc2_5')(x_out)\n",
    "sfcn_2_3 = Dense(1, name='fc2_6')(sfcn_2_3)\n",
    "\n",
    "score_vector_2 = Concatenate()([sfcn_2_1,sfcn_2_2,sfcn_2_3]) # Score vector 1\n",
    "att_weights_2 = Activation('softmax', name='attention_weights_2')(score_vector_2) # Attention weights 1\n",
    "weightned_sum_2 = Add()([c_1_bch_out*att_weights_2[0][0],c_2_bch_out*att_weights_2[0][1],x_out*att_weights_2[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_2_concat = Concatenate()([c_2_bch_out,weightned_sum_2])\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(coarse_2_concat)\n",
    "\n",
    "\n",
    "#-- Att for fine---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_3_1 = Dense(64, name='fc3_1')(c_1_bch_out)\n",
    "sfcn_3_1 = Dense(1, name='fc3_2')(sfcn_3_1)\n",
    "# Coarse 2\n",
    "sfcn_3_2 = Dense(64, name='fc3_3')(c_2_bch_out)\n",
    "sfcn_3_2 = Dense(1, name='fc3_4')(sfcn_3_2)\n",
    "# Fine\n",
    "sfcn_3_3 = Dense(64, name='fc3_5')(x_out)\n",
    "sfcn_3_3 = Dense(1, name='fc3_6')(sfcn_3_3)\n",
    "\n",
    "score_vector_3 = Concatenate()([sfcn_3_1,sfcn_3_2,sfcn_3_3]) # Score vector 1\n",
    "att_weights_3 = Activation('softmax', name='attention_weights_3')(score_vector_3) # Attention weights 1\n",
    "weightned_sum_3 = Add()([c_1_bch_out*att_weights_3[0][0],c_2_bch_out*att_weights_3[0][1],x_out*att_weights_3[0][2]]) # Weightned sum 3\n",
    "\n",
    "# Concat and prediction\n",
    "fine_concat = Concatenate()([x_out,weightned_sum_3])\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(fine_concat)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bacnn_base_b')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmBDSugfkAcn",
    "outputId": "f4b90537-8888-46f6-ce6a-2a208abf8979"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bacnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_ba_cnn_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_ba_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_ba_cnn_b,cons_ba_cnn_b,f1_ba_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- BA-CNN Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_ba_cnn_b[4])\n",
    "print(\"Accuracy level 2:\",score_ba_cnn_b[5])\n",
    "print(\"Accuracy level 3:\",score_ba_cnn_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_ba_cnn_b)\n",
    "print(\"Consistency:\",cons_ba_cnn_b)\n",
    "print(\"f1:\",f1_ba_cnn_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_ba_cnn_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLRG5--_lmjc"
   },
   "source": [
    "# H-CNN Base B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8lNJ_DDnzWB"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYXL_rl1FMR2",
    "outputId": "76412fd5-6c3d-4ad0-b0d7-3f8920c3bc26"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch_flatt = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch_flatt)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch_flatt = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch_concat = Concatenate()([c_1_bch_flatt,c_2_bch_flatt]) # Conectivity Pattern\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar100_1')(c_2_bch_concat)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x_flatt = Flatten(name='flatten')(x)\n",
    "x = Concatenate()([c_2_bch_concat,x_flatt]) # Conectivity Pattern\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='hcnn_base_b')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0-YWS7moD46",
    "outputId": "abda058d-d8af-4deb-9e53-648837febb00"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_hcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_h_cnn_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_h_cnn_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_h_cnn_b,cons_h_cnn_b,f1_h_cnn_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- H-CNN Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_h_cnn_b[4])\n",
    "print(\"Accuracy level 2:\",score_h_cnn_b[5])\n",
    "print(\"Accuracy level 3:\",score_h_cnn_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_h_cnn_b)\n",
    "print(\"Consistency:\",cons_h_cnn_b)\n",
    "print(\"f1:\",f1_h_cnn_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_h_cnn_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-net Base B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out = Add()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu', name='fc2')(x)\n",
    "x = Add()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='add_net_Base_B')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_addnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_addnet_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_addnet_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_addnet_b,cons_addnet_b,f1_addnet_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Add-net Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_addnet_b[4])\n",
    "print(\"Accuracy level 2:\",score_addnet_b[5])\n",
    "print(\"Accuracy level 3:\",score_addnet_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_addnet_b)\n",
    "print(\"Consistency:\",cons_addnet_b)\n",
    "print(\"f1:\",f1_addnet_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_addnet_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat-net Base B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out = Concatenate()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = Concatenate()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='Concatnet_Base_B')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_concatnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_concatnet_b = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_concatnet_b = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_concatnet_b,cons_concatnet_b,f1_concatnet_b= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Concat-net Base B ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_concatnet_b[4])\n",
    "print(\"Accuracy level 2:\",score_concatnet_b[5])\n",
    "print(\"Accuracy level 3:\",score_concatnet_b[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_concatnet_b)\n",
    "print(\"Consistency:\",cons_concatnet_b)\n",
    "print(\"f1:\",f1_concatnet_b)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_concatnet_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcqC1GZ6kLlH"
   },
   "source": [
    "# Flat CNN Base C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cpunwm3o5UAx",
    "outputId": "399d69c8-fcda-466c-e851-1bd88f758028"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "#----------------------- model definition ---------------------------\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, fine_pred, name='flat_cnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KojpAgRHkgig"
   },
   "outputs": [],
   "source": [
    "change_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Training\n",
    "history_base_c = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=change_lr,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation on test set\n",
    "score_base_c = model.evaluate(x_test, y_test, verbose=0)\n",
    "parameters_base_c = np.sum([K.count_params(w) for w in model.trainable_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnW09JGQpo_J"
   },
   "source": [
    "# B-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_8ahOlc50aKP"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w9rtWZ07IK77",
    "outputId": "bdc5941b-d61a-42c9-d317-9790d378f9f0"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bcnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LzCzPlRV0uhW",
    "outputId": "2486e230-0298-4f48-b3e5-f14ddc2407d9"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_b_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_b_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_b_cnn_c,cons_b_cnn_c,f1_b_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- B-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_b_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_b_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_b_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_b_cnn_c)\n",
    "print(\"Consistency:\",cons_b_cnn_c)\n",
    "print(\"f1:\",f1_b_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_b_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmTNGOqEpu4g"
   },
   "source": [
    "# BA-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7VZWx0yfWQZk"
   },
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True\n",
    "\n",
    "# neurons of all dense layers on each branch \n",
    "branch_neurons = 32 # Parsimonious version is 256\n",
    "\n",
    "# neurons of all attention mechanism\n",
    "att_neurons = 2048  # Parsimonious version is 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5FPODG2fcuuK",
    "outputId": "10bfad21-c3da-4f9c-ead2-0c2d7905853f"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch_out = Dropout(0.5)(c_1_bch)\n",
    "#c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch_out)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch_out = Dropout(0.5)(c_2_bch)\n",
    "#c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch_out)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc_cifar10_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x_out = Dropout(0.5)(x)\n",
    "\n",
    "#-- Att for coarse 1---\n",
    "# Coarse 1\n",
    "sfcn_1_1 = Dense(att_neurons, name='fc1_1')(c_1_bch_out)\n",
    "sfcn_1_1 = Dense(1, name='fc1_2')(sfcn_1_1)\n",
    "# Coarse 2\n",
    "sfcn_1_2 = Dense(att_neurons, name='fc1_3')(c_2_bch_out)\n",
    "sfcn_1_2 = Dense(1, name='fc1_4')(sfcn_1_2)\n",
    "# Fine\n",
    "sfcn_1_3 = Dense(att_neurons, name='fc1_5')(x_out)\n",
    "sfcn_1_3 = Dense(1, name='fc1_6')(sfcn_1_3)\n",
    "\n",
    "score_vector_1 = Concatenate()([sfcn_1_1,sfcn_1_2,sfcn_1_3]) # Score vector 1\n",
    "att_weights_1 = Activation('softmax', name='attention_weights_1')(score_vector_1) # Attention weights 1\n",
    "weightned_sum_1 = Add()([c_1_bch_out*att_weights_1[0][0],c_2_bch_out*att_weights_1[0][1],x_out*att_weights_1[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_1_concat = Concatenate()([c_1_bch_out,weightned_sum_1])\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(coarse_1_concat)\n",
    "\n",
    "\n",
    "#-- Att for coarse 2---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_2_1 = Dense(att_neurons, name='fc2_1')(c_1_bch_out)\n",
    "sfcn_2_1 = Dense(1, name='fc2_2')(sfcn_2_1)\n",
    "# Coarse 2\n",
    "sfcn_2_2 = Dense(att_neurons, name='fc2_3')(c_2_bch_out)\n",
    "sfcn_2_2 = Dense(1, name='fc2_4')(sfcn_2_2)\n",
    "# Fine\n",
    "sfcn_2_3 = Dense(att_neurons, name='fc2_5')(x_out)\n",
    "sfcn_2_3 = Dense(1, name='fc2_6')(sfcn_2_3)\n",
    "\n",
    "score_vector_2 = Concatenate()([sfcn_2_1,sfcn_2_2,sfcn_2_3]) # Score vector 1\n",
    "att_weights_2 = Activation('softmax', name='attention_weights_2')(score_vector_2) # Attention weights 1\n",
    "weightned_sum_2 = Add()([c_1_bch_out*att_weights_2[0][0],c_2_bch_out*att_weights_2[0][1],x_out*att_weights_2[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_2_concat = Concatenate()([c_2_bch_out,weightned_sum_2])\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(coarse_2_concat)\n",
    "\n",
    "\n",
    "#-- Att for fine---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_3_1 = Dense(att_neurons, name='fc3_1')(c_1_bch_out)\n",
    "sfcn_3_1 = Dense(1, name='fc3_2')(sfcn_3_1)\n",
    "# Coarse 2\n",
    "sfcn_3_2 = Dense(att_neurons, name='fc3_3')(c_2_bch_out)\n",
    "sfcn_3_2 = Dense(1, name='fc3_4')(sfcn_3_2)\n",
    "# Fine\n",
    "sfcn_3_3 = Dense(att_neurons, name='fc3_5')(x_out)\n",
    "sfcn_3_3 = Dense(1, name='fc3_6')(sfcn_3_3)\n",
    "\n",
    "score_vector_3 = Concatenate()([sfcn_3_1,sfcn_3_2,sfcn_3_3]) # Score vector 1\n",
    "att_weights_3 = Activation('softmax', name='attention_weights_3')(score_vector_3) # Attention weights 1\n",
    "weightned_sum_3 = Add()([c_1_bch_out*att_weights_3[0][0],c_2_bch_out*att_weights_3[0][1],x_out*att_weights_3[0][2]]) # Weightned sum 3\n",
    "\n",
    "# Concat and prediction\n",
    "fine_concat = Concatenate()([x_out,weightned_sum_3])\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(fine_concat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bacnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "g_l6k6PeWZOr",
    "outputId": "a27eca1b-8524-4c3f-effe-1aac92bac444"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_ba_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_ba_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_ba_cnn_c,cons_ba_cnn_c,f1_ba_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- BA-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_ba_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_ba_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_ba_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_ba_cnn_c)\n",
    "print(\"Consistency:\",cons_ba_cnn_c)\n",
    "print(\"f1:\",f1_ba_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_ba_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7qKvpSMp2iE"
   },
   "source": [
    "# H-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9pshU_L-p0Sm"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wJ7BXWiAFQ_J",
    "outputId": "b3acf63f-5951-45df-f473-b838f36b27c0"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch_flatt = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch_flatt)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch_flatt = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch_concat = Concatenate()([c_1_bch_flatt,c_2_bch_flatt]) # Conectivity Pattern\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch_concat)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x_flatt = Flatten(name='flatten')(x)\n",
    "x = Concatenate()([c_2_bch_concat,x_flatt]) # Conectivity Pattern\n",
    "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='hcnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uTUo07wAW__7",
    "outputId": "ce827be6-1466-4c43-f605-d356ac0e40bd"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_h_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_h_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_h_cnn_c,cons_h_cnn_c,f1_h_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- H-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_h_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_h_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_h_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_h_cnn_c)\n",
    "print(\"Consistency:\",cons_h_cnn_c)\n",
    "print(\"f1:\",f1_h_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_h_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-net Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out= Add()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu', name='fc2_cifar10_1')(x)\n",
    "x = Add()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='addnet_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_addnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_addnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_addnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_addnet_c,cons_addnet_c,f1_addnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Add-net Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_addnet_c[4])\n",
    "print(\"Accuracy level 2:\",score_addnet_c[5])\n",
    "print(\"Accuracy level 3:\",score_addnet_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_addnet_c)\n",
    "print(\"Consistency:\",cons_addnet_c)\n",
    "print(\"f1:\",f1_addnet_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_addnet_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat-net Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out= Concatenate()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc2_cifar10_1')(x)\n",
    "x = Concatenate()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='concatnet_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_concatnet_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_concatnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_concatnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "acc_concatnet_c,cons_concatnet_c,f1_concatnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Concat-net Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_concatnet_c[4])\n",
    "print(\"Accuracy level 2:\",score_concatnet_c[5])\n",
    "print(\"Accuracy level 3:\",score_concatnett_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Accuracy:\",acc_concatnet_c)\n",
    "print(\"Consitency:\",cons_concatnet_c)\n",
    "print(\"f1:\",f1_concatnet_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_concatnet_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOp_WriEXRdI"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-g9xmRuBXTx0",
    "outputId": "6524904f-d12e-4856-df3d-75f56fd3f581"
   },
   "outputs": [],
   "source": [
    "summary = {'':['Flat CNN Base B','B-CNN Base B','BA-CNN Base B','H-CNN Base B','Add-net Base B','Concat-net Base B'],'Coarse 1': [0,score_b_cnn_b[4],score_ba_cnn_b[4],score_h_cnn_b[4],score_addnet_b[4],score_concatnet_b[4]],'Coarse 2': [0,score_b_cnn_b[5],score_ba_cnn_b[5],score_h_cnn_b[5],score_addnet_b[5],score_concatnet_b[5]],'Fine': [score_base_b[1],score_b_cnn_b[6],score_ba_cnn_b[6],score_h_cnn_b[6],score_addnet_b[6],score_concatnet_b[6]],'h_Accuracy':[0,acc_b_cnn_b,acc_ba_cnn_b,acc_h_cnn_b,acc_addnet_b,acc_concatnet_b],'h_Consistency':[0,cons_b_cnn_b,cons_ba_cnn_b,cons_h_cnn_b,cons_addnet_b,cons_concatnet_b],'F1':[0,f1_b_cnn_b,f1_ba_cnn_b,f1_h_cnn_b,f1_addnet_b,f1_concatnet_b],'Parameters': [parameters_base_b ,parameters_b_cnn_b,parameters_ba_cnn_b,parameters_h_cnn_b,parameters_addnet_b,parameters_concatnet_b]}\n",
    "summary = pd.DataFrame(summary)\n",
    "summary['Parameters'] = (summary['Parameters'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
    "summary = summary.set_index('')\n",
    "summary.style.highlight_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "iKqpTST7YtjT",
    "outputId": "4c1708bf-3919-4144-b488-6a2f8f6635e3"
   },
   "outputs": [],
   "source": [
    "summary = {'':['Flat CNN Base C','B-CNN Base C','BA-CNN Base C','H-CNN Base C','Add-net Base B','Concat-net Base B'],'Coarse 1': [0,score_b_cnn_c[4],score_ba_cnn_c[4],score_h_cnn_c[4],score_addnet_c[4],score_concatnet_c[4]],'Coarse 2': [0,score_b_cnn_c[5],score_ba_cnn_c[5],score_h_cnn_c[5],score_addnet_c[5],score_concatnet_c[5]],'Fine': [score_base_c[1],score_b_cnn_c[6],score_ba_cnn_c[6],score_h_cnn_c[6],score_addnet_c[6],score_concatnet_c[6]],'h_Accuracy':[0,acc_b_cnn_c,acc_ba_cnn_c,acc_h_cnn_c,acc_addnet_c,acc_concatnet_c],'Consistency':[0,cons_b_cnn_c,cons_ba_cnn_c,cons_h_cnn_c,cons_addnet_c,cons_concatnet_c],'F1':[0,f1_b_cnn_c,f1_ba_cnn_c,f1_h_cnn_c,f1_addnet_c,f1_concatnet_c],'Parameters': [parameters_base_c ,parameters_b_cnn_c,parameters_ba_cnn_c,parameters_h_cnn_c,parameters_addnet_c,parameters_concatnet_c]}\n",
    "summary = pd.DataFrame(summary)\n",
    "summary['Parameters'] = (summary['Parameters'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
    "summary = summary.set_index('')\n",
    "summary.style.highlight_max()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BA_CNN_CIFAR_10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
