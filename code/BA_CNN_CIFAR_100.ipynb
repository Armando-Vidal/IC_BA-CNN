{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvPb0YJNU1RM"
   },
   "source": [
    "\n",
    "<H3 align='center'> An Attention-Based Architecture for\n",
    "Hierarchical Classification with CNNs </H3>\n",
    "\n",
    "<H5 align='center'> CIFAR-100 </H3>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zlvBmYMm1z1"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb27tNtvUuKD"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Concatenate, Add, Softmax\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "from keras.initializers import he_normal\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, CSVLogger\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def unpickle(filename):\n",
    "  file = filename\n",
    "  with open(file, 'rb') as fo:\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    "  return dict\n",
    "  \n",
    "# Computes hierarchical metrics proposed by Kiritchenko et al (2005)\n",
    "def hierarchical_metrics(true,pred):\n",
    "  true_labels = []\n",
    "  true_fine = true[2].argmax(axis=1)\n",
    "  true_c2 = true[1].argmax(axis=1)\n",
    "  true_c1 = true[0].argmax(axis=1)\n",
    "  for i in range(len(true_fine)):\n",
    "    true_labels.append([true_c1[i],true_c2[i],true_fine[i]])\n",
    "  pred_labels = []\n",
    "  pred_c1 = pred[0].argmax(axis = 1)\n",
    "  pred_c2 = pred[1].argmax(axis = 1)\n",
    "  pred_fine = pred[2].argmax(axis = 1) \n",
    "  for i in range(len(pred_c1)):\n",
    "    pred_labels.append([pred_c1[i],pred_c2[i],pred_fine[i]])\n",
    "  preci = precision(true_labels,pred_labels)\n",
    "  reca = recall(true_labels,pred_labels)\n",
    "  f_1 = f1(true_labels,pred_labels)\n",
    "  return preci,reca,f_1\n",
    "\n",
    "# Hierarchical metrics, proposed by Kiritchenko et al (2005)\n",
    "# Implementation \n",
    "# https://gitlab.com/dacs-hpi/hiclass/-/blob/main/hiclass/metrics.py\n",
    "\n",
    "def precision(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute precision score for hierarchical classification.\n",
    "\n",
    "    hP = sum(|S intersection T|) / sum(|S|),\n",
    "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
    "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        What proportion of positive identifications was actually correct?\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    sum_intersection = 0\n",
    "    sum_prediction_and_ancestors = 0\n",
    "    for ground_truth, prediction in zip(y_true, y_pred):\n",
    "        sum_intersection = sum_intersection + len(\n",
    "            set(ground_truth).intersection(set(prediction))\n",
    "        )\n",
    "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
    "            set(prediction)\n",
    "        )\n",
    "    precision = sum_intersection / sum_prediction_and_ancestors\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute recall score for hierarchical classification.\n",
    "\n",
    "    hR = sum(|S intersection T|) / sum(|T|),\n",
    "    where S is the set consisting of the most specific class(es) predicted for a test example and all respective ancestors\n",
    "    and T is the set consisting of the true most specific class(es) for a test example and all respective ancestors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        What proportion of actual positives was identified correctly?\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    sum_intersection = 0\n",
    "    sum_prediction_and_ancestors = 0\n",
    "    for ground_truth, prediction in zip(y_true, y_pred):\n",
    "        sum_intersection = sum_intersection + len(\n",
    "            set(ground_truth).intersection(set(prediction))\n",
    "        )\n",
    "        sum_prediction_and_ancestors = sum_prediction_and_ancestors + len(\n",
    "            set(ground_truth)\n",
    "        )\n",
    "    recall = sum_intersection / sum_prediction_and_ancestors\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute f1 score for hierarchical classification.\n",
    "\n",
    "    hF = 2 * hP * hR / (hP + hR),\n",
    "    where hP is the hierarchical precision and hR is the hierarchical recall.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.array of shape (n_samples, n_levels)\n",
    "        Ground truth (correct) labels.\n",
    "    y_pred : np.array of shape (n_samples, n_levels)\n",
    "        Predicted labels, as returned by a classifier.\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Weighted average of the precision and recall\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oluoWzPPi29B"
   },
   "source": [
    "# General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ys4SZ4fti3Mr"
   },
   "outputs": [],
   "source": [
    "#-------- dimensions ---------\n",
    "height, width = 32, 32\n",
    "channel = 3\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (channel, height, width)\n",
    "else:\n",
    "    input_shape = (height, width, channel)\n",
    "#-----------------------------\n",
    "\n",
    "train_size = 50000\n",
    "test_size = 10000\n",
    "\n",
    "#--- coarse 1 classes ---\n",
    "coarse1_classes = 8\n",
    "#--- coarse 2 classes ---\n",
    "coarse2_classes = 20\n",
    "#--- fine classes ---\n",
    "num_classes  = 100\n",
    "\n",
    "batch_size   = 128\n",
    "epochs       = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VnZLarAjfDe"
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "  learning_rate_init = 0.001\n",
    "  if epoch > 55:\n",
    "    learning_rate_init = 0.0002\n",
    "  if epoch > 70:\n",
    "    learning_rate_init = 0.00005\n",
    "  return learning_rate_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arduYBRPkQNC"
   },
   "outputs": [],
   "source": [
    "# Obs en implementacion decia epoch 13,23 y 33 , y en paper 10,20,30\n",
    "class LossWeightsModifier(keras.callbacks.Callback):\n",
    "  def __init__(self, alpha, beta, gamma):\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "    # customize your behavior\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if epoch == 10:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.8)\n",
    "      K.set_value(self.gamma, 0.1)\n",
    "    if epoch == 20:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.2)\n",
    "      K.set_value(self.gamma, 0.7)\n",
    "    if epoch == 30:\n",
    "      K.set_value(self.alpha, 0)\n",
    "      K.set_value(self.beta, 0)\n",
    "      K.set_value(self.gamma, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLK5dkIHkhcI",
    "outputId": "03ce3e1f-4249-4d69-e28c-bb4dfe4c3bdb"
   },
   "outputs": [],
   "source": [
    "#----------get VGG16 pre-trained weights--------\n",
    "data_dir = \"cifar-100-python\"\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                         WEIGHTS_PATH,\n",
    "                         cache_subdir='models')\n",
    "\n",
    "#---------get data---------\n",
    "meta = unpickle(\"../data/\"+data_dir+\"/meta\")\n",
    "test = unpickle(\"../data/\"+data_dir+\"/test\")\n",
    "train = unpickle(\"../data/\"+data_dir+\"/train\")\n",
    "\n",
    "#-------------------- data loading ----------------------\n",
    "x_train = np.reshape(train[b'data'], (train_size, channel, height, width)).transpose(0, 2, 3, 1).astype(\"float32\")\n",
    "x_train = (x_train-np.mean(x_train)) / np.std(x_train)\n",
    "\n",
    "x_test = np.reshape(test[b'data'], (test_size, channel, height, width)).transpose(0, 2, 3, 1).astype(\"float32\")\n",
    "x_test = (x_test-np.mean(x_test)) / np.std(x_test)\n",
    "\n",
    "y_train = np.zeros((train_size, num_classes)).astype('float32')\n",
    "y_c2_train = np.zeros((train_size, coarse2_classes)).astype('float32')\n",
    "\n",
    "y_test = np.zeros((test_size, num_classes)).astype('float32')\n",
    "y_c2_test = np.zeros((test_size, coarse2_classes)).astype('float32')\n",
    "\n",
    "y_train[np.arange(train_size), train[b'fine_labels']] = 1\n",
    "y_c2_train[np.arange(train_size), train[b'coarse_labels']] = 1\n",
    "\n",
    "y_test[np.arange(test_size), test[b'fine_labels']] = 1\n",
    "y_c2_test[np.arange(test_size), test[b'coarse_labels']] = 1\n",
    "\n",
    "c2_to_f = np.zeros((coarse2_classes, num_classes)).astype('float32')\n",
    "fine_unique, fine_unique_indices = np.unique(train[b'fine_labels'], return_index=True)\n",
    "for i in fine_unique_indices:\n",
    "  c2_to_f[train[b'coarse_labels'][i]][train[b'fine_labels'][i]] = 1\n",
    "\n",
    "parent_c2 = {\n",
    "  0:0, 1:0, 2:1, 3:2, \n",
    "  4:1, 5:2, 6:2, 7:3, \n",
    "  8:4, 9:5, 10:5, 11:4, \n",
    "  12:4, 13:3, 14:6, 15:4, \n",
    "  16:4, 17:1, 18:7, 19:7\n",
    "}\n",
    "\n",
    "y_c1_train = np.zeros((y_c2_train.shape[0], coarse1_classes)).astype(\"float32\")\n",
    "y_c1_test = np.zeros((y_c2_test.shape[0], coarse1_classes)).astype(\"float32\")\n",
    "for i in range(y_c1_train.shape[0]):\n",
    "  y_c1_train[i][parent_c2[np.argmax(y_c2_train[i])]] = 1.0\n",
    "for i in range(y_c1_test.shape[0]):\n",
    "  y_c1_test[i][parent_c2[np.argmax(y_c2_test[i])]] = 1.0\n",
    "\n",
    "del(train)\n",
    "del(test)\n",
    "#---------------------------\n",
    "\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"y_c1_train shape: \", y_c1_train.shape)\n",
    "print(\"y_c1_test shape: \", y_c1_test.shape)\n",
    "print(\"y_c2_train shape: \", y_c2_train.shape)\n",
    "print(\"y_c2_test shape: \", y_c2_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QgJRBAElCsU"
   },
   "source": [
    "# Flat CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXekb-lIlFkb",
    "outputId": "d7309ef4-3ed9-45a8-80c9-fcb33177dcb2"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(x)\n",
    "\n",
    "model = Model(img_input, fine_pred, name='Flat_CNN_Base_C')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umUqNdkwlXCr",
    "outputId": "9c10cce3-db8c-421f-dca5-c87dea836255"
   },
   "outputs": [],
   "source": [
    "change_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Training\n",
    "history_base_c = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=change_lr,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation on test set\n",
    "score_base_c = model.evaluate(x_test, y_test, verbose=0)\n",
    "parameters_base_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "# Results\n",
    "print(\"--- Flat CNN Base C ---\")\n",
    "print(\"Accuracy:\",score_base_c[1])\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_base_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AAO2i_3llKE"
   },
   "source": [
    "# B-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iu13ILFwmbvC"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OUdd3hblrnF",
    "outputId": "efa7256a-66db-4c33-ee5c-880fed5c1f4c"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bcnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBFZX4nemlEM",
    "outputId": "2f4c71b5-c138-4ce3-94de-73d35f88252d"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_c = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_b_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_b_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_b_cnn_c,reca_b_cnn_c,f1_b_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- B-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_b_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_b_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_b_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_b_cnn_c)\n",
    "print(\"Recall:\",reca_b_cnn_c)\n",
    "print(\"f1:\",f1_b_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_b_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1Q6T--Rm7n9"
   },
   "source": [
    "# BA-CNN Base C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjdWeCDFnE6d"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True\n",
    "\n",
    "# neurons of all dense layers on each branch \n",
    "branch_neurons = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsAMV-VinFS8",
    "outputId": "dfde0a7c-b3d9-47e4-df3a-1803c416782d"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(branch_neurons, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "#c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(branch_neurons, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "#c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(branch_neurons, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "#fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(x)\n",
    "\n",
    "\n",
    "#-- Att for coarse 1---\n",
    "# Coarse 1\n",
    "sfcn_1_1 = Dense(64, name='fc1_1')(c_1_bch)\n",
    "sfcn_1_1 = Dense(1, name='fc1_2')(sfcn_1_1)\n",
    "# Coarse 2\n",
    "sfcn_1_2 = Dense(64, name='fc1_3')(c_2_bch)\n",
    "sfcn_1_2 = Dense(1, name='fc1_4')(sfcn_1_2)\n",
    "# Fine\n",
    "sfcn_1_3 = Dense(64, name='fc1_5')(x)\n",
    "sfcn_1_3 = Dense(1, name='fc1_6')(sfcn_1_3)\n",
    "\n",
    "score_vector_1 = Concatenate()([sfcn_1_1,sfcn_1_2,sfcn_1_3]) # Score vector 1\n",
    "att_weights_1 = Activation('softmax', name='attention_weights_1')(score_vector_1) # Attention weights 1\n",
    "weightned_sum_1 = Add()([c_1_bch*att_weights_1[0][0],c_2_bch*att_weights_1[0][1],x*att_weights_1[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_1_concat = Concatenate()([c_1_bch,weightned_sum_1])\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar100')(coarse_1_concat)\n",
    "\n",
    "\n",
    "#-- Att for coarse 2---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_2_1 = Dense(64, name='fc2_1')(c_1_bch)\n",
    "sfcn_2_1 = Dense(1, name='fc2_2')(sfcn_2_1)\n",
    "# Coarse 2\n",
    "sfcn_2_2 = Dense(64, name='fc2_3')(c_2_bch)\n",
    "sfcn_2_2 = Dense(1, name='fc2_4')(sfcn_2_2)\n",
    "# Fine\n",
    "sfcn_2_3 = Dense(64, name='fc2_5')(x)\n",
    "sfcn_2_3 = Dense(1, name='fc2_6')(sfcn_2_3)\n",
    "\n",
    "score_vector_2 = Concatenate()([sfcn_2_1,sfcn_2_2,sfcn_2_3]) # Score vector 1\n",
    "att_weights_2 = Activation('softmax', name='attention_weights_2')(score_vector_2) # Attention weights 1\n",
    "weightned_sum_2 = Add()([c_1_bch*att_weights_2[0][0],c_2_bch*att_weights_2[0][1],x*att_weights_2[0][2]]) # Weightned sum 1\n",
    "\n",
    "# Concat and prediction\n",
    "coarse_2_concat = Concatenate()([c_2_bch,weightned_sum_2])\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(coarse_2_concat)\n",
    "\n",
    "\n",
    "#-- Att for fine---\n",
    "\n",
    "# Coarse 1\n",
    "sfcn_3_1 = Dense(64, name='fc3_1')(c_1_bch)\n",
    "sfcn_3_1 = Dense(1, name='fc3_2')(sfcn_3_1)\n",
    "# Coarse 2\n",
    "sfcn_3_2 = Dense(64, name='fc3_3')(c_2_bch)\n",
    "sfcn_3_2 = Dense(1, name='fc3_4')(sfcn_3_2)\n",
    "# Fine\n",
    "sfcn_3_3 = Dense(64, name='fc3_5')(x)\n",
    "sfcn_3_3 = Dense(1, name='fc3_6')(sfcn_3_3)\n",
    "\n",
    "score_vector_3 = Concatenate()([sfcn_3_1,sfcn_3_2,sfcn_3_3]) # Score vector 1\n",
    "att_weights_3 = Activation('softmax', name='attention_weights_3')(score_vector_3) # Attention weights 1\n",
    "weightned_sum_3 = Add()([c_1_bch*att_weights_3[0][0],c_2_bch*att_weights_3[0][1],x*att_weights_3[0][2]]) # Weightned sum 3\n",
    "\n",
    "# Concat and prediction\n",
    "fine_concat = Concatenate()([x,weightned_sum_3])\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(fine_concat)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='bacnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrcUqi9pnsMT",
    "outputId": "f7ad8867-5e67-48b0-94c1-ac03252b1267"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_ba_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_ba_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_ba_cnn_c,reca_ba_cnn_c,f1_ba_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- BA-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_ba_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_ba_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_ba_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_ba_cnn_c)\n",
    "print(\"Recall:\",reca_ba_cnn_c)\n",
    "print(\"f1:\",f1_ba_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_ba_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjnFXa29nvcm"
   },
   "source": [
    "# H-CNN Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JjFhDFFnx9j"
   },
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2rWS0PJn1OJ",
    "outputId": "c5810b09-d521-4071-de82-2268c564836e"
   },
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch_flatt = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch_flatt)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch_flatt = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch_concat = Concatenate()([c_1_bch_flatt,c_2_bch_flatt]) # Conectivity Pattern\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch_concat)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(c_2_bch)\n",
    "\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- coarse 3(Fine) ---\n",
    "\n",
    "x_flatt = Flatten(name='flatten')(x)\n",
    "x = Concatenate()([c_2_bch_concat,x_flatt]) # Conectivity Pattern\n",
    "x = Dense(4096, activation='relu', name='fc_1_cifar100')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc_2_cifar100')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "c_3_pred = Dense(num_classes, activation='softmax', name='c_3_2_predictions_cifar100')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred,c_2_pred,c_3_pred], name='hcnn_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha,beta,gamma], \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3TLgvXqoGF3",
    "outputId": "21aad2ef-35dc-4425-fbbf-4eca8682a7b0"
   },
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_h_cnn_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_h_cnn_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_h_cnn_c,reca_h_cnn_c,f1_h_cnn_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- H-CNN Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_h_cnn_c[4])\n",
    "print(\"Accuracy level 2:\",score_h_cnn_c[5])\n",
    "print(\"Accuracy level 3:\",score_h_cnn_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_h_cnn_c)\n",
    "print(\"Recall:\",reca_h_cnn_c)\n",
    "print(\"f1:\",f1_h_cnn_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_h_cnn_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-net Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(256, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out = Add()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = Add()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='addnet_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_addnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_addnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_addnet_c,reca_addnet_c,f1_addnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Add-net Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_addnet_c[4])\n",
    "print(\"Accuracy level 2:\",score_addnet_c[5])\n",
    "print(\"Accuracy level 3:\",score_addnet_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_addnet_c)\n",
    "print(\"Recall:\",reca_addnet_c)\n",
    "print(\"f1:\",f1_addnet_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_addnet_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat-net Base C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, the model uses BT-strategy for training\n",
    "bt_strategy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------- model definition ---------------------------\n",
    "if bt_strategy == True:\n",
    "  alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "else:\n",
    "  alpha = K.variable(value=0.33, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "  beta = K.variable(value=0.33, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "  gamma = K.variable(value=0.34, dtype=\"float32\", name=\"gamma\") # A3 in paper  \n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(512, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch_out = Dense(512, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch_out)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(coarse1_classes, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch_out = Concatenate()([c_1_bch_out,c_2_bch])\n",
    "c_2_bch = BatchNormalization()(c_2_bch_out)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(coarse2_classes, activation='softmax', name='c2_predictions_cifar100')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "#--- block 5 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
    "x = Concatenate()([x,c_2_bch_out])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar100')(x)\n",
    "\n",
    "model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='concatnet_base_c')\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma], \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "\n",
    "if bt_strategy == True:\n",
    "  cbks = [change_lr, change_lw]\n",
    "else:\n",
    "  cbks = [change_lr]\n",
    "\n",
    "history_bcnn_b = model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=0,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "score_concatnet_c = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "parameters_concatnet_c = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "preci_concatnet_c,reca_concatnet_c,f1_concatnet_c= hierarchical_metrics([y_c1_test, y_c2_test, y_test],predictions)\n",
    "\n",
    "# Results\n",
    "print(\"--- Concat-net Base C ---\")\n",
    "print(\"--- Accuracy per level ---\")\n",
    "print(\"Accuracy level 1:\",score_concatnet_c[4])\n",
    "print(\"Accuracy level 2:\",score_concatnet_c[5])\n",
    "print(\"Accuracy level 3:\",score_concatnet_c[6])\n",
    "print(\"--- Hierarchical Metrics ---\")\n",
    "print(\"Precision:\",preci_concatnet_c)\n",
    "print(\"Recall:\",reca_concatnet_c)\n",
    "print(\"f1:\",f1_concatnet_c)\n",
    "print(\"Parameters:\",\"{:,}\".format(parameters_concatnet_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MiHq1pBoKzJ"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_SO--Z3ZoMmB",
    "outputId": "0d9b83f7-899f-452c-e614-8a3171089b4f"
   },
   "outputs": [],
   "source": [
    "summary = {'':['Flat CNN Base C','B-CNN Base C','BA-CNN Base C','H-CNN Base C','Add-net Base C','Concat-net Base C'],'Coarse 1': [0,score_b_cnn_c[4],score_ba_cnn_c[4],score_h_cnn_c[4],score_addnet_c[4],score_concatnet_c[4]],'Coarse 2': [0,score_b_cnn_c[5],score_ba_cnn_c[5],score_h_cnn_c[5],score_addnet_c[5],score_concatnet_c[5]],'Fine': [score_base_c[1],score_b_cnn_c[6],score_ba_cnn_c[6],score_h_cnn_c[6],score_addnet_c[6],score_concatnet_c[6]],'Precision':[0,preci_b_cnn_c,preci_ba_cnn_c,preci_h_cnn_c,preci_addnet_c,preci_concatnet_c],'Recall':[0,reca_b_cnn_c,reca_ba_cnn_c,reca_h_cnn_c,reca_addnet_c,reca_concatnet_c],'f1':[0,f1_b_cnn_c,f1_ba_cnn_c,f1_h_cnn_c,f1_addnet_c,f1_concatnet_c],'Parameters': [parameters_base_c ,parameters_b_cnn_c,parameters_ba_cnn_c,parameters_h_cnn_c,parameters_addnet_c,parameters_concatnet_c]}\n",
    "summary = pd.DataFrame(summary)\n",
    "summary['Parameters'] = (summary['Parameters'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
    "summary = summary.set_index('')\n",
    "summary.style.highlight_max()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "BA_CNN_CIFAR_100.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
